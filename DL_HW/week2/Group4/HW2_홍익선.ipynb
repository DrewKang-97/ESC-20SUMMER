{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정 \n",
    "learning_rate=0.1\n",
    "training_epochs=15\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST data 불러오기 & train-test split\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset loader에 train_test assign (*batch size, shuffle, drop_last*)\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layering(조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), relu 사용, batch normalization)\n",
    "#각 layer의 hidden node 수: (784,100), (100,100),(100,10)\n",
    "linear1 = torch.nn.Linear(784, 100, bias=True)\n",
    "linear2 = torch.nn.Linear(100, 100, bias=True)\n",
    "linear3 = torch.nn.Linear(100, 10, bias=True)\n",
    "relu=torch.nn.ReLU()\n",
    "\n",
    "bn1 = torch.nn.BatchNorm1d(100)\n",
    "bn2 = torch.nn.BatchNorm1d(100)\n",
    "dropout = torch.nn.Dropout(p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 5.4443e-02,  2.2403e-01,  4.8933e-02, -1.4577e-01, -1.8920e-01,\n",
       "          8.7791e-02, -6.2778e-02, -1.8040e-01, -2.0778e-01,  2.9983e-02,\n",
       "          1.9306e-01, -2.0536e-02,  1.7146e-01,  1.3683e-01,  2.3379e-02,\n",
       "         -1.3184e-01, -8.2875e-02,  1.1905e-01, -1.6735e-01,  1.0278e-01,\n",
       "         -1.4644e-01,  2.8059e-02, -2.0533e-01,  6.6647e-02,  3.9617e-02,\n",
       "          1.7191e-01, -2.1065e-01, -1.5448e-01, -1.3070e-02,  1.0096e-02,\n",
       "         -1.9099e-01,  2.0193e-01, -3.7552e-02,  9.0258e-02, -1.0214e-01,\n",
       "          2.9346e-02, -1.0090e-01,  7.0380e-02,  5.0289e-02, -1.2136e-01,\n",
       "          6.6057e-02,  1.7333e-01,  2.1797e-02,  2.3108e-01, -2.1180e-02,\n",
       "          1.7189e-01, -1.6324e-01,  1.3139e-01, -7.1212e-02, -1.3852e-01,\n",
       "         -1.8221e-02,  1.4865e-01, -1.0223e-01, -5.8830e-02,  6.5457e-02,\n",
       "          3.0334e-02,  1.3609e-01,  1.5321e-01,  2.1843e-01, -3.0243e-02,\n",
       "          4.0459e-02, -9.8311e-02,  3.7482e-04, -9.9259e-02,  6.0476e-04,\n",
       "          7.2781e-02,  1.6850e-02,  1.0337e-01,  1.6892e-01,  1.7350e-01,\n",
       "         -1.0977e-01,  5.4633e-02,  6.8705e-02, -7.8750e-02,  8.9635e-02,\n",
       "          7.0287e-02,  1.6977e-01,  1.1822e-01, -2.1416e-01, -3.4871e-02,\n",
       "          1.5665e-01, -7.1288e-02,  1.4429e-01,  1.9111e-01,  6.2753e-02,\n",
       "         -1.3349e-01, -1.5770e-01,  1.1327e-01, -2.1085e-01, -1.3180e-01,\n",
       "         -1.1835e-01,  1.3631e-02,  9.5181e-02, -7.3279e-02,  2.2103e-01,\n",
       "         -1.3062e-01,  1.6709e-01, -5.1843e-02,  6.3688e-02, -5.6683e-02],\n",
       "        [-8.1786e-02,  5.4939e-02, -5.8413e-02,  2.2365e-02,  2.0117e-01,\n",
       "          2.5733e-02, -2.7674e-02, -5.5867e-02,  2.6809e-02,  9.0159e-02,\n",
       "         -1.0075e-01, -4.0713e-02, -1.4607e-01,  1.8242e-01, -1.4606e-01,\n",
       "         -9.5401e-02, -7.2739e-03, -1.8484e-01,  8.4652e-02, -2.1077e-01,\n",
       "         -1.5349e-02,  1.8269e-01,  2.1383e-02, -1.8794e-01, -1.7467e-01,\n",
       "         -1.2100e-01, -1.6154e-01, -1.9621e-01,  9.5523e-02,  1.7705e-01,\n",
       "         -4.2188e-02, -2.2504e-01, -2.0796e-01, -3.8338e-02,  1.0075e-01,\n",
       "          3.6675e-02,  1.4975e-01,  9.9491e-02,  1.2282e-02, -7.0420e-02,\n",
       "          2.2940e-01,  1.8503e-01,  2.0129e-01, -3.0336e-02,  8.9133e-03,\n",
       "          6.8281e-02,  6.7364e-02,  1.4111e-02,  2.1925e-01,  1.8644e-01,\n",
       "          1.1096e-01,  7.9594e-02, -7.7059e-02, -1.0549e-01,  2.1271e-01,\n",
       "         -1.5111e-01,  1.9383e-01, -5.1105e-02, -1.0859e-01, -6.2965e-02,\n",
       "         -3.6328e-02,  1.6396e-01,  1.8256e-01,  9.8409e-02, -8.7846e-02,\n",
       "         -6.1583e-02,  8.7965e-02,  2.1139e-01, -1.8635e-01,  1.2455e-01,\n",
       "          2.3182e-01,  5.9918e-02,  1.5406e-01,  2.1648e-01,  1.1077e-01,\n",
       "          8.2937e-04, -9.3224e-02, -2.1029e-01, -9.7479e-02, -1.9203e-01,\n",
       "          9.3728e-02,  4.0568e-02, -6.7248e-02, -1.5235e-01,  1.4925e-01,\n",
       "         -1.5178e-01,  2.1284e-01,  1.5799e-01,  9.0397e-02,  2.0254e-01,\n",
       "          4.7934e-02,  1.0961e-01, -5.3263e-02,  2.1258e-01,  1.4764e-01,\n",
       "          1.6121e-01,  4.3660e-02, -1.9443e-01, -1.4363e-02,  1.1535e-01],\n",
       "        [-6.6689e-02,  1.0054e-01, -1.5574e-01, -2.0920e-01, -4.3248e-02,\n",
       "         -8.7707e-02, -1.7466e-01,  1.6708e-01,  5.8092e-02, -7.1892e-02,\n",
       "          2.1297e-01, -1.6226e-01,  1.2961e-01,  9.7274e-02, -2.1943e-01,\n",
       "         -1.7402e-01, -2.5862e-02, -1.8079e-01, -1.8949e-01, -1.9300e-02,\n",
       "          7.1205e-02,  1.0319e-01,  1.5673e-01, -1.3219e-01, -2.2285e-01,\n",
       "         -1.2828e-01,  1.7437e-02, -1.7485e-01, -1.2943e-01, -1.5290e-01,\n",
       "         -1.7810e-01,  2.6676e-02, -1.6487e-01,  9.8724e-02,  1.2869e-01,\n",
       "          1.1863e-01, -1.8489e-01,  1.5888e-01, -1.1717e-01,  1.1701e-01,\n",
       "         -2.9726e-02, -6.4693e-02, -3.4426e-02, -1.3468e-01,  1.5926e-01,\n",
       "          1.1263e-01, -5.6308e-02,  1.6038e-01, -1.1573e-01,  2.3069e-01,\n",
       "         -4.0351e-03,  1.3794e-01, -1.8766e-01, -1.4114e-01, -1.9885e-01,\n",
       "          1.3638e-01, -1.1217e-01, -1.4336e-01, -1.1146e-01,  4.3958e-02,\n",
       "         -2.0016e-01, -1.5996e-01,  1.6379e-01, -3.9856e-02, -1.5308e-02,\n",
       "          1.6764e-01,  1.8279e-01, -5.2175e-02,  1.9040e-01,  1.9182e-01,\n",
       "          2.2990e-01,  1.4108e-01,  4.2282e-02,  1.6832e-01, -1.9397e-04,\n",
       "         -5.7886e-02,  2.1447e-01,  9.2734e-04, -4.5820e-02, -1.8885e-01,\n",
       "          1.1681e-01, -7.9032e-02,  5.3540e-03, -2.1554e-01, -1.6282e-01,\n",
       "         -7.3057e-02, -4.7986e-02, -1.7757e-01, -1.1403e-01,  4.8431e-02,\n",
       "         -8.5881e-02, -5.2642e-02, -1.1171e-01, -4.4202e-03, -1.7135e-01,\n",
       "          1.4929e-01,  2.1668e-02,  5.2351e-02, -8.7557e-02,  1.3536e-01],\n",
       "        [ 7.1897e-02,  2.1577e-02,  2.2492e-01, -1.4754e-01,  2.2547e-01,\n",
       "          1.1904e-01, -1.3859e-01, -2.1885e-01, -1.1574e-01,  1.0480e-01,\n",
       "          1.9056e-02, -1.8745e-01, -1.8908e-01, -1.3258e-01,  1.2335e-01,\n",
       "         -1.9202e-01, -1.1891e-01, -2.3337e-01, -1.6661e-02,  1.0061e-01,\n",
       "          5.7777e-03,  4.9799e-02,  6.4392e-02, -1.6188e-01,  5.2665e-02,\n",
       "          1.5569e-02, -1.9001e-01,  2.0516e-01, -7.6314e-02, -9.8353e-03,\n",
       "         -1.9908e-01, -1.6167e-01,  2.0407e-01, -1.8942e-01, -2.8327e-02,\n",
       "          1.1681e-01,  6.4213e-02, -3.2441e-02, -4.0718e-02,  2.1849e-01,\n",
       "         -1.9969e-01,  1.3498e-01, -1.1234e-01, -6.1846e-02, -5.0006e-02,\n",
       "         -2.1003e-01, -1.3103e-02, -1.2095e-01, -1.2020e-01,  1.2673e-01,\n",
       "         -4.6642e-02,  1.2831e-02,  1.0891e-01,  1.6086e-01,  2.3257e-01,\n",
       "         -1.2481e-01, -1.1211e-01,  5.1993e-02, -1.2406e-01, -1.7807e-01,\n",
       "          2.0708e-01,  3.5304e-02,  1.4086e-01, -1.3427e-01, -1.2406e-01,\n",
       "         -2.8188e-02,  1.8117e-01, -1.6890e-01, -1.9937e-01,  7.9125e-02,\n",
       "         -1.4456e-01,  1.3397e-01,  9.6047e-03,  1.4682e-01,  1.1604e-01,\n",
       "         -2.0676e-01,  1.4389e-01, -2.1747e-01,  1.7395e-01, -8.7784e-02,\n",
       "          4.4569e-03, -4.9246e-02,  1.4744e-01, -1.1610e-01, -2.2955e-01,\n",
       "          3.1092e-03,  2.0805e-01, -1.6797e-01, -5.0065e-02,  3.2721e-02,\n",
       "          1.2294e-01,  1.0076e-01, -2.0581e-01,  1.0400e-01,  1.3708e-01,\n",
       "         -1.2756e-01,  1.3869e-01, -1.0398e-01,  7.3936e-02,  2.2419e-01],\n",
       "        [-1.2026e-01, -2.5489e-02, -4.5124e-02, -1.8021e-01,  3.0216e-02,\n",
       "          9.6434e-02,  1.0971e-01,  1.0152e-01,  2.7756e-02, -2.3560e-03,\n",
       "         -1.8728e-01, -2.3961e-02,  1.4540e-01, -4.9747e-02, -1.0957e-01,\n",
       "          2.0017e-01, -1.9241e-01, -1.8015e-01, -2.2731e-01,  1.5249e-01,\n",
       "          6.0968e-03, -1.9170e-01,  1.1915e-01,  6.7593e-03, -2.3310e-01,\n",
       "          1.9703e-01, -7.5180e-02, -1.5623e-01, -1.9613e-01, -4.5279e-04,\n",
       "         -1.8401e-01, -4.5203e-02, -1.0002e-01, -1.7837e-01,  1.8717e-01,\n",
       "         -1.1737e-01, -2.0180e-01, -1.6020e-01,  4.7539e-02, -1.8792e-01,\n",
       "          1.9285e-02, -9.4787e-02,  4.3026e-02,  2.0883e-01,  5.2810e-02,\n",
       "         -7.1204e-02, -1.3760e-01, -2.0624e-01,  1.7776e-02,  9.5608e-02,\n",
       "          1.7423e-01, -2.3363e-02, -8.6036e-02, -1.8400e-01,  6.2712e-02,\n",
       "          1.5965e-01,  2.1449e-01, -7.1507e-02,  8.7074e-02, -2.1549e-01,\n",
       "         -1.0198e-01,  1.2232e-01,  1.8652e-01, -8.3879e-02, -4.6493e-02,\n",
       "          2.9387e-03,  1.1182e-01, -1.9173e-01,  9.8316e-02,  1.7129e-02,\n",
       "          1.0519e-01,  1.0235e-01,  1.4563e-01,  1.2527e-01,  1.8612e-01,\n",
       "          9.5210e-02,  1.9673e-01,  6.6269e-03, -5.1292e-02,  8.9915e-02,\n",
       "         -1.9149e-01,  1.5081e-01,  1.7140e-01,  6.0709e-02, -6.7430e-02,\n",
       "          2.1193e-01,  8.3280e-02, -2.0654e-01,  1.0878e-01, -1.2998e-01,\n",
       "         -1.5726e-01,  8.3264e-02, -4.0725e-02, -1.9792e-01,  1.0184e-01,\n",
       "         -3.6215e-02, -5.3102e-02, -5.0975e-02, -1.1998e-01,  9.7291e-02],\n",
       "        [-1.4409e-01,  8.4150e-02,  1.8196e-01, -1.3847e-01,  8.8781e-02,\n",
       "         -1.0203e-02, -1.3057e-01,  2.0279e-01, -2.0289e-01,  3.9665e-02,\n",
       "          1.3943e-01,  1.7455e-01,  5.3318e-02,  6.3246e-02,  5.1119e-02,\n",
       "         -2.0467e-01,  7.3428e-02,  9.8891e-03, -1.9629e-01, -4.2387e-02,\n",
       "         -7.5726e-02, -9.7452e-02, -1.5238e-01,  1.5756e-01, -3.1231e-02,\n",
       "         -1.5256e-01, -2.1524e-01,  4.5505e-02, -1.8854e-02, -1.8451e-01,\n",
       "         -1.7156e-01,  9.4752e-02, -1.2798e-02, -2.2253e-01,  1.6729e-01,\n",
       "         -1.5446e-01,  2.2587e-01, -2.1067e-01,  9.3000e-02,  1.3634e-01,\n",
       "          1.5763e-01, -2.1052e-01,  5.0798e-02,  1.3998e-01,  7.7753e-02,\n",
       "         -4.8791e-02,  9.2813e-03,  1.3159e-01, -1.6703e-01, -1.5480e-01,\n",
       "          5.0382e-02, -1.3104e-01,  2.2887e-01, -1.7099e-01,  8.3372e-02,\n",
       "         -1.7868e-02, -5.2673e-02, -6.6626e-03, -1.3539e-01,  1.8836e-01,\n",
       "         -2.0263e-03, -1.0384e-01, -1.7036e-01, -2.3152e-01, -2.0352e-01,\n",
       "          1.1075e-01, -2.1573e-01,  1.6461e-01,  2.0803e-01,  6.2270e-02,\n",
       "          3.0417e-02,  9.2029e-02,  7.7121e-02, -1.3533e-01, -8.5874e-02,\n",
       "          1.4800e-01,  2.1340e-02, -1.9486e-01, -7.1532e-02,  1.5108e-01,\n",
       "         -1.6667e-01,  1.8489e-01, -6.8259e-04, -1.6365e-01, -1.0319e-02,\n",
       "         -1.1329e-01, -8.0320e-02,  1.1693e-01, -1.9182e-02, -1.0665e-01,\n",
       "         -1.2845e-01, -8.1525e-02, -6.6188e-03, -2.6486e-02, -1.5114e-01,\n",
       "         -7.5625e-02, -1.8283e-01,  1.2832e-01,  2.0966e-01,  7.1208e-02],\n",
       "        [ 4.6898e-02,  1.9632e-01,  3.5494e-02,  1.7162e-02, -9.2884e-02,\n",
       "          1.2547e-01, -2.1008e-01,  2.1439e-02,  1.9526e-01, -1.0077e-01,\n",
       "         -5.0739e-02,  1.9518e-01, -8.1942e-02, -1.7199e-01,  2.0654e-01,\n",
       "         -1.8927e-01,  1.4445e-01,  1.5557e-01,  5.8707e-02,  3.9085e-02,\n",
       "         -1.2559e-01, -1.4758e-01,  2.1530e-01,  1.8859e-01, -1.8854e-01,\n",
       "          2.0416e-01,  2.2306e-01,  1.6780e-01,  1.7691e-01,  1.4541e-01,\n",
       "          1.9575e-01, -1.2937e-01,  2.0148e-01, -1.5894e-01,  7.3827e-02,\n",
       "         -1.8205e-01, -1.3396e-01, -1.6905e-01,  1.3888e-01, -1.3151e-01,\n",
       "         -9.5993e-02,  2.3076e-02, -1.2921e-01, -2.5198e-02, -1.2715e-01,\n",
       "         -1.6140e-01, -1.2949e-01,  1.4619e-01, -1.8260e-01,  1.2942e-02,\n",
       "         -5.3927e-02, -7.8174e-02, -1.2769e-01, -1.0257e-01,  1.0360e-01,\n",
       "         -1.2585e-01,  1.9658e-01, -8.8615e-02,  1.1618e-01, -5.5029e-02,\n",
       "         -4.3199e-03, -1.6219e-01,  1.2706e-01, -5.7036e-02, -5.5258e-02,\n",
       "         -3.6975e-02, -3.3365e-03,  2.2000e-01, -5.9901e-02,  2.2136e-01,\n",
       "         -1.0005e-02,  7.4961e-02,  1.9749e-01, -1.7784e-01,  1.2238e-01,\n",
       "         -1.5719e-01, -1.2471e-01, -1.6672e-02,  3.4786e-02, -1.9246e-01,\n",
       "          2.3303e-01,  4.1572e-02,  1.0944e-01, -2.7345e-02,  2.1418e-01,\n",
       "         -2.2880e-01,  4.8403e-02,  1.3430e-01, -2.1008e-01, -2.3345e-01,\n",
       "          8.6209e-02, -1.5967e-01,  2.1478e-01,  7.3214e-02,  1.5225e-01,\n",
       "         -1.8785e-01, -1.2208e-01, -1.5309e-01,  1.2271e-01, -5.7252e-02],\n",
       "        [ 1.9334e-01, -2.4115e-02, -1.7567e-01, -1.0130e-01,  1.0381e-01,\n",
       "         -1.2470e-01, -1.6433e-01,  1.1328e-01, -9.4369e-02,  1.2646e-01,\n",
       "         -1.5562e-01, -2.2678e-01,  4.8676e-02, -2.0282e-01,  2.1082e-01,\n",
       "         -1.2161e-01, -1.6323e-01, -1.7303e-01,  1.3273e-01,  2.1214e-01,\n",
       "          8.8672e-02, -5.6907e-02,  3.7657e-03, -2.9817e-02, -5.7089e-02,\n",
       "          1.5735e-01,  1.1148e-01, -3.9769e-02,  8.9398e-02,  5.9823e-02,\n",
       "         -1.8706e-01,  1.9532e-01,  2.1704e-01,  1.9597e-01,  2.1448e-01,\n",
       "          1.1243e-01,  2.8948e-02, -2.2647e-01,  2.4965e-02,  1.5048e-01,\n",
       "         -8.1365e-02, -2.3227e-02,  2.1307e-01, -2.2405e-01, -8.0469e-02,\n",
       "         -2.2368e-01, -1.3717e-01,  1.5877e-01,  1.0733e-01,  1.0062e-01,\n",
       "          5.7358e-02, -1.3857e-01, -7.9752e-03,  1.4763e-01,  1.6048e-01,\n",
       "         -1.5044e-01,  1.0525e-01, -2.2549e-01, -3.8809e-02, -2.0589e-01,\n",
       "         -4.2409e-02, -1.7325e-01,  8.0291e-02,  1.5229e-03,  1.0840e-01,\n",
       "          8.8389e-03, -5.4624e-02, -2.1651e-01, -1.7715e-01, -1.0271e-01,\n",
       "         -1.9445e-01, -6.8428e-02, -1.9932e-01, -9.1632e-02,  2.1904e-02,\n",
       "          3.2870e-02,  1.9126e-01, -8.1603e-02,  1.7084e-02,  2.0956e-02,\n",
       "          6.9295e-02,  1.7234e-01,  1.5228e-01, -1.0176e-01, -5.0527e-02,\n",
       "         -1.0053e-01,  1.7309e-01, -5.5461e-02,  7.4549e-02, -7.2461e-02,\n",
       "          1.5685e-01,  1.1659e-01, -8.8847e-02, -8.5725e-02,  1.1739e-01,\n",
       "          2.2658e-01, -1.2119e-01,  2.1049e-02, -1.0379e-01, -1.4435e-01],\n",
       "        [-7.4983e-02,  1.5653e-02,  1.3592e-01,  1.0462e-01, -4.3162e-02,\n",
       "          1.9039e-01, -6.8034e-02,  4.1040e-02, -4.7204e-02,  1.2892e-01,\n",
       "         -9.2141e-02,  5.4627e-03, -1.5690e-01,  5.9262e-02,  6.5789e-05,\n",
       "          3.4681e-02,  1.9237e-01,  2.3200e-01,  8.8721e-02,  1.3885e-01,\n",
       "          6.6477e-02, -8.8290e-02,  2.2183e-01,  1.3954e-01, -3.4592e-03,\n",
       "         -5.1869e-02, -4.3807e-02,  6.3992e-02,  2.1741e-01, -2.2892e-01,\n",
       "         -2.1654e-01, -1.8272e-01, -1.9375e-01,  1.1270e-01,  1.2082e-01,\n",
       "          9.2396e-03,  8.4439e-02, -7.1370e-02, -1.7076e-01, -8.1984e-02,\n",
       "          1.5511e-01,  1.0235e-02, -6.1306e-02,  2.4018e-03, -9.4261e-02,\n",
       "          1.9240e-01, -2.0058e-01,  6.3883e-02, -2.2521e-01,  2.0872e-02,\n",
       "         -1.4820e-01,  5.2033e-02, -1.9370e-01,  4.6800e-02,  1.5933e-01,\n",
       "         -4.5654e-02, -1.2276e-01,  1.8181e-02, -1.1287e-01,  7.6951e-02,\n",
       "         -8.8475e-03,  1.5605e-01, -1.6724e-01, -3.2802e-02, -1.7979e-01,\n",
       "          2.1092e-02,  1.9840e-01, -1.3881e-01,  5.4256e-02, -1.1039e-01,\n",
       "          1.4592e-01,  2.0520e-01, -2.0475e-01, -1.4813e-01, -1.9704e-01,\n",
       "         -2.3571e-02, -2.0296e-01, -1.1154e-01, -1.3447e-01, -1.8263e-01,\n",
       "          2.0281e-01, -1.1804e-01,  1.1498e-01,  1.5540e-01,  2.0502e-01,\n",
       "          2.9589e-04, -8.2612e-02, -1.3508e-01,  1.6885e-01, -1.8346e-01,\n",
       "         -1.7359e-01,  1.0792e-01, -2.2342e-01,  1.3263e-01,  7.6053e-03,\n",
       "          2.2308e-01, -3.7455e-03,  9.1416e-02,  9.6986e-04,  1.3121e-01],\n",
       "        [ 1.5396e-01, -2.2187e-01,  2.2036e-01,  2.1763e-01, -2.6846e-03,\n",
       "          8.1607e-02,  1.9534e-01, -2.0950e-01, -9.7152e-03, -8.4995e-02,\n",
       "         -6.1987e-02, -1.0340e-01,  1.5504e-01,  1.8567e-02,  4.6807e-02,\n",
       "          1.3639e-02, -5.6456e-02, -1.0596e-02,  6.8444e-02, -1.5589e-01,\n",
       "         -7.8273e-02, -8.0133e-02,  1.1680e-01, -9.2072e-02, -2.1962e-01,\n",
       "          1.8350e-01, -1.1027e-01,  1.3803e-01, -1.1102e-01, -1.6912e-02,\n",
       "          5.4061e-02,  8.7738e-02,  1.3074e-01,  9.7903e-02,  1.4087e-01,\n",
       "          1.8743e-02, -1.5348e-01, -8.1250e-02,  1.5601e-01,  1.0079e-01,\n",
       "          1.9494e-01,  9.2350e-02, -4.1271e-02, -2.1756e-01,  9.8962e-02,\n",
       "         -2.9532e-02,  2.0414e-02, -2.1671e-01, -1.9951e-01, -1.0280e-01,\n",
       "         -5.9011e-02,  8.3179e-02,  7.2663e-02,  1.6897e-01, -5.2407e-02,\n",
       "         -7.2674e-02, -1.1381e-01,  1.0389e-01,  1.9495e-02,  7.8656e-02,\n",
       "          1.3747e-02,  1.8615e-01, -8.1725e-02,  1.8887e-01,  1.1566e-02,\n",
       "         -1.1269e-01,  1.1696e-01, -8.5670e-02,  2.2472e-01,  2.5423e-02,\n",
       "         -7.3240e-02,  1.2077e-01,  9.0814e-02, -2.2356e-01, -1.1433e-01,\n",
       "         -1.1876e-01,  1.0925e-02, -1.7775e-01,  1.2559e-01, -9.5188e-02,\n",
       "          1.8911e-01, -1.6210e-01,  1.7513e-01,  6.6278e-02,  1.8180e-01,\n",
       "         -1.4152e-02, -1.4652e-01,  1.0260e-01,  2.1799e-01, -2.0060e-01,\n",
       "         -5.7577e-02,  8.1059e-02, -1.0726e-01, -1.1937e-01,  1.2098e-01,\n",
       "         -2.3286e-01, -7.6218e-02, -1.1551e-02, -3.5026e-02,  9.7753e-02]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xavier initialization \n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.nn.Sequential을 이용하여 model 정의하기(linear->batch norm layer -> relu -> dropout)\n",
    "bn_model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n",
    "                            linear2, bn2, relu, dropout,\n",
    "                            linear3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(bn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost 계산을 위한 변수설정\n",
    "train_total_batch=len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.507275224\n",
      "Epoch: 0002 cost = 0.367212802\n",
      "Epoch: 0003 cost = 0.341287643\n",
      "Epoch: 0004 cost = 0.316315740\n",
      "Epoch: 0005 cost = 0.307551503\n",
      "Epoch: 0006 cost = 0.289353013\n",
      "Epoch: 0007 cost = 0.279490441\n",
      "Epoch: 0008 cost = 0.260232151\n",
      "Epoch: 0009 cost = 0.253042072\n",
      "Epoch: 0010 cost = 0.259918630\n",
      "Epoch: 0011 cost = 0.248664975\n",
      "Epoch: 0012 cost = 0.256456763\n",
      "Epoch: 0013 cost = 0.241914690\n",
      "Epoch: 0014 cost = 0.241396412\n",
      "Epoch: 0015 cost = 0.236278251\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "#Training epoch (cost 값 초기설정(0으로)과 model의 train 설정 꼭 할 것)\n",
    "bn_model.train()\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    #train dataset을 불러오고 (X, Y 불러오기), back propagation 과 optimizer를 사용하여 loss 최적화\n",
    "    for X, Y in data_loader:\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        # label is not one-hot encoded\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = bn_model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / train_total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9302999973297119\n",
      "Label:  8\n",
      "Prediction:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "#test set으로 모델의 정확도를 검증하는 코드(model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view 를 사용하여 차원 변환할 것/ Y_test를 불러올 때 labels 사용\n",
    "#accuracy의 초기값 설정(0으로) 꼭 할 것\n",
    "# Test model and check accuracy\n",
    "with torch.no_grad():\n",
    "    bn_model.eval()    # set the model to evaluation mode (dropout=False)\n",
    "\n",
    "    # Test the model using test sets\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = bn_model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "\n",
    "    #test set에서 random으로 data를 뽑아, label과 prediction을 비교하는 코드\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = bn_model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layering(조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), relu 사용, batch normalization)\n",
    "#각 layer의 hidden node 수: (784,200), (200,150),(150,10)\n",
    "linear1 = torch.nn.Linear(784, 200, bias=True)\n",
    "linear2 = torch.nn.Linear(200, 150, bias=True)\n",
    "linear3 = torch.nn.Linear(150, 10, bias=True)\n",
    "relu=torch.nn.ReLU()\n",
    "\n",
    "bn1 = torch.nn.BatchNorm1d(200)\n",
    "bn2 = torch.nn.BatchNorm1d(150)\n",
    "dropout = torch.nn.Dropout(p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0732, -0.1406,  0.1581,  ...,  0.0917,  0.1545, -0.0351],\n",
       "        [ 0.0092, -0.0431,  0.1504,  ..., -0.0383,  0.0031, -0.1123],\n",
       "        [ 0.0855, -0.1348, -0.1492,  ..., -0.0406,  0.1810,  0.0392],\n",
       "        ...,\n",
       "        [ 0.0528, -0.1253, -0.1901,  ..., -0.1609,  0.1377, -0.0422],\n",
       "        [ 0.1061,  0.0090,  0.1650,  ..., -0.0336, -0.1621,  0.1535],\n",
       "        [-0.0260, -0.0480, -0.0255,  ..., -0.0064,  0.1248,  0.0955]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xavier initialization \n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.nn.Sequential을 이용하여 model 정의하기(linear->batch norm layer -> relu -> dropout)\n",
    "bn_model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n",
    "                            linear2, bn2, relu, dropout,\n",
    "                            linear3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(bn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost 계산을 위한 변수설정\n",
    "train_total_batch=len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.467206866\n",
      "Epoch: 0002 cost = 0.334420025\n",
      "Epoch: 0003 cost = 0.295281202\n",
      "Epoch: 0004 cost = 0.273213476\n",
      "Epoch: 0005 cost = 0.256688237\n",
      "Epoch: 0006 cost = 0.244146928\n",
      "Epoch: 0007 cost = 0.232485130\n",
      "Epoch: 0008 cost = 0.226971209\n",
      "Epoch: 0009 cost = 0.219768703\n",
      "Epoch: 0010 cost = 0.221625179\n",
      "Epoch: 0011 cost = 0.207883760\n",
      "Epoch: 0012 cost = 0.213482812\n",
      "Epoch: 0013 cost = 0.199974164\n",
      "Epoch: 0014 cost = 0.193404630\n",
      "Epoch: 0015 cost = 0.194228932\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "#Training epoch (cost 값 초기설정(0으로)과 model의 train 설정 꼭 할 것)\n",
    "bn_model.train()\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    #train dataset을 불러오고 (X, Y 불러오기), back propagation 과 optimizer를 사용하여 loss 최적화\n",
    "    for X, Y in data_loader:\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        # label is not one-hot encoded\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = bn_model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / train_total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8583999872207642\n",
      "Label:  5\n",
      "Prediction:  5\n"
     ]
    }
   ],
   "source": [
    "#test set으로 모델의 정확도를 검증하는 코드(model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view 를 사용하여 차원 변환할 것/ Y_test를 불러올 때 labels 사용\n",
    "#accuracy의 초기값 설정(0으로) 꼭 할 것\n",
    "# Test model and check accuracy\n",
    "with torch.no_grad():\n",
    "    bn_model.eval()    # set the model to evaluation mode (dropout=False)\n",
    "\n",
    "    # Test the model using test sets\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = bn_model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "\n",
    "    #test set에서 random으로 data를 뽑아, label과 prediction을 비교하는 코드\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = bn_model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
