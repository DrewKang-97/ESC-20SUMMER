{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020 SUMMER ESC :: Week 2 세션 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2020.08.02 김채형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-1)\n",
    "\n",
    "아래에 주어진 주석을 기반으로 하여 코딩을 해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51175ee53ffa421394c89ba9cd4dcb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62332cf223104b03ab30d4527d63b9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb3aea2e54c4108a9f370f27916323c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4a6bde02e04326baebb29bac16ca85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/distiller/project/conda/conda-bld/pytorch_1591914925853/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "## train set과 test set으로 나누어 MNIST data 불러오기\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/', \n",
    "                          train=True, \n",
    "                          transform=transforms.ToTensor(), \n",
    "                          download=True)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False, \n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "## dataset loader에 train과 test 할당하기 (batch size, shuffle, drop_last 잘 설정할 것!)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True, \n",
    "                                           drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True, \n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn layers\n",
    "## Layer 쌓기\n",
    "## 조건 : 3개의 Layer 사용 / DropOut 사용 (p=0.3) / ReLU 함수 사용 / Batch normalization 하기  \n",
    "## 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100), 3rd Layer(100,10)\n",
    "linear1 = torch.nn.Linear(784, 100, bias=True)\n",
    "linear2 = torch.nn.Linear(100, 100, bias=True)\n",
    "linear3 = torch.nn.Linear(100, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "bn1 = torch.nn.BatchNorm1d(100)\n",
    "bn2 = torch.nn.BatchNorm1d(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-3.8792e-02, -1.5501e-01,  1.2453e-01,  1.4407e-01, -2.0475e-01,\n",
       "         -1.5531e-01,  1.8045e-01, -1.8666e-01, -6.5143e-02, -8.0612e-02,\n",
       "         -1.9018e-01, -2.8504e-03, -1.6596e-01, -1.6611e-01,  8.5252e-02,\n",
       "         -1.9500e-01, -6.9105e-02,  7.5690e-02,  2.0679e-01,  1.3518e-01,\n",
       "         -1.3819e-01,  4.2981e-02, -2.2658e-01,  1.6310e-01,  5.9000e-02,\n",
       "          1.2462e-01, -2.2421e-01,  1.3168e-01, -3.1367e-02,  1.6436e-01,\n",
       "         -2.3227e-01,  1.6357e-01,  1.0269e-01, -1.6034e-01,  2.0724e-01,\n",
       "          1.7530e-01,  7.5835e-02,  6.8787e-02, -1.6235e-01, -1.0325e-01,\n",
       "          1.7413e-02,  1.4418e-01, -2.2946e-01,  6.3656e-02,  1.2483e-01,\n",
       "         -1.3916e-01,  1.2382e-01,  1.0330e-01, -1.7153e-02,  1.0329e-01,\n",
       "         -1.8718e-01,  1.5454e-01, -2.0483e-01, -1.2387e-02,  6.7946e-03,\n",
       "         -2.0540e-01,  8.1489e-02,  1.5138e-01, -1.0487e-01, -7.2547e-02,\n",
       "          2.1172e-01,  2.2560e-01,  1.7233e-01,  6.5866e-02,  8.2770e-03,\n",
       "         -1.3090e-02, -2.2176e-01, -6.1365e-02, -2.2222e-01,  9.3992e-03,\n",
       "          1.1244e-01, -8.2272e-02,  2.1142e-01,  1.8321e-01, -1.2263e-01,\n",
       "         -1.9405e-01,  6.4235e-02, -1.6483e-01, -3.9673e-02,  9.8507e-02,\n",
       "         -1.6487e-01,  1.6365e-01,  4.7108e-02,  1.8761e-01, -2.0515e-01,\n",
       "         -1.5393e-01, -4.6972e-02, -9.8822e-02,  9.2022e-02,  9.4694e-02,\n",
       "          1.0183e-01, -1.9226e-02,  4.2588e-02, -1.7388e-02,  1.0906e-01,\n",
       "         -1.0808e-01,  5.7996e-02,  7.8326e-02,  1.5720e-01, -2.1936e-01],\n",
       "        [-6.3436e-02, -4.6470e-03, -1.3958e-01, -1.0357e-01,  1.2119e-01,\n",
       "          2.0252e-01,  1.3898e-01,  1.5637e-01,  3.2374e-02, -2.3330e-01,\n",
       "          1.5698e-01,  7.2622e-02,  2.3312e-01,  2.3334e-02, -1.1251e-02,\n",
       "          9.5374e-02, -1.1672e-01,  1.6226e-01,  4.0139e-02,  1.4830e-01,\n",
       "          1.9801e-02,  2.0479e-01,  2.7872e-02, -4.4664e-02,  1.9055e-01,\n",
       "          2.2420e-01,  2.1024e-01, -1.2264e-01, -1.6258e-01, -1.3130e-01,\n",
       "         -1.2106e-01, -5.1735e-02, -7.0131e-02,  1.9333e-01,  4.5776e-02,\n",
       "          1.0431e-01,  9.0408e-02,  1.2953e-01, -1.6501e-02, -4.7883e-02,\n",
       "          6.7500e-02, -2.5190e-02, -3.3019e-02, -2.0863e-01,  2.1864e-01,\n",
       "          1.1099e-01,  1.9306e-01,  7.8000e-02, -9.7166e-02,  9.2597e-02,\n",
       "         -1.0360e-01,  2.3348e-01, -1.6803e-01,  1.4605e-01, -2.0260e-01,\n",
       "          1.1205e-03,  1.4939e-02,  1.0052e-01,  1.0608e-01,  7.5439e-02,\n",
       "         -2.4747e-02, -1.6762e-01, -1.3855e-01,  1.7095e-01,  1.3386e-01,\n",
       "         -1.2526e-01,  6.9682e-02, -1.4836e-01, -1.7472e-01,  2.0284e-01,\n",
       "         -1.6459e-01, -1.9166e-01,  1.4384e-01, -9.1207e-03,  8.3410e-02,\n",
       "         -1.5522e-02, -1.0061e-02,  6.3248e-02,  6.9633e-02,  1.4629e-01,\n",
       "          1.9595e-01,  8.5817e-02, -6.6230e-02,  2.1803e-02, -4.6399e-02,\n",
       "         -3.1667e-02, -1.5443e-01,  2.1353e-01, -1.4580e-01, -2.0022e-01,\n",
       "          1.9172e-01, -1.7298e-02,  1.8056e-01, -2.2315e-01, -1.6715e-01,\n",
       "         -1.4865e-01, -1.2463e-01, -2.5241e-03,  2.9666e-02,  1.8400e-01],\n",
       "        [-2.2291e-01,  1.7144e-01,  9.8985e-02,  1.8944e-01, -2.0402e-01,\n",
       "          2.2235e-01, -1.4323e-01,  1.4616e-01, -5.5173e-02, -1.9018e-01,\n",
       "          9.1294e-02,  1.7348e-01,  1.3314e-01, -4.1350e-02, -2.3852e-02,\n",
       "          7.8933e-02, -2.0824e-01, -1.4617e-01, -6.1857e-02,  8.6315e-03,\n",
       "         -2.1655e-01, -2.1278e-01,  2.3052e-01,  5.6045e-02, -9.3965e-02,\n",
       "          1.5113e-01, -1.7687e-01, -7.2729e-02,  1.7731e-01, -2.1922e-01,\n",
       "          1.0719e-01, -1.0305e-01,  4.1885e-02,  2.0179e-01,  1.3482e-01,\n",
       "         -2.7345e-02,  9.9686e-02,  5.1723e-02,  8.3095e-02,  5.8094e-04,\n",
       "          3.3253e-02,  1.5436e-01, -2.8509e-02, -1.9826e-01, -7.8304e-02,\n",
       "         -1.0642e-01, -1.2854e-01, -1.8876e-01, -1.8055e-03, -1.5599e-01,\n",
       "         -1.5150e-01,  1.5850e-01,  2.0331e-01, -1.1179e-01, -1.9209e-01,\n",
       "         -1.5546e-01, -1.6121e-01,  9.6333e-02,  1.0966e-01, -2.0021e-01,\n",
       "          6.7725e-02,  1.6296e-01, -8.6038e-03, -3.8281e-02,  1.7207e-01,\n",
       "          8.7019e-02, -1.4296e-01,  1.3251e-01, -1.5754e-01,  2.7987e-02,\n",
       "          2.2195e-01, -1.6383e-01,  1.5939e-01, -7.1953e-02, -2.2675e-02,\n",
       "         -1.2157e-01, -8.6042e-02,  2.0238e-02,  1.7661e-01,  2.2602e-01,\n",
       "         -1.8467e-01,  1.1253e-01, -2.2388e-01, -3.3611e-02,  1.4257e-01,\n",
       "          1.9634e-01, -9.3817e-02, -1.4240e-01,  1.4049e-01, -2.1432e-02,\n",
       "         -2.9914e-02, -4.5117e-02, -3.7673e-02, -5.8890e-02,  2.0004e-01,\n",
       "         -4.8753e-02, -7.2562e-02, -9.2347e-03, -2.7246e-02, -2.2619e-01],\n",
       "        [-1.7916e-03, -1.5657e-01,  1.6295e-01,  8.5181e-02, -2.1174e-01,\n",
       "          2.2805e-01, -4.2498e-02, -7.4725e-02, -1.3730e-01,  6.3555e-02,\n",
       "         -6.0832e-03, -6.4047e-02,  2.1675e-01,  9.8516e-04, -1.3554e-01,\n",
       "         -1.1372e-01,  1.2736e-01,  1.4739e-01,  5.8719e-02, -7.1756e-02,\n",
       "          1.5452e-01, -8.8896e-02,  1.5559e-01,  9.3988e-02, -1.2919e-01,\n",
       "          7.7983e-03,  9.1518e-02,  1.2889e-01, -2.1003e-01, -7.0723e-03,\n",
       "          5.4195e-02, -2.1748e-01,  1.4365e-02,  1.8872e-02,  2.4860e-02,\n",
       "          6.9934e-03,  1.0123e-01,  2.3006e-01, -1.7694e-01, -2.3252e-01,\n",
       "         -1.5823e-01, -7.3163e-02,  1.3549e-01, -6.7499e-02, -1.5664e-01,\n",
       "          1.1361e-01,  4.7744e-02,  1.9506e-01,  4.0987e-02,  2.1201e-01,\n",
       "          1.3656e-01,  1.2796e-01,  4.6087e-02, -1.3029e-01,  1.1979e-01,\n",
       "          2.0170e-01,  9.6923e-02,  8.6582e-02, -1.3332e-01,  7.7343e-02,\n",
       "         -7.9636e-02, -1.9425e-01,  2.0468e-01, -3.8688e-02,  2.4118e-02,\n",
       "          3.4185e-03,  1.1645e-01,  1.0717e-01, -1.0290e-01,  1.1947e-01,\n",
       "          9.2088e-02, -1.0858e-01,  6.3161e-02,  1.0119e-02, -2.2064e-01,\n",
       "          2.0928e-01, -1.7590e-01,  1.2156e-01, -2.1538e-01,  1.5930e-01,\n",
       "          1.9610e-03, -5.9531e-03, -1.2351e-01, -5.9816e-02, -2.1070e-03,\n",
       "          1.4206e-01, -4.7844e-02, -1.7509e-01, -1.1464e-01, -3.6042e-02,\n",
       "         -5.2723e-03, -1.5270e-01,  5.4016e-02,  1.9788e-02, -1.2648e-01,\n",
       "         -1.5141e-01, -1.3925e-03, -6.9876e-02, -2.0433e-02, -2.7382e-02],\n",
       "        [ 1.2858e-01, -1.2945e-02,  2.3133e-01, -1.0591e-01,  1.9044e-01,\n",
       "         -2.1795e-01, -1.5169e-01, -1.3852e-01,  2.2487e-02,  4.5291e-02,\n",
       "          2.9711e-02,  3.9283e-02,  1.0442e-01,  7.2557e-02, -2.2793e-02,\n",
       "         -1.1406e-01,  1.2909e-01, -1.2848e-01,  5.2532e-02, -1.5736e-01,\n",
       "          9.4216e-02,  7.4739e-02, -1.8708e-01,  2.0070e-01, -8.3638e-02,\n",
       "         -1.2371e-01, -2.1077e-01,  1.0373e-02, -5.5286e-02, -1.1238e-01,\n",
       "          9.0753e-02,  1.4291e-01, -1.7448e-01,  1.7846e-01, -8.9747e-02,\n",
       "         -2.5421e-04,  1.2878e-01, -5.4072e-02, -5.9529e-03,  2.1483e-02,\n",
       "         -1.5880e-01, -2.0945e-01, -1.9160e-01, -7.4800e-02,  1.3196e-01,\n",
       "          1.1263e-01, -1.9989e-01,  9.1360e-03, -2.1072e-01,  2.2369e-01,\n",
       "         -1.6870e-01, -2.2237e-01,  1.5303e-03,  8.7134e-02, -4.6999e-02,\n",
       "         -1.7649e-01, -2.1107e-01,  2.2604e-01,  1.2826e-01,  1.3334e-02,\n",
       "          1.0846e-02, -1.6841e-01, -6.6234e-02,  2.2143e-01,  2.0644e-01,\n",
       "          1.0852e-01, -6.2113e-02, -1.1422e-01, -2.0580e-01, -1.0844e-01,\n",
       "          1.6626e-02, -2.2647e-01, -1.8825e-01, -4.8190e-03, -1.0926e-01,\n",
       "         -1.4466e-01,  1.9123e-01,  1.5474e-02,  2.4276e-02,  2.0885e-02,\n",
       "         -1.1662e-01, -9.6443e-02,  1.3886e-01,  7.5710e-02,  1.7229e-01,\n",
       "          2.1972e-01,  1.0696e-02,  1.1336e-01,  2.0893e-01,  9.7957e-02,\n",
       "          8.1904e-02, -1.1671e-01, -1.7018e-01, -4.3984e-02,  9.5055e-02,\n",
       "         -1.7449e-01, -1.4631e-01, -1.5157e-01, -2.0271e-01,  1.1240e-01],\n",
       "        [-1.0335e-01,  1.6375e-01,  1.6481e-01,  1.0320e-01,  7.0014e-02,\n",
       "          5.9953e-02, -2.3254e-01,  8.7481e-02, -5.6925e-02,  4.7179e-02,\n",
       "          1.9015e-01,  2.2764e-01, -8.5934e-02, -2.0494e-01,  1.3842e-01,\n",
       "         -9.7284e-02,  5.1302e-02, -3.2023e-02, -1.8413e-01, -5.2126e-02,\n",
       "         -1.0644e-01,  1.0405e-01, -2.2592e-01,  1.3076e-01, -1.9295e-01,\n",
       "          1.7051e-01, -1.9700e-01,  8.7705e-03, -1.9069e-01, -1.7151e-01,\n",
       "         -5.2032e-02, -1.5801e-01, -2.2218e-01, -6.7740e-02,  1.0038e-01,\n",
       "          1.1126e-01, -7.6607e-02,  1.1509e-03,  2.3261e-01,  1.2181e-01,\n",
       "         -9.4042e-02,  5.6201e-02, -1.6179e-01,  1.3844e-01, -1.7084e-01,\n",
       "         -2.0304e-01,  8.1024e-02, -5.4919e-02, -1.2760e-01,  3.6063e-02,\n",
       "         -5.0310e-02,  1.0662e-02, -2.1223e-01, -1.0874e-01, -2.0319e-01,\n",
       "         -8.5789e-02, -1.3968e-01, -1.8029e-01, -2.1564e-01, -7.4691e-02,\n",
       "         -2.2834e-01, -1.4426e-03,  1.3766e-01,  1.2097e-02,  2.0968e-01,\n",
       "         -1.0630e-01, -9.7562e-02, -4.9511e-02, -1.8576e-01, -7.7086e-02,\n",
       "          1.3571e-01, -1.3595e-01,  1.3307e-01, -6.1591e-02, -6.3505e-02,\n",
       "          5.4010e-02,  1.7165e-01,  2.1838e-01, -4.4362e-02,  1.0645e-02,\n",
       "         -1.7433e-01,  2.2720e-01, -2.2122e-01,  1.7678e-01, -2.4521e-02,\n",
       "          2.2518e-01,  4.0264e-02, -7.8423e-02,  9.4616e-02,  8.3997e-03,\n",
       "          1.9005e-01, -1.7758e-01,  2.2085e-01, -2.1760e-01,  1.9863e-01,\n",
       "          9.2416e-02, -3.9709e-02, -3.9380e-02,  1.0745e-01, -1.6578e-01],\n",
       "        [-5.7206e-02, -1.9442e-01,  2.2157e-01, -3.8172e-03, -1.9432e-01,\n",
       "         -5.0178e-02,  7.0739e-02,  1.2826e-02,  1.7509e-01,  2.0662e-01,\n",
       "         -7.2670e-02,  1.2504e-01,  1.8517e-01, -9.4008e-02,  1.7980e-01,\n",
       "          1.6717e-03, -4.6534e-02, -2.1350e-01, -2.1734e-01, -2.2753e-01,\n",
       "          1.9064e-01,  5.3566e-02, -1.9924e-01, -1.5972e-01,  2.1006e-01,\n",
       "          1.4718e-01,  1.5801e-01,  1.5019e-01,  1.8970e-01,  2.1569e-01,\n",
       "          1.4927e-01, -1.2583e-02,  1.0663e-01,  6.3722e-02,  7.6946e-02,\n",
       "          2.0126e-01, -1.8519e-01, -8.5586e-02,  3.9884e-02,  3.0763e-02,\n",
       "         -7.0915e-02, -2.2784e-01,  1.5967e-01, -2.2265e-01, -1.7890e-01,\n",
       "          2.2063e-03,  1.0120e-01,  9.2995e-02,  8.8768e-02,  1.7316e-01,\n",
       "          1.2211e-01, -2.1412e-01, -2.9695e-02,  1.0599e-01, -2.0855e-01,\n",
       "         -7.2386e-02,  7.5546e-02,  9.9788e-02, -1.4872e-01, -3.4269e-02,\n",
       "          7.9867e-02,  1.4161e-01, -5.2005e-05,  5.3431e-02, -7.5841e-02,\n",
       "         -1.0683e-01,  9.3849e-02, -2.2887e-01, -1.2280e-01,  2.2455e-01,\n",
       "         -2.2233e-01,  2.2353e-01, -3.7655e-02,  1.5402e-02, -2.0513e-01,\n",
       "         -8.1092e-02,  1.7113e-01, -2.1293e-01, -2.2401e-01, -1.8995e-01,\n",
       "          1.6487e-01, -9.9180e-02,  1.2983e-01, -1.9776e-01,  2.8888e-02,\n",
       "          7.2701e-02, -2.1791e-01, -2.3284e-01,  1.0454e-01,  2.3171e-01,\n",
       "         -7.2030e-02, -4.1063e-02,  2.7314e-02, -9.1425e-03,  2.2239e-01,\n",
       "          2.0262e-02,  3.1592e-02,  1.0488e-01, -3.1549e-03, -8.8373e-02],\n",
       "        [ 2.3244e-01, -4.6694e-02, -1.2571e-01,  1.5142e-02, -2.2578e-01,\n",
       "         -2.1208e-01,  1.3662e-02,  4.7734e-02, -1.3666e-01,  1.3643e-01,\n",
       "         -8.2285e-02,  1.1501e-01, -3.1823e-02, -1.7109e-01,  1.5063e-01,\n",
       "         -1.7853e-01, -1.9256e-01, -1.5552e-01, -1.9025e-01, -2.2725e-01,\n",
       "         -2.1964e-01, -1.0080e-01, -2.2313e-01, -1.5746e-01, -7.4114e-02,\n",
       "         -6.8979e-02, -9.8570e-02,  5.5288e-03, -9.3001e-02,  2.2275e-01,\n",
       "          1.7933e-01,  2.2021e-01,  1.8066e-02,  2.1140e-01, -1.1640e-01,\n",
       "          2.2434e-02, -1.3575e-01,  1.7366e-01, -1.3895e-01, -9.8527e-02,\n",
       "          1.1262e-01,  1.6575e-01,  8.4121e-02,  1.9918e-01, -1.0113e-01,\n",
       "          2.9771e-02,  1.3766e-01, -1.5288e-01,  4.2424e-02, -1.2883e-01,\n",
       "         -1.1764e-01, -7.4903e-03,  8.5304e-02,  2.0541e-01, -1.2997e-01,\n",
       "         -1.5261e-01, -1.0624e-01, -1.7834e-01,  1.2592e-01,  1.8851e-01,\n",
       "          2.2742e-01,  1.0815e-01,  1.3754e-01,  1.5994e-01, -1.3061e-01,\n",
       "         -8.9020e-02, -2.1425e-01, -1.9676e-01, -1.1853e-01,  6.9043e-02,\n",
       "          8.3859e-03,  1.8033e-01, -3.5167e-02,  2.2138e-01, -1.1529e-02,\n",
       "         -8.2606e-02, -7.1918e-02,  8.8710e-03, -2.4302e-02, -1.3040e-01,\n",
       "         -7.5557e-02, -2.0840e-01,  2.3098e-01, -8.1065e-02, -1.8166e-01,\n",
       "         -1.4538e-01,  1.8072e-02, -2.1592e-01, -8.5026e-02, -1.9226e-01,\n",
       "         -3.8164e-03, -1.1406e-01,  1.1441e-01,  1.7200e-01,  1.7859e-01,\n",
       "          3.6224e-02, -1.3054e-01, -5.5099e-03, -1.6859e-01, -5.1855e-02],\n",
       "        [ 1.9345e-01, -1.9203e-01, -7.3063e-02,  4.9379e-02,  1.8762e-01,\n",
       "         -8.9547e-02,  1.8690e-01, -1.1363e-01, -1.3597e-01, -6.1040e-02,\n",
       "         -4.0444e-02, -4.4659e-02, -2.1489e-02,  9.9443e-04, -1.7034e-01,\n",
       "         -1.6082e-01,  1.6590e-01,  3.8888e-03, -9.0686e-02, -1.7760e-01,\n",
       "         -1.4828e-01, -5.1733e-02,  9.7578e-02, -1.5300e-01, -1.0383e-01,\n",
       "          2.1047e-01,  2.2673e-01, -1.6935e-01,  1.6082e-01, -8.9659e-02,\n",
       "          1.0307e-01, -1.3576e-01,  1.3754e-01,  1.3512e-01, -1.5600e-01,\n",
       "         -2.3077e-01,  4.6232e-02,  1.8970e-01, -5.8103e-02,  1.1799e-01,\n",
       "          5.2446e-02, -1.1189e-01, -5.3605e-02, -1.5752e-01,  1.9785e-01,\n",
       "         -2.1761e-01, -1.8590e-01,  2.2035e-01, -1.0063e-01, -1.8337e-01,\n",
       "          1.9837e-01, -4.0508e-02, -1.6886e-01,  9.8259e-02, -1.2868e-01,\n",
       "         -6.2934e-03,  6.4679e-02, -6.3602e-02, -2.1389e-01,  9.7882e-02,\n",
       "         -5.8241e-02, -3.6384e-02, -3.6353e-03,  8.8197e-02,  1.7915e-01,\n",
       "          1.3997e-01,  7.2390e-02, -2.1422e-01, -2.2429e-01, -1.8718e-01,\n",
       "         -1.8472e-01,  4.6685e-02,  5.0192e-02, -8.0789e-02, -4.3919e-02,\n",
       "          1.1491e-01,  2.2274e-01, -2.2314e-01,  2.1794e-01,  2.1359e-01,\n",
       "         -1.2363e-01,  6.4126e-02, -1.2286e-01,  1.8794e-01,  4.6874e-02,\n",
       "          1.9072e-01, -1.4328e-01, -1.2877e-01, -1.7872e-01, -9.8424e-02,\n",
       "          1.5946e-01,  4.6695e-02, -8.2120e-02, -1.4770e-01, -2.1077e-01,\n",
       "         -1.0421e-01,  8.3819e-02,  2.0422e-01, -1.5152e-01,  6.6815e-02],\n",
       "        [-2.4390e-02, -9.7792e-02, -1.7341e-01, -5.8991e-02, -3.4340e-02,\n",
       "         -2.0767e-01,  4.1421e-02, -8.2280e-03, -1.5081e-01, -1.6347e-01,\n",
       "         -1.4635e-01,  3.3757e-02,  1.4076e-01, -2.2301e-01,  1.1979e-01,\n",
       "          1.3248e-01,  1.4980e-01, -2.2109e-01,  1.8875e-01,  2.0541e-02,\n",
       "          4.8311e-02, -2.1475e-01,  2.1386e-02, -4.0529e-02, -1.6352e-02,\n",
       "          1.0230e-01,  5.7305e-02,  1.1864e-01, -1.9928e-01,  2.1677e-02,\n",
       "         -8.2385e-02,  2.0605e-01,  2.1714e-01, -1.2265e-01,  5.6100e-02,\n",
       "          1.3276e-01, -2.0685e-01,  1.7566e-01,  8.0007e-03, -2.9210e-02,\n",
       "         -2.0825e-01,  2.2803e-01, -1.6962e-01,  1.2134e-01, -5.9370e-02,\n",
       "          4.5730e-02, -5.6906e-02,  1.0362e-01,  1.7626e-01,  3.8279e-02,\n",
       "         -6.7823e-02, -1.2312e-01, -6.7765e-02,  7.7070e-02,  8.2724e-02,\n",
       "          1.1195e-01,  5.1072e-02, -1.9682e-01,  1.5950e-01, -1.6336e-01,\n",
       "          1.7549e-01, -1.1473e-01, -1.5829e-01,  3.6989e-02, -1.7653e-01,\n",
       "         -2.0784e-01, -1.5208e-01, -3.2382e-02, -2.1672e-01, -1.8933e-01,\n",
       "         -3.8062e-02, -1.1890e-01, -1.0234e-01,  1.2827e-01,  2.1527e-01,\n",
       "         -1.7302e-01,  2.1745e-01,  1.5114e-01,  1.1118e-01,  4.8603e-02,\n",
       "         -3.8392e-02,  2.1361e-01, -8.8179e-02,  2.2736e-01, -5.1987e-02,\n",
       "         -1.6522e-01, -1.4679e-01, -1.5546e-01,  8.0348e-02,  9.2567e-02,\n",
       "          2.2759e-01, -1.1615e-01,  1.2654e-01, -1.5128e-01, -3.4333e-02,\n",
       "         -1.5026e-01,  1.7299e-01, -1.0624e-01, -1.2789e-01, -6.5278e-02]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight initialization : xavier initialization\n",
    "## xavier initialization을 이용하여 각 layer의 weight 초기화\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "## torch.nn.Sequential을 이용하여 model 정의하기\n",
    "## 쌓는 순서: linear - Batch Normalization Layer - ReLU - DropOut\n",
    "model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n",
    "                            linear2, bn2, relu, dropout,\n",
    "                            linear3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function & optimizer\n",
    "## Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "## optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.512693763\n",
      "Epoch: 0002 cost = 0.379160732\n",
      "Epoch: 0003 cost = 0.328012943\n",
      "Epoch: 0004 cost = 0.305955201\n",
      "Epoch: 0005 cost = 0.295698673\n",
      "Epoch: 0006 cost = 0.285342395\n",
      "Epoch: 0007 cost = 0.279721856\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "## cost 계산을 위한 변수 설정\n",
    "train_total_batch = len(train_loader)\n",
    "\n",
    "## Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것)\n",
    "for epoch in range(training_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    avg_cost = 0\n",
    "        \n",
    "    ## train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
    "    for X, Y in train_loader:\n",
    "        \n",
    "        # X, Y reshape\n",
    "        X = X.view(-1, 28*28)\n",
    "        Y = Y\n",
    "        \n",
    "        optimizer.zero_grad() # 0으로 초기화\n",
    "        \n",
    "        # H(x) 계산\n",
    "        prediction = model(X)\n",
    "        \n",
    "        # Cost 계산\n",
    "        loss = criterion(prediction, Y)\n",
    "        \n",
    "        # Cost 통해 H(x) 개선\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        avg_cost += loss / train_total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "## test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "## X_test 불러올 때 view를 사용하여 차원 변환할 것 / Y_test를 불러올때 labels 사용\n",
    "## accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "with torch.no_grad(): \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    accuracy = 0\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1, 28*28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "    \n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "\n",
    "    print(\"Accuracy: \", accuracy.item())\n",
    "    \n",
    "    ## Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-2)\n",
    "\n",
    "지금까지는 Layer의 수를 바꾸거나, Batch Normalization Layer를 추가하는 등 Layer에만 변화를 주며 모델의 성능을 향상 시켰습니다.  \n",
    "이번 문제에서는 위에서 만든 모델에서 있던 Layer 들의 Hidden node 수를 증가 또는 감소 (ex: 200, 300, 50...) 시켰을 때, train set에서의 cost와 test set에서 Accuracy가 기존 결과와 비교하였을 때 어떻게 달라졌는지 비교해주시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hidden node = 75**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# data\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/', train=True, transform=transforms.ToTensor(), download=False)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 75, bias=True)\n",
    "linear2 = torch.nn.Linear(75, 75, bias=True)\n",
    "linear3 = torch.nn.Linear(75, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "bn1 = torch.nn.BatchNorm1d(75)\n",
    "bn2 = torch.nn.BatchNorm1d(75)\n",
    "\n",
    "# weight initialization : xavier initialization\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "\n",
    "# model\n",
    "model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n",
    "                            linear2, bn2, relu, dropout,\n",
    "                            linear3)\n",
    "\n",
    "# Loss function & Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "train_total_batch = len(train_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    \n",
    "    model.train() # For batch normalization\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X, Y in train_loader:\n",
    "        \n",
    "        # X, Y reshape\n",
    "        X = X.view(-1, 28*28)\n",
    "        Y = Y\n",
    "        \n",
    "        optimizer.zero_grad() # 0으로 초기화\n",
    "        \n",
    "        # H(x) 계산\n",
    "        prediction = model(X)\n",
    "        \n",
    "        # Cost 계산\n",
    "        loss = criterion(prediction, Y)\n",
    "        \n",
    "        # Cost 통해 H(x) 개선\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        avg_cost += loss / train_total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished')\n",
    "print('\\n')\n",
    "# Test the model using test sets\n",
    "with torch.no_grad(): \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1, 28*28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "    \n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print(\"Accuracy: \", accuracy.item())\n",
    "    \n",
    "    # Get one and predict\n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hidden node = 50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# data\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/', train=True, transform=transforms.ToTensor(), download=False)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 50, bias=True)\n",
    "linear2 = torch.nn.Linear(50, 50, bias=True)\n",
    "linear3 = torch.nn.Linear(50, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "bn1 = torch.nn.BatchNorm1d(50)\n",
    "bn2 = torch.nn.BatchNorm1d(50)\n",
    "\n",
    "# weight initialization : xavier initialization\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "\n",
    "# model\n",
    "model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n",
    "                            linear2, bn2, relu, dropout,\n",
    "                            linear3)\n",
    "\n",
    "# Loss function & Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "train_total_batch = len(train_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    avg_cost = 0\n",
    "    for X, Y in train_loader:\n",
    "        \n",
    "        # X, Y reshape\n",
    "        X = X.view(-1, 28*28)\n",
    "        Y = Y\n",
    "        \n",
    "        optimizer.zero_grad() # 초기화\n",
    "        \n",
    "        # H(x) 계산\n",
    "        prediction = model(X)\n",
    "        \n",
    "        # Cost 계산\n",
    "        loss = criterion(prediction, Y)\n",
    "        \n",
    "        # Cost 통해 H(x) 개선\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        avg_cost += loss / train_total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished')\n",
    "print('\\n')\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad(): \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1, 28*28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "    \n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print(\"Accuracy: \", accuracy.item())\n",
    "    \n",
    "    # Get one and predict\n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hidden node = 25**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# data\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/', train=True, transform=transforms.ToTensor(), download=False)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 25, bias=True)\n",
    "linear2 = torch.nn.Linear(25, 25, bias=True)\n",
    "linear3 = torch.nn.Linear(25, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "bn1 = torch.nn.BatchNorm1d(25)\n",
    "bn2 = torch.nn.BatchNorm1d(25)\n",
    "\n",
    "# weight initialization : xavier initialization\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "\n",
    "# model\n",
    "model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n",
    "                            linear2, bn2, relu, dropout,\n",
    "                            linear3)\n",
    "\n",
    "# Loss function & Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "train_total_batch = len(train_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    avg_cost = 0\n",
    "    for X, Y in train_loader:\n",
    "        \n",
    "        # X, Y reshape\n",
    "        X = X.view(-1, 28*28)\n",
    "        Y = Y\n",
    "        \n",
    "        optimizer.zero_grad() # 0으로 초기화\n",
    "        \n",
    "        # H(x) 계산\n",
    "        prediction = model(X)\n",
    "        \n",
    "        # Cost 계산\n",
    "        loss = criterion(prediction, Y)\n",
    "        \n",
    "        # Cost 통해 H(x) 개선\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        avg_cost += loss / train_total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished')\n",
    "print('\\n')\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad(): \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1, 28*28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "    \n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print(\"Accuracy: \", accuracy.item())\n",
    "    \n",
    "    # Get one and predict\n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy 결과**\n",
    "\n",
    "|hidden node 개수|Cost|Accuracy|\n",
    "|---------------|----|--------|\n",
    "|25|0.495|0.836|\n",
    "|50|0.325|0.935|\n",
    "|75|0.280|0.930|\n",
    "|100|0.238|0.932|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# parameter\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# data\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/', train=True, transform=transforms.ToTensor(), download=False)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "hidden_nodes = [20, 60, 100, 140, 180, 220, 260, 300]\n",
    "acc_list = []\n",
    "for hn in hidden_nodes:\n",
    "    \n",
    "    # nn layers\n",
    "    linear1 = torch.nn.Linear(784, hn, bias=True)\n",
    "    linear2 = torch.nn.Linear(hn, hn, bias=True)\n",
    "    linear3 = torch.nn.Linear(hn, 10, bias=True)\n",
    "    relu = torch.nn.ReLU()\n",
    "    dropout = torch.nn.Dropout(p=0.3)\n",
    "    bn1 = torch.nn.BatchNorm1d(hn)\n",
    "    bn2 = torch.nn.BatchNorm1d(hn)\n",
    "    \n",
    "    # weight initialization : xavier initialization\n",
    "    torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "    torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "    torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "    \n",
    "    # model\n",
    "    model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n",
    "                                linear2, bn2, relu, dropout,\n",
    "                                linear3)\n",
    "\n",
    "    # Loss function & Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    train_total_batch = len(train_loader)\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        avg_cost = 0\n",
    "        for X, Y in train_loader:\n",
    "            \n",
    "            # X, Y reshape\n",
    "            X = X.view(-1, 28*28)\n",
    "            Y = Y\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            prediction = model(X)\n",
    "            loss = criterion(prediction, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_cost += loss / train_total_batch\n",
    "            \n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "        \n",
    "        print('Learning finished')\n",
    "        print('\\n')\n",
    "    \n",
    "    # Test the model\n",
    "    with torch.no_grad(): \n",
    "        \n",
    "        model.eval()\n",
    "        X_test = mnist_test.test_data.view(-1, 28*28).float()\n",
    "        Y_test = mnist_test.test_labels\n",
    "        \n",
    "        prediction = model(X_test)\n",
    "        correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "        accuracy = correct_prediction.float().mean()\n",
    "        acc_list.append(accuracy)\n",
    "        print(\"Accuracy: \", accuracy.item())\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
