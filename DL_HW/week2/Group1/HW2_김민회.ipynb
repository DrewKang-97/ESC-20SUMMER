{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week2: HW \n",
    "### 김민회"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 주석 기반 코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ReLU + BatchNorm](https://github.com/deeplearningzerotoall/PyTorch/blob/master/lab-09_6_mnist_batchnorm.ipynb)  \n",
    "[ReLU + Dropout](https://github.com/deeplearningzerotoall/PyTorch/blob/master/lab-09_5_mnist_nn_dropout.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정 (learning rate, training epochs, batch_size)\n",
    "\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train과 test set으로 나누어 MNIST data 불러오기\n",
    "\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n",
    "# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n",
    "\n",
    "linear1 = torch.nn.Linear(784, 100, bias=True)\n",
    "linear2 = torch.nn.Linear(100, 100, bias=True)\n",
    "linear3 = torch.nn.Linear(100, 10, bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "bn1 = torch.nn.BatchNorm1d(100)\n",
    "bn2 = torch.nn.BatchNorm1d(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0982, -0.0550,  0.2209,  0.2286,  0.0469, -0.0362,  0.1229,  0.0523,\n",
       "         -0.0064,  0.0592,  0.1908,  0.0773,  0.1596, -0.0948,  0.1055, -0.0632,\n",
       "         -0.0124, -0.0390, -0.0823,  0.0599,  0.0940,  0.0718, -0.1384,  0.1566,\n",
       "         -0.0402,  0.1256,  0.0708,  0.0563,  0.2251,  0.0763, -0.0974,  0.1039,\n",
       "          0.1944, -0.0152,  0.1894, -0.0522, -0.2156,  0.0644, -0.1813, -0.0065,\n",
       "         -0.0703, -0.2119,  0.1261, -0.0947,  0.0989, -0.1729,  0.0034, -0.1530,\n",
       "          0.1973, -0.1719, -0.2085,  0.0689,  0.0940,  0.0318, -0.2329, -0.0049,\n",
       "         -0.0749, -0.0542, -0.1351, -0.1640,  0.1477,  0.0976, -0.1833,  0.0049,\n",
       "         -0.0185, -0.1022, -0.1370, -0.0821,  0.1838,  0.2245, -0.2113, -0.0492,\n",
       "         -0.0897, -0.0105,  0.0377, -0.0337, -0.2274,  0.1602, -0.0410,  0.1693,\n",
       "         -0.0423,  0.0036,  0.0811, -0.0132,  0.0051,  0.2129, -0.1136,  0.0369,\n",
       "         -0.1363,  0.0058, -0.0194,  0.1273,  0.2252, -0.1302,  0.2307,  0.1700,\n",
       "         -0.2151,  0.1955,  0.2062, -0.1994],\n",
       "        [ 0.0551,  0.1887, -0.1181, -0.0514, -0.0986,  0.0958, -0.0307,  0.1306,\n",
       "         -0.0308, -0.0390,  0.0300, -0.1993,  0.1830,  0.0225, -0.0127,  0.1845,\n",
       "         -0.1670,  0.0200, -0.1782,  0.2129,  0.1816, -0.0774, -0.0508, -0.1199,\n",
       "          0.0796, -0.0759, -0.1573, -0.1240,  0.0197, -0.1627,  0.1719,  0.1537,\n",
       "          0.2335, -0.0784, -0.1940, -0.0105, -0.1722, -0.1644,  0.1808,  0.0035,\n",
       "          0.1503, -0.1735,  0.2172,  0.0139,  0.0964,  0.0016,  0.0732, -0.1792,\n",
       "         -0.1848, -0.1811, -0.2098,  0.0788,  0.1552, -0.1875, -0.1026, -0.2271,\n",
       "         -0.2294,  0.2163,  0.0367,  0.0842,  0.1647, -0.0506,  0.1150, -0.0126,\n",
       "          0.2242,  0.0415, -0.1141,  0.0277, -0.1153, -0.2330,  0.2334,  0.1710,\n",
       "         -0.0677, -0.1718,  0.0088,  0.0759,  0.1156, -0.0075, -0.0526,  0.0795,\n",
       "          0.0796,  0.1722, -0.1073, -0.2184, -0.2054, -0.1729, -0.1110, -0.1821,\n",
       "          0.2241,  0.2013, -0.2287,  0.1433, -0.0839, -0.1192,  0.0622,  0.1055,\n",
       "         -0.0107,  0.1925, -0.0210,  0.1874],\n",
       "        [ 0.0686, -0.1339, -0.0784,  0.1335,  0.1178, -0.1809, -0.0647, -0.2009,\n",
       "          0.1054,  0.1634, -0.0947,  0.1031, -0.1985, -0.0506, -0.0971,  0.0904,\n",
       "          0.1102, -0.1563,  0.1873,  0.0972,  0.0663,  0.0792, -0.0601,  0.2181,\n",
       "          0.0512, -0.1144,  0.0034,  0.1643, -0.1253,  0.2078, -0.0645,  0.0306,\n",
       "         -0.1878,  0.0621, -0.0633, -0.1394,  0.1698,  0.1842,  0.2316, -0.2275,\n",
       "         -0.1239, -0.1701,  0.1545,  0.0095, -0.0379, -0.1570,  0.1320,  0.0574,\n",
       "         -0.1275, -0.1325,  0.1731, -0.0682, -0.2150, -0.0361, -0.0695, -0.0800,\n",
       "         -0.2219,  0.1870, -0.0801, -0.1100,  0.1845,  0.0440, -0.1486,  0.1584,\n",
       "          0.1347, -0.1773, -0.0213,  0.0843, -0.0471, -0.0809, -0.1020, -0.1811,\n",
       "         -0.0232,  0.0613, -0.0536, -0.1559,  0.0818, -0.0895, -0.1510, -0.1107,\n",
       "         -0.1514,  0.1790, -0.2151,  0.0198, -0.1281,  0.1119, -0.1491, -0.1056,\n",
       "         -0.0336, -0.1443,  0.1119, -0.2109, -0.2072, -0.2299,  0.0313,  0.0742,\n",
       "         -0.1583, -0.1158,  0.1997, -0.0592],\n",
       "        [-0.0024, -0.0798, -0.1811,  0.0284, -0.2021, -0.1775, -0.0254,  0.1314,\n",
       "         -0.1003,  0.1145, -0.1995, -0.1989,  0.0307,  0.2217,  0.0450, -0.0600,\n",
       "          0.1832,  0.0851,  0.1479,  0.0496, -0.1427, -0.2175, -0.1324,  0.1702,\n",
       "         -0.1219,  0.0559,  0.0116, -0.0880,  0.0718, -0.2310,  0.0177, -0.1457,\n",
       "         -0.2317, -0.1702,  0.0125, -0.2276, -0.0746, -0.1550,  0.1572,  0.0827,\n",
       "         -0.1088, -0.1280,  0.2116, -0.1012, -0.1639, -0.0082,  0.1387,  0.2079,\n",
       "          0.0646,  0.1630,  0.1534,  0.1737,  0.2067, -0.0852,  0.2083,  0.0256,\n",
       "         -0.1972,  0.2113,  0.1846,  0.1461, -0.0528,  0.0105, -0.0028, -0.0428,\n",
       "         -0.0765,  0.0476, -0.2126, -0.0658,  0.1775,  0.2050,  0.2146, -0.0648,\n",
       "         -0.0450, -0.1981, -0.0905, -0.0859, -0.1967, -0.1161, -0.2207, -0.0441,\n",
       "         -0.1951,  0.1608, -0.1848,  0.1290,  0.0564,  0.1620,  0.2076, -0.2160,\n",
       "         -0.1520, -0.1503,  0.1126, -0.2089,  0.1095, -0.1114, -0.1885,  0.1438,\n",
       "          0.1578, -0.1027,  0.0081, -0.2185],\n",
       "        [-0.2000,  0.0371, -0.1985,  0.0854,  0.2048, -0.0889,  0.1451,  0.1738,\n",
       "         -0.0282,  0.1691,  0.1503, -0.1056, -0.1691,  0.0665,  0.0920, -0.2120,\n",
       "          0.1307, -0.2296, -0.2071, -0.0219,  0.1662, -0.0599, -0.0651, -0.2024,\n",
       "         -0.1470, -0.1794, -0.1587,  0.1012,  0.0497, -0.1905, -0.0798,  0.1696,\n",
       "         -0.1394, -0.0837,  0.1267, -0.1086, -0.0219,  0.2219, -0.0918,  0.1628,\n",
       "          0.0076,  0.2333,  0.1831, -0.0959, -0.1627, -0.2110, -0.1430,  0.1969,\n",
       "         -0.2244, -0.2333,  0.2050, -0.0253, -0.1362,  0.0853,  0.0378,  0.0233,\n",
       "          0.1322, -0.1263, -0.2017, -0.1743,  0.1784,  0.1747, -0.0488, -0.2108,\n",
       "         -0.1753, -0.1598,  0.1201, -0.1425, -0.1609,  0.0502, -0.0631, -0.0923,\n",
       "          0.2095, -0.0185,  0.0708,  0.1491,  0.1010, -0.2246,  0.1846, -0.1487,\n",
       "         -0.0310,  0.0084,  0.2032,  0.0082,  0.0746, -0.0670, -0.0384,  0.0306,\n",
       "          0.1091,  0.2042,  0.0917, -0.2144, -0.1964,  0.0400, -0.1683, -0.1882,\n",
       "         -0.0006, -0.1366,  0.0688, -0.0696],\n",
       "        [ 0.1931,  0.0402, -0.1670, -0.0672, -0.1707,  0.1353,  0.0497, -0.0590,\n",
       "         -0.1905,  0.0115, -0.0046,  0.0841,  0.0765, -0.0746,  0.2078, -0.1198,\n",
       "          0.1370, -0.1949,  0.1259, -0.0523,  0.2275,  0.0105, -0.2285, -0.0946,\n",
       "         -0.1034,  0.1163, -0.1104, -0.1856, -0.2291,  0.1125,  0.1416, -0.0191,\n",
       "         -0.1761,  0.0586,  0.1298,  0.2248, -0.2080, -0.1110, -0.0632, -0.1978,\n",
       "         -0.1928, -0.1026, -0.1036, -0.1570,  0.0602, -0.0620, -0.2142, -0.0927,\n",
       "         -0.0524, -0.1982, -0.0762, -0.0278, -0.1964, -0.0832,  0.1436, -0.1100,\n",
       "          0.0710, -0.1034, -0.1494,  0.0051,  0.2321,  0.0689, -0.1418, -0.0550,\n",
       "         -0.0243, -0.0109,  0.1713, -0.2232, -0.1479, -0.0882, -0.2073, -0.0423,\n",
       "          0.0724, -0.0827, -0.1257, -0.0057,  0.0495,  0.1374,  0.0504,  0.1763,\n",
       "         -0.1576, -0.2042, -0.1904, -0.0247, -0.1885,  0.2141, -0.1141,  0.0396,\n",
       "          0.0941,  0.0040,  0.1408, -0.1304, -0.1107,  0.0498,  0.0060,  0.2280,\n",
       "         -0.0813,  0.1896, -0.1537,  0.1216],\n",
       "        [-0.1371,  0.0651,  0.0471, -0.0048, -0.1110, -0.2059,  0.0461, -0.0456,\n",
       "         -0.1560,  0.2332,  0.0021, -0.1470,  0.0653, -0.0595,  0.2114, -0.2011,\n",
       "          0.0519,  0.1439,  0.1360, -0.0905,  0.1390,  0.0018,  0.1311, -0.1472,\n",
       "         -0.0746,  0.0230, -0.2276,  0.1600, -0.1327, -0.0726, -0.2055, -0.1719,\n",
       "          0.2012, -0.2314,  0.1437, -0.0230, -0.0923,  0.0902, -0.1487, -0.0460,\n",
       "          0.1874, -0.0743,  0.1311, -0.0277, -0.0570, -0.1371, -0.1367, -0.0600,\n",
       "         -0.0220,  0.0123, -0.0934, -0.0784, -0.1844,  0.2171,  0.0094,  0.0986,\n",
       "         -0.0935, -0.2335, -0.0791,  0.1932,  0.0298, -0.0270,  0.1473,  0.0777,\n",
       "         -0.1486,  0.0522, -0.0410, -0.0315, -0.0446,  0.0916, -0.1078, -0.0473,\n",
       "          0.1026,  0.0379, -0.2271,  0.1182,  0.0269,  0.1139, -0.1414, -0.1750,\n",
       "         -0.0173, -0.0279,  0.1088, -0.2115, -0.0551,  0.1067,  0.2153,  0.0120,\n",
       "         -0.1244,  0.0301,  0.1234,  0.1832,  0.1088,  0.1382, -0.0264,  0.2027,\n",
       "         -0.1061,  0.0009, -0.0262, -0.0803],\n",
       "        [-0.0062, -0.0904, -0.2159, -0.1636,  0.1631,  0.2194, -0.0047, -0.0987,\n",
       "          0.0552,  0.1206,  0.0679, -0.1804,  0.0970, -0.0730,  0.0152,  0.1278,\n",
       "          0.1401, -0.0102,  0.2000,  0.1301, -0.0121, -0.0737, -0.0770,  0.0121,\n",
       "         -0.0234,  0.0344, -0.0947, -0.1731, -0.0815, -0.1143,  0.0034,  0.2257,\n",
       "         -0.1363,  0.0159, -0.1578,  0.0517,  0.1781, -0.0275, -0.0567, -0.1396,\n",
       "         -0.2058,  0.0172,  0.1374,  0.1055,  0.1492,  0.1817, -0.1982,  0.2173,\n",
       "         -0.0836,  0.0360,  0.2036, -0.2141, -0.0384,  0.1548, -0.0793, -0.1538,\n",
       "          0.1211,  0.1703,  0.2045, -0.2288, -0.0601,  0.2280,  0.2319,  0.2317,\n",
       "          0.1258,  0.1999,  0.1931, -0.1080,  0.0785,  0.0543, -0.0176, -0.1390,\n",
       "          0.1796, -0.1120,  0.1578,  0.0872, -0.0444,  0.2318, -0.0238, -0.1670,\n",
       "         -0.2081, -0.0545,  0.1712,  0.1245,  0.1691,  0.1977,  0.0995, -0.1438,\n",
       "         -0.2268, -0.1829,  0.0621,  0.1115,  0.0176, -0.0421, -0.2317,  0.0584,\n",
       "          0.1097,  0.0360,  0.1732, -0.2045],\n",
       "        [ 0.1144, -0.2174, -0.1686,  0.1550,  0.1438, -0.1729, -0.0926, -0.2135,\n",
       "         -0.1434,  0.0175, -0.0554, -0.0632, -0.1125, -0.1769,  0.2147, -0.1536,\n",
       "          0.1024,  0.0254,  0.0257, -0.2279,  0.0897, -0.2232,  0.0181,  0.1652,\n",
       "         -0.1101, -0.0354, -0.0535, -0.2272, -0.0385,  0.0141,  0.1749, -0.1251,\n",
       "          0.1884, -0.1638, -0.0985,  0.1378,  0.2330,  0.0100, -0.1133, -0.2244,\n",
       "         -0.1346, -0.0629,  0.0832, -0.0041,  0.0256, -0.0650, -0.1487, -0.0306,\n",
       "          0.1296,  0.1217,  0.1635,  0.1551, -0.0424, -0.0981,  0.1411,  0.0388,\n",
       "         -0.0793, -0.0869, -0.0981,  0.2221, -0.0037,  0.0329, -0.1609, -0.1584,\n",
       "         -0.1872,  0.1733,  0.0543, -0.1391, -0.2150,  0.1474, -0.0666,  0.2290,\n",
       "         -0.1508, -0.0108, -0.1192,  0.0712,  0.0259,  0.1661,  0.2211,  0.0248,\n",
       "         -0.2038, -0.1251, -0.1270,  0.1184,  0.1523, -0.0435,  0.1361,  0.1385,\n",
       "          0.0614, -0.0364, -0.0927,  0.0059, -0.1223, -0.0833, -0.1952,  0.2024,\n",
       "          0.1388, -0.1521, -0.0879,  0.0452],\n",
       "        [ 0.1041,  0.0199,  0.0402,  0.2183,  0.0573, -0.1871,  0.1423,  0.1520,\n",
       "          0.2251,  0.1563, -0.1657,  0.1230, -0.2240, -0.2271, -0.1913,  0.0112,\n",
       "         -0.0650,  0.1208,  0.0840, -0.1876,  0.0669, -0.0914, -0.1791,  0.0394,\n",
       "          0.1463,  0.1014, -0.0050, -0.0585, -0.1198, -0.1216, -0.0015,  0.0036,\n",
       "          0.2084, -0.2149,  0.2053,  0.1795, -0.0576,  0.0240,  0.1289,  0.2268,\n",
       "         -0.1591, -0.0517,  0.0428,  0.0376, -0.0354,  0.1444, -0.0806, -0.0712,\n",
       "          0.0571,  0.1847,  0.1671, -0.0250,  0.1331, -0.0906, -0.1708, -0.0245,\n",
       "          0.1524,  0.0774, -0.0653,  0.0655, -0.1894,  0.0516, -0.0154, -0.1228,\n",
       "         -0.0544,  0.1599, -0.1239,  0.1427,  0.1434,  0.0395, -0.1088, -0.0651,\n",
       "          0.2167, -0.1353,  0.1048, -0.0460,  0.2308,  0.0070,  0.0601, -0.2331,\n",
       "         -0.2181,  0.0073,  0.2045, -0.0503, -0.1339, -0.1692, -0.0646, -0.0957,\n",
       "          0.1248,  0.0156,  0.0243,  0.0721, -0.0011, -0.1736, -0.2009, -0.0160,\n",
       "         -0.1744,  0.1702, -0.0735, -0.0513]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xavier initialization을 이용하여 각 layer의 weight 초기화 \n",
    "\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Sequential을 이용하여 model 정의하기(쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n",
    "\n",
    "model = torch.nn.Sequential(linear1, bn1, relu, dropout, \n",
    "                            linear2, bn2, relu, dropout, \n",
    "                            linear3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost 계산을 위한 변수 설정 \n",
    "\n",
    "train_total_batch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.502910972\n",
      "Epoch: 0002 cost = 0.372203529\n",
      "Epoch: 0003 cost = 0.340591162\n",
      "Epoch: 0004 cost = 0.311733752\n",
      "Epoch: 0005 cost = 0.301339507\n",
      "Epoch: 0006 cost = 0.284228802\n",
      "Epoch: 0007 cost = 0.281810135\n",
      "Epoch: 0008 cost = 0.273160994\n",
      "Epoch: 0009 cost = 0.266850591\n",
      "Epoch: 0010 cost = 0.266489565\n",
      "Epoch: 0011 cost = 0.254399896\n",
      "Epoch: 0012 cost = 0.243856922\n",
      "Epoch: 0013 cost = 0.247039258\n",
      "Epoch: 0014 cost = 0.254141092\n",
      "Epoch: 0015 cost = 0.239667773\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "# Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    model.train()\n",
    "    avg_cost = 0  # cost 초기값 설정\n",
    "    \n",
    "    #train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드 \n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X) \n",
    "        cost = criterion(hypothesis, Y) \n",
    "        cost.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        avg_cost += cost / train_total_batch\n",
    "        \n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9103999733924866\n",
      "Label:  6\n",
      "Prediction:  6\n"
     ]
    }
   ],
   "source": [
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "#accuracy의 초기 값 설정(0으로) 꼭 할 것 \n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval() #evaluation mode (검증하는 단계이므로 dropout 제외)\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = model(X_test)\n",
    "    \n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test # prediction 값과 실제 test data 값이 같은가 (correct = 1)\n",
    "    accuracy = correct_prediction.float().mean() # 0 or 1 값들의 평균 >>> 정확도\n",
    "    print('Accuracy:', accuracy.item())\n",
    "    \n",
    "    ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n",
    "    r = random.randint(0, len(mnist_test) - 1) # randint: 범위 내 임의의 정수(난수) 추출 \n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hidden Node 조작 시 차이점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 전체적으로 node 수를 늘렸을 경우\n",
    "#### 784 > 300 > 200 > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node 수 변경\n",
    "\n",
    "linear4 = torch.nn.Linear(784, 300, bias=True)\n",
    "linear5 = torch.nn.Linear(300, 200, bias=True)\n",
    "linear6 = torch.nn.Linear(200, 10, bias=True)\n",
    "\n",
    "bn3 = torch.nn.BatchNorm1d(300)\n",
    "bn4 = torch.nn.BatchNorm1d(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0624, -0.1323,  0.0979,  ...,  0.0795,  0.1644,  0.0238],\n",
       "        [-0.1012, -0.1020, -0.0767,  ..., -0.0967,  0.0805, -0.1462],\n",
       "        [-0.0476,  0.1664,  0.0205,  ..., -0.1234,  0.0241, -0.0244],\n",
       "        ...,\n",
       "        [-0.0374,  0.1590,  0.0063,  ...,  0.1373, -0.1311, -0.0103],\n",
       "        [-0.1139, -0.1332,  0.1153,  ..., -0.1413, -0.0044, -0.0073],\n",
       "        [-0.0411,  0.1690, -0.1019,  ..., -0.0657, -0.0127,  0.1021]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(linear4.weight)\n",
    "torch.nn.init.xavier_uniform_(linear5.weight)\n",
    "torch.nn.init.xavier_uniform_(linear6.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새 모델 설정\n",
    "\n",
    "model2 = torch.nn.Sequential(linear4, bn3, relu, dropout, \n",
    "                            linear5, bn4, relu, dropout, \n",
    "                            linear6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.447802544\n",
      "Epoch: 0002 cost = 0.316706449\n",
      "Epoch: 0003 cost = 0.267640710\n",
      "Epoch: 0004 cost = 0.249820262\n",
      "Epoch: 0005 cost = 0.233272001\n",
      "Epoch: 0006 cost = 0.219855964\n",
      "Epoch: 0007 cost = 0.210375264\n",
      "Epoch: 0008 cost = 0.227158725\n",
      "Epoch: 0009 cost = 0.199710339\n",
      "Epoch: 0010 cost = 0.196030900\n",
      "Epoch: 0011 cost = 0.194281578\n",
      "Epoch: 0012 cost = 0.192709640\n",
      "Epoch: 0013 cost = 0.182906941\n",
      "Epoch: 0014 cost = 0.173025414\n",
      "Epoch: 0015 cost = 0.176281631\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "# Training epoch \n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    model2.train()\n",
    "    avg_cost = 0  # cost 초기값 설정\n",
    "    \n",
    "    #train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드 \n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model2(X) \n",
    "        cost = criterion(hypothesis, Y) \n",
    "        cost.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        avg_cost += cost / train_total_batch\n",
    "        \n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9172000288963318\n",
      "Label:  0\n",
      "Prediction:  0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model2.eval() #evaluation mode (검증하는 단계이므로 dropout 제외)\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = model2(X_test)\n",
    "    \n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test \n",
    "    accuracy = correct_prediction.float().mean() \n",
    "    print('Accuracy:', accuracy.item())\n",
    "    \n",
    "    r = random.randint(0, len(mnist_test) - 1) \n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model2(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 전체적으로 node 수를 줄였을 경우\n",
    "#### 784 > 80 > 50 > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node 수 변경\n",
    "\n",
    "linear7 = torch.nn.Linear(784, 80, bias=True)\n",
    "linear8 = torch.nn.Linear(80, 50, bias=True)\n",
    "linear9 = torch.nn.Linear(50, 10, bias=True)\n",
    "\n",
    "bn5 = torch.nn.BatchNorm1d(80)\n",
    "bn6 = torch.nn.BatchNorm1d(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 5.8076e-02,  5.8679e-02, -3.1408e-01, -2.2941e-01, -2.7232e-02,\n",
       "         -2.0816e-01, -7.6057e-02,  2.7781e-01, -2.1255e-01, -1.5896e-01,\n",
       "         -5.7349e-02,  3.0359e-01, -1.6189e-01, -1.5784e-01,  2.0372e-01,\n",
       "          2.1505e-01,  1.0756e-01,  4.2314e-02,  1.0020e-01,  1.8523e-01,\n",
       "         -2.1148e-01,  1.0073e-01,  1.7656e-01,  2.1576e-02, -1.7762e-02,\n",
       "         -6.4110e-02, -1.3598e-01, -1.0970e-01, -1.0404e-01, -1.4859e-01,\n",
       "          4.0875e-02,  2.0404e-01,  3.0612e-01,  3.0520e-01, -2.9589e-01,\n",
       "         -1.9178e-01,  6.7925e-02, -1.0832e-01, -9.1400e-02,  1.2293e-01,\n",
       "          3.0871e-02, -1.3555e-01, -1.7181e-01,  2.4449e-01, -2.3757e-01,\n",
       "          1.6370e-01, -2.8055e-01,  2.7008e-01,  1.9871e-01, -2.0131e-04],\n",
       "        [ 2.3480e-03, -1.1830e-01,  2.6135e-01,  5.7918e-03, -2.3352e-01,\n",
       "          4.7584e-03,  2.6468e-01, -1.4411e-01,  2.6322e-01,  1.0663e-01,\n",
       "         -1.7985e-01, -3.9026e-02,  1.8420e-01, -1.8220e-01,  1.4159e-01,\n",
       "         -1.2713e-01, -3.0034e-01, -2.3910e-01,  2.4155e-01,  2.3602e-01,\n",
       "         -8.3120e-02, -8.7788e-02,  2.5262e-01,  3.3251e-02, -2.2357e-01,\n",
       "         -2.9983e-02, -1.7271e-01, -7.2773e-02, -1.9684e-01, -1.1199e-01,\n",
       "          2.7119e-01,  7.1303e-02, -1.2870e-01, -9.1147e-03,  2.3474e-01,\n",
       "         -1.3078e-01,  9.1801e-02, -2.6121e-01,  2.2384e-02, -2.5787e-01,\n",
       "          9.3492e-02, -8.0916e-02,  2.6850e-02,  2.5304e-02, -2.3702e-01,\n",
       "          2.8902e-01, -2.7788e-01,  1.8285e-02,  1.3559e-01,  7.0677e-02],\n",
       "        [-2.1344e-01, -2.1557e-01,  2.5445e-01, -2.3402e-01,  5.1286e-02,\n",
       "         -9.2576e-02,  2.2757e-01,  2.3721e-02,  1.8904e-01,  9.3976e-02,\n",
       "         -2.8394e-01,  2.9561e-01,  7.9481e-02, -2.5841e-01,  3.0728e-01,\n",
       "          3.7229e-02, -2.7607e-01,  1.3809e-02, -1.6192e-01,  2.1031e-01,\n",
       "         -1.4340e-01, -1.3222e-01, -3.9174e-02, -2.1708e-01,  8.4780e-02,\n",
       "          2.1217e-01, -2.2671e-01, -1.8410e-01, -2.3607e-01, -1.4764e-01,\n",
       "          2.4149e-01,  2.3948e-01, -7.4381e-02, -1.6550e-01,  3.1612e-01,\n",
       "          2.2447e-01, -2.6139e-01,  1.6789e-01, -6.0928e-02,  2.9344e-01,\n",
       "          4.6982e-02,  2.1868e-01,  2.7096e-01, -1.0726e-01, -1.7590e-02,\n",
       "         -2.8654e-01,  1.3989e-01, -1.8254e-01, -1.4478e-01, -2.6833e-01],\n",
       "        [-9.1856e-02, -6.7809e-02, -2.7083e-01,  2.3948e-01,  9.9147e-02,\n",
       "         -2.4708e-01, -1.0556e-02, -2.1572e-01,  2.4198e-01,  1.7500e-01,\n",
       "         -5.7404e-03, -2.8395e-01,  3.2940e-02,  2.7984e-02,  8.0018e-02,\n",
       "          6.0176e-02, -2.6030e-01,  2.9672e-01,  1.2635e-01, -2.1979e-01,\n",
       "         -2.6914e-01,  1.2571e-01, -1.3954e-01,  1.4083e-01,  1.7955e-01,\n",
       "         -1.8497e-01, -2.7763e-01, -6.3415e-02, -2.4302e-01, -2.2883e-01,\n",
       "          4.1679e-02,  1.7172e-01,  1.0899e-01, -3.2299e-02, -1.0455e-01,\n",
       "          2.9241e-01,  6.7852e-02,  1.3654e-01,  1.1991e-01,  1.3041e-01,\n",
       "         -2.7541e-01,  1.8302e-01,  1.5018e-02, -3.0090e-01,  8.5833e-02,\n",
       "         -6.1339e-02, -3.0969e-01,  2.7845e-01, -1.5607e-01,  9.1914e-02],\n",
       "        [ 2.5127e-02,  2.9832e-01,  2.3040e-01,  2.7813e-01,  1.0132e-01,\n",
       "         -2.6341e-01, -1.7382e-01, -2.4078e-01, -3.0965e-01,  1.7679e-01,\n",
       "          2.5119e-01, -9.1527e-02, -1.8839e-01, -1.2753e-01, -1.7316e-01,\n",
       "          6.6244e-02,  2.4776e-01,  1.0086e-01, -2.1785e-01,  2.6449e-01,\n",
       "          3.0609e-01, -8.8907e-02, -1.8688e-01,  2.6198e-01, -5.0189e-02,\n",
       "         -1.1830e-01,  1.3432e-01,  2.8058e-01, -6.1574e-02,  2.5105e-01,\n",
       "         -9.4790e-02,  2.2603e-01, -2.8188e-01, -2.7699e-01, -2.6250e-01,\n",
       "          1.4441e-01, -1.1895e-01,  1.6076e-01, -2.3052e-01,  1.0032e-01,\n",
       "          3.0316e-01, -1.5939e-01,  2.2775e-01, -9.1265e-02, -1.2848e-01,\n",
       "          1.1519e-01, -1.6642e-01, -2.1329e-01, -2.4990e-01,  2.1917e-01],\n",
       "        [ 2.5205e-01,  2.9883e-01,  2.8104e-01,  2.1044e-01,  2.8807e-01,\n",
       "          2.7392e-01, -6.4565e-02, -2.1544e-01,  2.6791e-02, -6.6861e-02,\n",
       "         -9.7257e-03,  2.1167e-01, -8.2541e-02, -1.3502e-01, -1.1802e-01,\n",
       "         -1.5936e-01, -3.0021e-01,  4.7764e-02,  1.2289e-01,  2.8161e-01,\n",
       "          2.3141e-01,  2.0300e-01, -7.7891e-02,  6.2452e-02,  5.9882e-02,\n",
       "         -3.3063e-02, -2.9988e-01,  1.8667e-01, -1.8585e-01, -2.2093e-02,\n",
       "          5.4939e-02,  8.7104e-02, -2.1531e-01,  2.0999e-01, -1.8442e-01,\n",
       "         -3.9586e-03, -1.9576e-01, -1.7508e-01,  1.5520e-01, -1.4401e-01,\n",
       "         -1.8215e-01, -2.8668e-01, -3.4949e-03, -8.8666e-02,  2.6036e-01,\n",
       "          1.7985e-01,  2.6348e-01,  2.4879e-01, -7.3943e-02,  1.3527e-01],\n",
       "        [-3.9651e-03,  1.1356e-01,  1.1357e-01, -2.7611e-01,  2.4292e-01,\n",
       "         -2.0793e-01, -2.4520e-01,  2.2394e-01,  4.4411e-02,  3.0300e-01,\n",
       "         -2.7042e-02, -2.9516e-01, -1.2263e-01, -2.5471e-01,  1.3420e-01,\n",
       "         -1.5059e-01,  2.9873e-01,  2.5662e-01, -2.6089e-01, -1.9721e-01,\n",
       "          1.9291e-04,  2.0880e-01,  1.2353e-01, -2.2024e-01, -3.0552e-01,\n",
       "          1.5139e-01, -1.1185e-01,  1.2425e-01, -6.2505e-03, -1.2003e-01,\n",
       "          1.2421e-01,  2.8837e-01,  2.0779e-01,  3.1249e-01, -2.3206e-01,\n",
       "          2.4863e-01,  3.0471e-01, -1.7196e-01,  2.3246e-02,  4.5689e-02,\n",
       "         -2.9596e-01,  2.9945e-01, -8.0001e-02, -2.1179e-01,  1.1655e-01,\n",
       "         -6.9190e-02,  1.4056e-01,  2.0049e-01,  2.1937e-01, -3.0859e-01],\n",
       "        [-9.0177e-03,  1.2837e-01, -1.8461e-01, -1.4987e-02, -2.5947e-03,\n",
       "         -9.8216e-02,  2.2790e-03, -5.1196e-02,  2.6118e-01, -1.8872e-01,\n",
       "          4.0684e-02, -7.9572e-02, -4.3286e-02,  2.6983e-01, -8.7130e-02,\n",
       "          1.4403e-01,  1.5348e-01, -9.8500e-02,  2.3967e-01,  2.8347e-01,\n",
       "         -1.3266e-01, -3.1068e-01, -2.9416e-01, -7.1836e-02,  5.2877e-02,\n",
       "          2.8992e-01, -4.7201e-02, -4.7244e-02, -1.9726e-01, -2.3761e-01,\n",
       "          2.0382e-01, -2.2291e-01, -1.4364e-01, -2.1911e-01, -8.7117e-02,\n",
       "          2.2518e-01, -9.9421e-02,  1.9491e-02,  6.9072e-02,  1.6786e-03,\n",
       "         -2.9381e-01,  2.8585e-01,  9.1452e-02, -2.1238e-01,  2.0798e-01,\n",
       "         -1.2396e-01,  3.6644e-02,  1.5811e-01,  2.0625e-01,  1.3616e-01],\n",
       "        [-1.9649e-01,  8.7433e-02, -2.6211e-01, -1.0889e-01, -9.1884e-02,\n",
       "          8.5317e-03, -1.6732e-01,  1.0863e-01,  1.8943e-01,  2.9983e-01,\n",
       "          1.2291e-01,  5.6646e-02, -2.9716e-01, -8.3673e-02, -9.5299e-02,\n",
       "          1.5924e-01, -2.2231e-01, -2.7032e-01, -2.8560e-01,  3.7022e-02,\n",
       "         -8.7516e-03,  2.6307e-01,  2.5643e-01, -2.8802e-03,  1.8973e-01,\n",
       "          3.0654e-02, -2.8262e-01,  1.5238e-01,  1.2220e-01,  3.1893e-02,\n",
       "         -1.1359e-01, -3.3736e-02, -2.2572e-01, -2.4495e-01,  2.3210e-01,\n",
       "         -2.6014e-01, -2.2949e-02, -2.1632e-01,  1.5371e-01, -5.4756e-02,\n",
       "          1.5533e-01, -2.7260e-01, -6.9567e-02, -5.7115e-02, -2.7545e-01,\n",
       "         -3.0725e-01,  9.1189e-02,  2.0053e-02, -2.4937e-01,  2.7673e-01],\n",
       "        [ 2.8590e-01, -1.5268e-02, -8.3096e-02, -7.0956e-02,  2.9925e-01,\n",
       "          9.2781e-02, -7.0542e-02, -1.9588e-01, -2.2653e-01, -2.3967e-02,\n",
       "          2.7403e-01, -1.5091e-01,  7.9101e-02,  1.5969e-01, -3.8614e-03,\n",
       "          5.7499e-02,  8.2649e-02,  7.2224e-02, -2.0465e-01, -2.7353e-01,\n",
       "          1.0647e-01,  1.8085e-01, -2.8225e-01,  2.8769e-01,  1.5786e-01,\n",
       "          3.0256e-01,  1.5321e-01,  2.8398e-01, -3.0776e-01,  2.0620e-01,\n",
       "         -6.1378e-02,  7.8951e-02,  6.5657e-02,  1.6277e-03,  8.4025e-02,\n",
       "          1.6741e-01, -1.9436e-01, -2.5591e-01,  1.6736e-01,  2.6793e-01,\n",
       "          1.1653e-01,  2.1347e-01, -1.8537e-01,  3.1005e-02, -5.1736e-02,\n",
       "          1.4076e-02,  2.0238e-01,  1.6488e-01,  1.3703e-01, -6.7594e-02]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(linear7.weight)\n",
    "torch.nn.init.xavier_uniform_(linear8.weight)\n",
    "torch.nn.init.xavier_uniform_(linear9.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새 모델 설정\n",
    "model3 = torch.nn.Sequential(linear7, bn5, relu, dropout, \n",
    "                            linear8, bn6, relu, dropout, \n",
    "                            linear9).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.539309978\n",
      "Epoch: 0002 cost= 0.403191447\n",
      "Epoch: 0003 cost= 0.356301278\n",
      "Epoch: 0004 cost= 0.332073182\n",
      "Epoch: 0005 cost= 0.315065086\n",
      "Epoch: 0006 cost= 0.307772726\n",
      "Epoch: 0007 cost= 0.303723037\n",
      "Epoch: 0008 cost= 0.294286698\n",
      "Epoch: 0009 cost= 0.296302527\n",
      "Epoch: 0010 cost= 0.295208335\n",
      "Epoch: 0011 cost= 0.286631435\n",
      "Epoch: 0012 cost= 0.275844872\n",
      "Epoch: 0013 cost= 0.277649581\n",
      "Epoch: 0014 cost= 0.271076798\n",
      "Epoch: 0015 cost= 0.265455544\n",
      "Learning finished.\n"
     ]
    }
   ],
   "source": [
    "model3.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X,Y in train_loader:\n",
    "        X = X.view(-1, 28*28).to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model3(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost/ train_total_batch\n",
    "    \n",
    "    print('Epoch:','%04d' % (epoch+1),'cost=', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9495000243186951\n",
      "Label:  6\n",
      "Prediction:  6\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model3.eval() #evaluation mode (검증하는 단계이므로 dropout 제외)\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = model3(X_test)\n",
    "    \n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test \n",
    "    accuracy = correct_prediction.float().mean() \n",
    "    print('Accuracy:', accuracy.item())\n",
    "    \n",
    "    r = random.randint(0, len(mnist_test) - 1) \n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model3(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
