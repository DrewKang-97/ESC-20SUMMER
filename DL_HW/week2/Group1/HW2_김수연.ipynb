{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import random\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정 (learning rate, training epochs, batch_size)\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6726dae7aba842978a9598bd2ad5e6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51eec303fdd4edb924566cb2271bd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b7cbf5137141d1986da47c0c40ff42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944a7f9b1f2f4f659ecf416d72d78f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/distiller/project/conda/conda-bld/pytorch_1591914925853/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    }
   ],
   "source": [
    "# train과 test set으로 나누어 MNIST data 불러오기\n",
    "train_dataset = dsets.MNIST(root='./mnist_data/',\n",
    "                            train=True,\n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./mnist_data/',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n",
    "# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n",
    "l1 = nn.Linear(784, 100, bias=True)\n",
    "l2 = nn.Linear(100, 100, bias=True)\n",
    "l3 = nn.Linear(100, 10, bias=True)\n",
    "relu = nn.ReLU()\n",
    "dropout = nn.Dropout(p=0.3)\n",
    "bn = nn.BatchNorm1d(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soo._.yonee/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n",
      "/Users/soo._.yonee/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/soo._.yonee/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.5923e-01,  1.2804e-02,  2.2385e-01,  6.6520e-02, -1.7304e-01,\n",
       "         -1.7853e-01, -9.6810e-02,  2.0175e-01, -8.0244e-02,  2.1902e-01,\n",
       "          1.0228e-01,  1.1648e-01, -2.9521e-02, -1.8644e-01, -2.0157e-01,\n",
       "         -1.8056e-01,  1.2386e-01, -7.8582e-02, -9.9098e-02, -9.5483e-02,\n",
       "         -1.0700e-02,  1.9318e-01, -1.0829e-01, -2.0169e-02, -2.0358e-01,\n",
       "          1.5787e-01, -1.0330e-01,  5.1215e-02, -1.0194e-02, -8.4842e-02,\n",
       "         -1.5966e-01,  1.0230e-01,  1.7901e-01,  7.9604e-02, -1.4405e-02,\n",
       "          9.6878e-02, -1.2354e-01, -4.7724e-02, -1.2104e-01, -1.7828e-01,\n",
       "         -1.5313e-01, -6.3761e-02, -1.8521e-01,  2.1763e-01, -1.1343e-01,\n",
       "         -8.3920e-02, -8.1436e-03, -6.6567e-02,  2.1454e-01,  2.1404e-01,\n",
       "         -7.5073e-02, -7.7181e-02,  1.9544e-01,  9.5197e-02,  1.0162e-01,\n",
       "          5.1343e-02, -3.2681e-02, -3.5978e-02,  1.7361e-01, -1.2619e-01,\n",
       "          1.7015e-01,  1.0960e-01, -9.5830e-02, -6.0812e-02, -1.7672e-01,\n",
       "          3.8193e-02,  2.1931e-01, -1.9188e-01, -1.8313e-01,  5.0131e-02,\n",
       "         -1.7244e-01, -2.0099e-01,  1.0519e-01,  1.4357e-02, -1.3486e-01,\n",
       "         -2.3090e-01, -1.6829e-02, -6.4522e-02,  5.7611e-02, -6.7302e-03,\n",
       "         -1.6584e-01, -1.9560e-02, -1.5926e-01, -1.3583e-01,  9.6047e-02,\n",
       "         -7.0415e-02,  1.7282e-02, -2.1786e-01,  1.7958e-01,  6.4506e-02,\n",
       "         -2.1260e-01, -1.0384e-01, -8.4472e-02, -1.7837e-01,  2.9565e-02,\n",
       "          1.9962e-01,  1.6870e-01,  1.8861e-01, -3.1922e-02, -1.1542e-01],\n",
       "        [-6.2086e-02, -5.2798e-02,  4.6943e-04,  2.3153e-01,  3.2743e-02,\n",
       "          2.3505e-02,  5.3636e-03,  1.8649e-01, -3.5959e-03,  5.1223e-02,\n",
       "          2.1215e-01, -2.2444e-01, -1.0400e-01, -1.9134e-01,  2.2957e-01,\n",
       "         -3.6166e-02,  7.2941e-02,  1.2950e-01,  9.1679e-02, -2.0683e-02,\n",
       "         -1.2231e-01,  9.9637e-02, -6.8302e-02,  2.2209e-01, -1.5827e-01,\n",
       "          1.6871e-01, -1.8425e-01, -1.8612e-01, -1.8997e-01, -1.2054e-01,\n",
       "         -1.6397e-01,  1.4279e-01, -2.0973e-01,  1.7199e-01, -2.0572e-01,\n",
       "         -3.6150e-02, -9.1863e-02,  3.8372e-02, -7.2701e-04, -6.2741e-02,\n",
       "         -6.4097e-02,  1.5809e-01, -1.1129e-01, -1.8999e-02,  1.2961e-01,\n",
       "          7.7060e-02,  1.1432e-01, -1.9878e-01,  3.9840e-02,  1.8700e-01,\n",
       "          1.2888e-02, -1.2244e-01,  6.9957e-02,  1.1583e-01,  2.3319e-01,\n",
       "          1.5777e-01, -1.7306e-01, -4.9403e-03, -1.8522e-01,  9.3694e-03,\n",
       "          1.3837e-01,  2.3543e-02,  3.4429e-02,  1.7762e-01, -2.0265e-01,\n",
       "         -2.0436e-01, -9.0098e-02, -1.0483e-01,  2.0627e-01, -2.1997e-02,\n",
       "          5.6177e-02,  6.5010e-02, -3.6177e-02,  2.1827e-01,  8.0410e-03,\n",
       "          2.0055e-01, -2.1180e-01, -1.4135e-02,  1.2167e-02,  1.9017e-01,\n",
       "         -1.3135e-01, -2.2156e-01,  3.7945e-02, -2.2350e-01, -7.4779e-02,\n",
       "          6.5508e-02,  1.3472e-01,  8.7258e-02,  1.4972e-01,  7.7970e-02,\n",
       "          1.2672e-01,  1.8802e-01,  8.3535e-02,  9.0555e-02, -1.1074e-01,\n",
       "         -1.8592e-01,  1.6775e-01, -4.8532e-02,  2.2561e-01, -2.3139e-01],\n",
       "        [ 1.0198e-01,  1.9464e-02, -1.3631e-01,  1.5764e-01,  1.5981e-01,\n",
       "         -2.2815e-01,  1.3025e-01,  1.9661e-01, -2.0900e-01,  9.1537e-02,\n",
       "          4.0946e-02,  3.7835e-03,  5.7442e-02,  3.1724e-02, -5.1209e-02,\n",
       "          1.1896e-01,  1.3911e-01, -1.1393e-01,  1.5200e-01, -1.2066e-01,\n",
       "         -2.9073e-02,  2.2043e-01,  2.8703e-02,  6.2576e-03,  3.3038e-02,\n",
       "         -1.5469e-01, -1.0200e-01,  6.5580e-02, -9.8709e-02,  3.9378e-02,\n",
       "         -4.4083e-03,  9.4237e-02, -1.1865e-01, -2.1514e-01,  3.1345e-02,\n",
       "         -4.7619e-02, -2.1486e-01,  1.4180e-01, -1.1726e-02, -1.6212e-01,\n",
       "          3.9638e-02, -3.7107e-02, -2.2619e-01, -2.2824e-01,  1.9003e-01,\n",
       "          1.9425e-01, -1.9740e-01,  6.2102e-02, -1.9481e-01,  1.7677e-01,\n",
       "         -1.8419e-01, -5.3781e-02,  1.6339e-02, -1.7284e-01,  7.8326e-02,\n",
       "         -1.7582e-02, -1.3434e-02,  3.4844e-02,  9.9856e-02, -9.3768e-02,\n",
       "          1.0232e-01,  7.6156e-02, -7.0939e-03, -5.7217e-02,  1.5049e-01,\n",
       "         -3.6280e-02,  2.2818e-01,  2.0970e-01, -2.2941e-01, -5.1154e-02,\n",
       "          2.8127e-03, -1.6957e-01, -5.4894e-02, -1.8137e-02,  1.4972e-01,\n",
       "         -1.7026e-01,  1.9843e-01, -1.2453e-02, -8.4347e-02,  3.8790e-02,\n",
       "          1.5044e-01, -7.1899e-03, -2.1322e-01, -2.3294e-01, -1.6008e-01,\n",
       "          1.1140e-01, -3.5050e-02,  3.1755e-03,  1.7832e-03,  5.5326e-02,\n",
       "         -1.5849e-01, -2.6502e-02,  1.2053e-01, -5.0652e-03,  8.7607e-02,\n",
       "          1.2925e-01,  8.5219e-02, -1.5457e-01, -6.0211e-02, -2.0763e-01],\n",
       "        [ 1.1955e-01,  9.6999e-02, -6.2550e-02, -1.3844e-01, -1.0048e-01,\n",
       "          1.0580e-02, -6.6194e-02,  1.8090e-03,  4.3930e-02, -2.2656e-01,\n",
       "         -1.9491e-01,  1.4447e-01,  2.2114e-01,  1.5419e-01,  1.7108e-01,\n",
       "         -8.2093e-02,  1.7149e-01,  1.0792e-02, -4.7926e-02, -7.5678e-02,\n",
       "         -1.4214e-01, -4.9168e-02, -2.0718e-01, -1.8010e-01,  1.6718e-01,\n",
       "         -1.0391e-01, -5.5569e-02, -1.1574e-01,  2.0529e-01,  5.7380e-02,\n",
       "         -2.5536e-02, -6.9715e-02, -2.3264e-03,  7.3378e-02,  1.5580e-01,\n",
       "          2.2504e-01, -1.9240e-01, -1.7953e-01, -5.5872e-02,  1.3054e-01,\n",
       "         -3.3010e-02, -1.0878e-01, -2.0546e-01,  1.6736e-01,  2.3154e-01,\n",
       "          2.2923e-01,  1.5482e-01, -2.2136e-01,  1.0306e-01,  2.9211e-02,\n",
       "         -1.8365e-01,  3.6282e-02, -6.3651e-02,  1.7570e-01,  2.1562e-02,\n",
       "         -1.7606e-01,  1.9493e-01,  1.3506e-01,  1.1465e-01, -2.1487e-01,\n",
       "          1.3369e-01,  1.6457e-01,  8.8001e-02, -6.7492e-02,  1.4370e-01,\n",
       "          9.1425e-02, -1.6476e-02,  1.5020e-01, -8.1927e-02,  3.9496e-02,\n",
       "          1.7789e-01, -2.3188e-01,  4.6929e-02, -5.7833e-02, -5.9311e-02,\n",
       "          1.0119e-01,  7.2863e-03, -2.1614e-01,  1.3263e-01,  2.2296e-01,\n",
       "         -1.1646e-01, -2.0490e-01, -6.1582e-03, -1.3103e-01,  3.0976e-02,\n",
       "         -2.3092e-01,  8.3819e-02, -1.0178e-01, -1.5293e-01, -6.9380e-02,\n",
       "         -1.0855e-01, -5.7250e-03,  6.1639e-03, -2.3264e-01, -9.6177e-02,\n",
       "          1.6976e-01, -2.2756e-01, -2.7195e-02, -6.0440e-02,  1.4697e-01],\n",
       "        [ 1.5131e-01, -1.1550e-01, -1.8998e-01, -9.1767e-02,  2.2480e-01,\n",
       "          2.2035e-01, -1.8769e-01, -1.8943e-01,  7.6926e-02,  2.1109e-01,\n",
       "         -2.2288e-01, -5.1801e-02, -1.2778e-01,  8.2329e-02,  5.6890e-02,\n",
       "         -2.3050e-01, -1.0571e-01,  7.0203e-02,  1.7673e-01,  9.9582e-02,\n",
       "          2.2896e-01,  1.9243e-01, -1.0820e-01, -1.8413e-01, -1.1776e-01,\n",
       "          2.0795e-01, -1.5204e-01, -2.1444e-01,  1.6926e-01,  2.2990e-01,\n",
       "          1.2490e-01,  6.7870e-02, -6.5678e-02,  1.4748e-01,  1.6700e-01,\n",
       "         -1.2795e-01, -2.1548e-01,  7.5320e-02,  1.9412e-01,  1.6653e-01,\n",
       "         -1.3810e-01, -1.7672e-01, -1.6784e-01, -4.7715e-02,  2.0439e-01,\n",
       "         -1.6914e-01, -2.1615e-01,  3.4802e-02, -2.1006e-01, -1.0724e-01,\n",
       "         -1.9908e-01,  5.1348e-02,  1.4827e-01, -9.3774e-02,  1.7105e-01,\n",
       "          4.2269e-03, -1.6796e-01, -1.4594e-01,  1.8739e-01, -1.3003e-01,\n",
       "          2.1814e-01,  1.2229e-01,  3.7618e-02, -8.1839e-02, -1.6127e-01,\n",
       "          2.0615e-01, -9.8361e-02, -1.9436e-02, -1.2358e-01, -1.1212e-02,\n",
       "         -1.5060e-01,  2.1748e-01, -2.0586e-01, -1.3145e-01,  8.9889e-02,\n",
       "         -1.5611e-01,  2.1935e-01, -3.5784e-02,  1.9165e-01,  7.7850e-02,\n",
       "         -7.0959e-02, -2.0226e-01,  5.8548e-02, -1.0480e-01, -2.3309e-01,\n",
       "          6.6553e-02, -1.1103e-01, -1.2002e-01, -3.8173e-03, -7.2663e-02,\n",
       "          2.3062e-01,  5.2405e-02,  9.5114e-02, -3.2881e-02,  3.1301e-02,\n",
       "         -2.2376e-01,  7.7095e-02,  1.5890e-01,  1.7259e-01,  2.7207e-02],\n",
       "        [-1.9831e-02,  1.8441e-01, -2.2709e-02,  1.8263e-01,  1.4204e-01,\n",
       "          1.3831e-01,  1.1943e-01,  7.2281e-02, -1.2724e-01, -1.5000e-01,\n",
       "          1.7460e-01, -1.1397e-01, -1.7974e-01,  1.3805e-01, -1.9087e-01,\n",
       "         -2.0668e-01, -2.7545e-02,  1.0181e-01, -3.1541e-02,  2.1023e-02,\n",
       "         -2.9236e-02, -4.2984e-02, -3.2899e-02,  1.6742e-01, -3.6936e-02,\n",
       "          1.7274e-01,  1.8235e-01, -7.8798e-03,  1.3549e-01,  5.0321e-02,\n",
       "          8.0008e-02, -1.2284e-01, -1.2287e-01, -1.2267e-01,  1.5421e-01,\n",
       "          2.0466e-01, -5.3784e-02, -1.7900e-01, -2.2143e-02,  1.4083e-01,\n",
       "         -5.5296e-02,  2.1861e-01,  7.3893e-02, -1.9458e-01,  8.2026e-02,\n",
       "         -1.5358e-01, -7.5610e-03,  1.9509e-01, -6.9371e-02,  1.3203e-01,\n",
       "         -9.2584e-02, -1.6322e-01,  4.2329e-02, -5.6832e-03, -1.5034e-01,\n",
       "         -5.6095e-02, -1.9854e-01, -2.2125e-01,  3.8775e-03,  3.2837e-02,\n",
       "         -8.1355e-02,  5.0098e-03, -1.1576e-01, -9.6167e-02,  1.2781e-01,\n",
       "         -1.1236e-01, -2.2572e-01, -3.6201e-02,  1.9045e-01, -1.6607e-01,\n",
       "          1.3392e-01,  4.3358e-02,  1.4893e-01, -3.7781e-02, -2.1891e-01,\n",
       "         -2.2671e-01, -1.8559e-01,  2.1381e-01,  5.7993e-02, -1.8573e-01,\n",
       "         -4.6910e-02,  2.3309e-01, -9.2417e-02, -3.7750e-03,  8.3365e-02,\n",
       "          2.0917e-01, -1.7855e-02,  9.6264e-02, -3.7535e-02,  1.9560e-01,\n",
       "          1.0026e-01, -1.7474e-01, -9.7435e-02, -1.7775e-01,  5.1328e-02,\n",
       "         -1.4533e-01, -1.0029e-01, -1.3039e-02,  1.4032e-01, -5.9860e-02],\n",
       "        [ 5.0514e-02, -1.0547e-01,  6.0288e-02, -5.1984e-02,  2.3411e-03,\n",
       "          2.2581e-01, -1.1354e-01, -1.9774e-01,  1.7618e-01, -1.3955e-01,\n",
       "          2.4417e-02, -1.0845e-01, -2.3295e-02,  1.0365e-01,  1.1391e-01,\n",
       "          1.4001e-01, -2.1271e-01, -8.4734e-02, -1.3969e-01, -1.7983e-02,\n",
       "          1.8186e-01, -2.3321e-01, -2.2883e-01, -6.1142e-02,  1.8047e-02,\n",
       "         -2.1258e-01,  1.0414e-01, -7.2629e-02, -8.9094e-03,  1.6567e-01,\n",
       "          6.8070e-02, -1.0724e-02, -1.6841e-01,  2.0424e-01, -5.5163e-02,\n",
       "         -1.5373e-01, -1.5902e-01, -1.5296e-01,  7.5124e-02,  1.9395e-01,\n",
       "         -1.2565e-02,  1.5145e-02,  1.4024e-01, -1.2356e-01,  1.2716e-01,\n",
       "          1.9325e-01,  1.6278e-02,  9.0449e-02, -1.0724e-01, -1.2217e-01,\n",
       "          3.3868e-02,  2.2582e-01,  1.1281e-01,  7.9223e-02, -2.8237e-02,\n",
       "          1.8229e-01, -2.5358e-02, -2.3657e-02,  2.0130e-01,  1.0849e-01,\n",
       "         -6.0543e-02, -8.1875e-02, -8.3974e-02, -1.3181e-01, -7.3537e-04,\n",
       "         -1.8578e-01,  5.6032e-02, -1.9313e-01, -9.0445e-02, -1.3649e-05,\n",
       "         -1.0322e-01, -2.1039e-01, -1.6208e-02,  9.2008e-02, -1.6907e-01,\n",
       "          1.9594e-02, -2.0496e-02, -4.1533e-03,  1.9443e-01, -5.9527e-02,\n",
       "          2.1215e-01,  2.1379e-01,  2.3428e-02,  2.2209e-01, -9.7709e-02,\n",
       "          6.1268e-02, -1.2911e-03, -1.5231e-01,  1.7793e-01, -1.0223e-01,\n",
       "         -1.2818e-01, -5.3218e-03, -2.3155e-01,  2.0812e-01,  1.1771e-01,\n",
       "         -6.4845e-02, -1.1923e-01, -3.4292e-02,  5.2750e-02, -6.9483e-03],\n",
       "        [ 1.2040e-01,  1.6018e-01, -4.5372e-02, -4.1795e-02,  1.0743e-02,\n",
       "         -1.4918e-01, -1.6940e-01,  7.7372e-02,  1.1562e-01,  1.4732e-01,\n",
       "          1.1499e-01,  1.9344e-01, -1.4406e-01, -1.7152e-01,  1.2572e-01,\n",
       "          4.9864e-02,  8.2387e-02,  1.7132e-01, -1.3651e-01, -9.2557e-02,\n",
       "          4.0519e-02,  2.8383e-02,  1.9586e-01,  2.0077e-01,  6.2403e-02,\n",
       "         -1.3496e-01,  1.9699e-01, -1.0659e-01, -1.6049e-01, -1.6982e-01,\n",
       "          6.7144e-02,  6.6175e-02,  6.4723e-02,  1.7755e-01,  6.4994e-02,\n",
       "          1.3690e-01,  1.5826e-01, -9.4311e-02, -1.5386e-01, -1.1226e-01,\n",
       "         -1.4714e-01,  1.6031e-01, -2.2251e-01,  2.1191e-01,  1.8397e-01,\n",
       "         -5.9119e-02,  1.5258e-01, -2.0613e-01, -2.6504e-02,  1.7348e-01,\n",
       "          2.2410e-01, -1.4090e-01,  1.2713e-01,  2.2294e-01, -5.1566e-02,\n",
       "          1.8372e-01,  7.7683e-02, -6.0348e-02,  4.3026e-02,  2.2842e-01,\n",
       "         -6.7067e-02,  1.5452e-01, -5.3429e-02,  3.5662e-02,  1.0598e-01,\n",
       "         -9.8000e-02, -3.5906e-02, -2.1952e-01,  1.8793e-01, -2.0034e-01,\n",
       "         -2.0256e-01, -1.1422e-01, -1.9335e-01, -1.1063e-01, -2.1338e-01,\n",
       "         -9.3325e-02,  1.2406e-01, -4.0193e-02, -2.2780e-01, -2.0447e-01,\n",
       "          1.8407e-01, -8.4632e-02, -6.8816e-02, -1.1902e-01, -1.9590e-01,\n",
       "         -2.1097e-01,  5.3856e-02,  6.2854e-02,  2.5196e-02,  2.0626e-01,\n",
       "          1.4839e-01, -4.5866e-02,  1.3475e-02, -6.6967e-02,  4.5490e-02,\n",
       "          1.5597e-01,  6.1657e-02,  2.1485e-01, -2.7394e-02, -2.6174e-02],\n",
       "        [ 1.1052e-01, -2.0615e-01,  1.1879e-01, -1.3068e-01,  1.4027e-02,\n",
       "         -1.6530e-02, -2.5186e-02,  1.8743e-01,  1.0079e-01,  1.2577e-01,\n",
       "         -8.6328e-02, -1.4685e-01, -2.9419e-02, -1.3694e-01, -1.7912e-01,\n",
       "          1.5379e-01, -2.2647e-02,  1.5493e-01,  2.0787e-01, -2.2340e-01,\n",
       "         -6.0207e-03, -2.1638e-01, -1.1952e-01,  1.6453e-02, -1.3990e-01,\n",
       "         -1.7224e-01, -1.3958e-01, -2.1419e-01,  9.0774e-02, -1.3179e-02,\n",
       "         -1.6062e-02,  1.6313e-01,  2.1058e-01,  1.8379e-01,  1.7248e-01,\n",
       "         -4.2968e-02,  8.5444e-03, -1.4614e-01,  1.0691e-02,  2.2325e-01,\n",
       "         -1.2275e-03,  8.3581e-02, -1.3336e-01, -1.4899e-01,  2.2632e-01,\n",
       "         -2.2812e-01,  1.3648e-01,  4.2441e-02,  4.6844e-02,  9.6767e-02,\n",
       "         -2.1551e-01, -1.5839e-02, -1.9437e-01, -8.8730e-02,  1.0023e-01,\n",
       "          2.0040e-01, -1.6675e-01, -1.6917e-01, -2.2055e-01, -1.4213e-01,\n",
       "         -1.7993e-01,  4.0757e-02,  1.0467e-01, -1.4062e-01, -7.4720e-04,\n",
       "         -1.0124e-01, -1.2289e-01, -7.6620e-03, -2.1804e-01,  1.7697e-01,\n",
       "         -1.8408e-01,  6.7333e-02,  1.6392e-01, -5.6508e-02, -1.2781e-01,\n",
       "         -2.3070e-01, -2.0650e-01,  3.1097e-02, -1.9895e-01,  7.5009e-02,\n",
       "          1.3912e-02,  2.0802e-01,  3.9991e-02, -3.4790e-02, -3.1467e-03,\n",
       "         -8.1302e-02, -1.3236e-01,  2.3292e-01, -7.9820e-03,  1.6409e-01,\n",
       "          1.4336e-01, -1.5364e-01, -1.3697e-01,  1.0572e-01, -1.5993e-01,\n",
       "         -3.2212e-02, -6.7231e-02,  2.3040e-01,  2.3192e-01,  2.6818e-02],\n",
       "        [ 2.1141e-01,  2.0571e-01, -1.2498e-01, -1.5762e-02, -6.3003e-02,\n",
       "          1.4635e-01,  2.1382e-01,  6.7237e-02,  1.1050e-01, -5.4753e-02,\n",
       "         -1.0828e-01,  5.5051e-02, -9.1162e-02,  1.3899e-01, -1.0790e-01,\n",
       "         -9.4513e-02, -4.1784e-02, -1.2924e-01,  9.3405e-02,  1.8902e-01,\n",
       "          5.6707e-03, -2.3141e-01,  1.3777e-01, -2.1478e-01,  1.0600e-02,\n",
       "          1.0287e-01,  2.2685e-01, -1.9027e-01, -4.8462e-02, -3.8110e-02,\n",
       "          4.9173e-02, -1.0589e-01, -1.7068e-01, -1.8336e-01, -1.6318e-01,\n",
       "          2.2645e-01,  6.4821e-02, -7.1701e-02,  5.9507e-02,  7.1852e-02,\n",
       "         -1.9878e-01, -9.9110e-02,  7.7905e-02,  2.9785e-02, -4.6355e-02,\n",
       "         -2.3023e-01,  1.6132e-01,  9.3637e-02,  9.2698e-02, -8.1366e-02,\n",
       "          8.4869e-02, -1.2373e-01,  8.9931e-02, -3.7348e-02, -1.6791e-01,\n",
       "         -2.1579e-01,  1.0263e-01, -1.0395e-01, -1.3562e-01,  2.0419e-01,\n",
       "         -1.3971e-01, -1.7309e-01, -1.0009e-01, -1.5066e-01, -1.2930e-01,\n",
       "          6.4635e-02,  5.0722e-02, -1.7729e-01,  1.0686e-01,  6.3396e-02,\n",
       "          1.3388e-01, -7.7928e-02, -1.8054e-01, -1.4912e-02, -8.5490e-02,\n",
       "          8.1299e-03,  9.5276e-02, -1.2322e-01,  9.6997e-02,  1.9520e-01,\n",
       "          1.9604e-01,  1.7250e-01, -1.3152e-01,  1.6317e-01, -6.4062e-02,\n",
       "         -7.6184e-02,  6.4194e-02,  2.7398e-02,  4.5960e-02, -2.3155e-01,\n",
       "         -8.5162e-02, -1.8420e-01, -1.4749e-01, -1.3547e-01, -7.7924e-02,\n",
       "          1.4209e-03, -1.9769e-01, -6.9881e-02,  8.5551e-02,  1.6794e-02]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xavier initialization을 이용하여 각 layer의 weight 초기화\n",
    "nn.init.xavier_uniform(l1.weight)\n",
    "nn.init.xavier_uniform(l2.weight)\n",
    "nn.init.xavier_uniform(l3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Sequential을 이용하여 model 정의하기(쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n",
    "model = nn.Sequential(l1, bn, relu, dropout,\n",
    "                      l2, bn, relu, dropout,\n",
    "                      l3).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "criterion = nn.CrossEntropyLoss().to('cpu')\n",
    "# optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#cost 계산을 위한 변수 설정\n",
    "train_total_batch = len(train_loader)\n",
    "test_total_batch = len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.532430947\n",
      "Epoch: 0002 cost = 0.421571046\n",
      "Epoch: 0003 cost = 0.393104583\n",
      "Epoch: 0004 cost = 0.369535118\n",
      "Epoch: 0005 cost = 0.349459350\n",
      "Epoch: 0006 cost = 0.351994306\n",
      "Epoch: 0007 cost = 0.335494578\n",
      "Epoch: 0008 cost = 0.350321680\n",
      "Epoch: 0009 cost = 0.321461827\n",
      "Epoch: 0010 cost = 0.319085449\n",
      "Epoch: 0011 cost = 0.300809681\n",
      "Epoch: 0012 cost = 0.321902364\n",
      "Epoch: 0013 cost = 0.305828780\n",
      "Epoch: 0014 cost = 0.305184990\n",
      "Epoch: 0015 cost = 0.302269131\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "# Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    # train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 784).to('cpu')\n",
    "        Y = Y.to('cpu')\n",
    "        \n",
    "        hypothesis = model(X)\n",
    "        bn_loss = criterion(hypothesis, Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        bn_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        avg_cost += bn_loss / train_total_batch\n",
    "        \n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.930300235748291\n",
      "Label:  5\n",
      "Prediction:  5\n"
     ]
    }
   ],
   "source": [
    "# test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "# X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "# accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    \n",
    "    loss, bn_acc = 0, 0\n",
    "    for i, (X, Y) in enumerate(test_loader):\n",
    "        X = X.view(-1, 784).to('cpu')\n",
    "        Y = Y.to('cpu')\n",
    "        \n",
    "        pred = model(X)\n",
    "        correct_pred = torch.argmax(pred, 1) == Y\n",
    "        loss += criterion(pred, Y)\n",
    "        bn_acc += correct_pred.float().mean()\n",
    "    \n",
    "    loss, bn_acc = loss / test_total_batch, bn_acc / test_total_batch\n",
    "    print(\"Accuracy: \", bn_acc.item())\n",
    "    \n",
    "    ## Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n",
    "    r = random.randint(0, len(test_dataset)-1)\n",
    "    X_single_data = test_dataset.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "    Y_single_data = test_dataset.test_labels[r:r + 1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1-2) 지금까지는 Layer의 수를 바꾸거나, Batch Normalization Layer를 추가하는 등 Layer에만 변화를 주며 모델의 성능을 향상 시켰습니다.  \n",
    "### 이번 문제에서는 위에서 만든 모델에서 있던 Layer 들의 Hidden node 수를 증가 또는 감소 (ex: 200, 300, 50...) 시켰을 때, train set에서의 cost와 test set에서 Accuracy가 기존 결과와 비교하였을 때 어떻게 달라졌는지 비교해주시면 됩니다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soo._.yonee/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/soo._.yonee/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/soo._.yonee/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.472852260\n",
      "Epoch: 0002 cost = 0.343448043\n",
      "Epoch: 0003 cost = 0.305057496\n",
      "Epoch: 0004 cost = 0.278459698\n",
      "Epoch: 0005 cost = 0.259482622\n",
      "Epoch: 0006 cost = 0.246392906\n",
      "Epoch: 0007 cost = 0.232810199\n",
      "Epoch: 0008 cost = 0.227727771\n",
      "Epoch: 0009 cost = 0.223600134\n",
      "Epoch: 0010 cost = 0.217849076\n",
      "Epoch: 0011 cost = 0.226236090\n",
      "Epoch: 0012 cost = 0.211904764\n",
      "Epoch: 0013 cost = 0.203420267\n",
      "Epoch: 0014 cost = 0.199457690\n",
      "Epoch: 0015 cost = 0.191901475\n",
      "Learning finished\n",
      "Accuracy:  0.9729997515678406\n"
     ]
    }
   ],
   "source": [
    "# Case 1 : increase the number of hidden nodes\n",
    "# Accuracy Increased :)\n",
    "l1 = nn.Linear(784, 200, bias=True)\n",
    "l2 = nn.Linear(200, 300, bias=True)\n",
    "l3 = nn.Linear(300, 10, bias=True)\n",
    "relu = nn.ReLU()\n",
    "dropout = nn.Dropout(p=0.3)\n",
    "bn1 = nn.BatchNorm1d(200)\n",
    "bn2 = nn.BatchNorm1d(300)\n",
    "\n",
    "nn.init.xavier_uniform(l1.weight)\n",
    "nn.init.xavier_uniform(l2.weight)\n",
    "nn.init.xavier_uniform(l3.weight)\n",
    "\n",
    "model = nn.Sequential(l1, bn1, relu, dropout,\n",
    "                      l2, bn2, relu, dropout,\n",
    "                      l3).to('cpu')\n",
    "\n",
    "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "criterion = nn.CrossEntropyLoss().to('cpu')\n",
    "# optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#cost 계산을 위한 변수 설정\n",
    "train_total_batch = len(train_loader)\n",
    "test_total_batch = len(test_loader)\n",
    "\n",
    "# train\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 784).to('cpu')\n",
    "        Y = Y.to('cpu')\n",
    "        \n",
    "        hypothesis = model(X)\n",
    "        bn_loss = criterion(hypothesis, Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        bn_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        avg_cost += bn_loss / train_total_batch\n",
    "           \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "print('Learning finished')\n",
    "\n",
    "# test\n",
    "with torch.no_grad():\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    \n",
    "    loss, bn_acc = 0, 0\n",
    "    for i, (X, Y) in enumerate(test_loader):\n",
    "        X = X.view(-1, 784).to('cpu')\n",
    "        Y = Y.to('cpu')\n",
    "        \n",
    "        pred = model(X)\n",
    "        correct_pred = torch.argmax(pred, 1) == Y\n",
    "        loss += criterion(pred, Y)\n",
    "        bn_acc += correct_pred.float().mean()\n",
    "    \n",
    "    loss, bn_acc = loss / test_total_batch, bn_acc / test_total_batch\n",
    "    print(\"Accuracy: \", bn_acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soo._.yonee/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/soo._.yonee/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/soo._.yonee/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.518346190\n",
      "Epoch: 0002 cost = 0.379162967\n",
      "Epoch: 0003 cost = 0.342260003\n",
      "Epoch: 0004 cost = 0.316496283\n",
      "Epoch: 0005 cost = 0.312597126\n",
      "Epoch: 0006 cost = 0.289735138\n",
      "Epoch: 0007 cost = 0.280683070\n",
      "Epoch: 0008 cost = 0.286977559\n",
      "Epoch: 0009 cost = 0.275946289\n",
      "Epoch: 0010 cost = 0.264384478\n",
      "Epoch: 0011 cost = 0.257800817\n",
      "Epoch: 0012 cost = 0.252921611\n",
      "Epoch: 0013 cost = 0.243324831\n",
      "Epoch: 0014 cost = 0.254261762\n",
      "Epoch: 0015 cost = 0.242500201\n",
      "Learning finished\n",
      "Accuracy:  0.9689001441001892\n"
     ]
    }
   ],
   "source": [
    "# Case 2 : decrease the number of hidden nodes\n",
    "# Accuracy increased but not as much as in Case 1!\n",
    "l1 = nn.Linear(784, 100, bias=True)\n",
    "l2 = nn.Linear(100, 50, bias=True)\n",
    "l3 = nn.Linear(50, 10, bias=True)\n",
    "relu = nn.ReLU()\n",
    "dropout = nn.Dropout(p=0.3)\n",
    "bn1 = nn.BatchNorm1d(100)\n",
    "bn2 = nn.BatchNorm1d(50)\n",
    "\n",
    "nn.init.xavier_uniform(l1.weight)\n",
    "nn.init.xavier_uniform(l2.weight)\n",
    "nn.init.xavier_uniform(l3.weight)\n",
    "\n",
    "model = nn.Sequential(l1, bn1, relu, dropout,\n",
    "                      l2, bn2, relu, dropout,\n",
    "                      l3).to('cpu')\n",
    "\n",
    "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "criterion = nn.CrossEntropyLoss().to('cpu')\n",
    "# optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#cost 계산을 위한 변수 설정\n",
    "train_total_batch = len(train_loader)\n",
    "test_total_batch = len(test_loader)\n",
    "\n",
    "# train\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 784).to('cpu')\n",
    "        Y = Y.to('cpu')\n",
    "        \n",
    "        hypothesis = model(X)\n",
    "        bn_loss = criterion(hypothesis, Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        bn_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        avg_cost += bn_loss / train_total_batch\n",
    "           \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "print('Learning finished')\n",
    "\n",
    "# test\n",
    "with torch.no_grad():\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    \n",
    "    loss, bn_acc = 0, 0\n",
    "    for i, (X, Y) in enumerate(test_loader):\n",
    "        X = X.view(-1, 784).to('cpu')\n",
    "        Y = Y.to('cpu')\n",
    "        \n",
    "        pred = model(X)\n",
    "        correct_pred = torch.argmax(pred, 1) == Y\n",
    "        loss += criterion(pred, Y)\n",
    "        bn_acc += correct_pred.float().mean()\n",
    "    \n",
    "    loss, bn_acc = loss / test_total_batch, bn_acc / test_total_batch\n",
    "    print(\"Accuracy: \", bn_acc.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to choose optimal number of neurons?**  \n",
    "https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw  \n",
    "*no clear answer or rule of thumb : find it randomly or by grid search or etc*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
