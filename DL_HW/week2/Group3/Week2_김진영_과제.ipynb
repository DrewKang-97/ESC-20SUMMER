{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week2 과제 - 김진영"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1-1) 아래에 주어진 주석을 기반으로 하여 코딩을 해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.512296796\n",
      "Epoch: 0002 cost = 0.375564367\n",
      "Epoch: 0003 cost = 0.340251803\n",
      "Epoch: 0004 cost = 0.310964823\n",
      "Epoch: 0005 cost = 0.296591580\n",
      "Epoch: 0006 cost = 0.290034473\n",
      "Epoch: 0007 cost = 0.281093180\n",
      "Epoch: 0008 cost = 0.275188625\n",
      "Epoch: 0009 cost = 0.267442733\n",
      "Epoch: 0010 cost = 0.273252875\n",
      "Epoch: 0011 cost = 0.258959025\n",
      "Epoch: 0012 cost = 0.248197109\n",
      "Epoch: 0013 cost = 0.249155253\n",
      "Epoch: 0014 cost = 0.240713373\n",
      "Epoch: 0015 cost = 0.242880628\n",
      "Learning finished\n",
      "Accuracy:  96.67001342773438\n",
      "Label:  2\n",
      "Prediction:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 설정 (learning rate, training epochs, batch_size)\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# train과 test set으로 나누어 MNIST data 불러오기\n",
    "mnist_train=dsets.MNIST(root='MNIST_data/',\n",
    "                      train=True,\n",
    "                      transform=transforms.ToTensor(),\n",
    "                      download=True)\n",
    "mnist_test=dsets.MNIST(root='MNIST_data/',\n",
    "                      train=False,\n",
    "                      transform=transforms.ToTensor(),\n",
    "                      download=True)\n",
    "\n",
    "# dataset loader에 train과 test할당하기\n",
    "# (batch size, shuffle, drop_last 잘 설정할 것!)\n",
    "train_loader = DataLoader(dataset=mnist_train,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=mnist_test,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         drop_last=True)\n",
    "\n",
    "\n",
    "# Layer 쌓기 \n",
    "# (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n",
    "# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n",
    "linear1=nn.Linear(784,100,bias=True)\n",
    "linear2=nn.Linear(100,100,bias=True)\n",
    "linear3=nn.Linear(100,10,bias=True)\n",
    "dropout=nn.Dropout(p=0.3)\n",
    "relu=nn.ReLU()\n",
    "bn1=nn.BatchNorm1d(100)\n",
    "bn2=nn.BatchNorm1d(100)\n",
    "\n",
    "#xavier initialization을 이용하여 각 layer의 weight 초기화\n",
    "nn.init.xavier_uniform_(linear1.weight)\n",
    "nn.init.xavier_uniform_(linear2.weight)\n",
    "nn.init.xavier_uniform_(linear3.weight)\n",
    "\n",
    "# torch.nn.Sequential을 이용하여 model 정의하기\n",
    "# (쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n",
    "bn_model=torch.nn.Sequential(linear1,bn1,relu,dropout,\n",
    "                         linear2,bn2,relu,dropout,\n",
    "                         linear3)\n",
    "\n",
    "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "criterion=nn.CrossEntropyLoss() # softmax는 안에 포함\n",
    "\n",
    "#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "optimizer=optim.Adam(bn_model.parameters(),lr=learning_rate)\n",
    "\n",
    "#cost 계산을 위한 변수 설정\n",
    "train_total_batch = len(train_loader)\n",
    "\n",
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
    "\n",
    "bn_model.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "\n",
    "#train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
    "    for X, Y in train_loader:\n",
    "        X=X.view(-1,28*28)\n",
    "        Y=Y\n",
    "        \n",
    "        hypothesis=bn_model(X)\n",
    "        bn_loss=criterion(hypothesis,Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        bn_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        avg_cost += bn_loss / train_total_batch\n",
    "        \n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')\n",
    "\n",
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "#accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "\n",
    "with torch.no_grad():\n",
    "    bn_model.eval() # Set the model to evaluation mode\n",
    "    bn_loss, bn_acc=0,0 # loss, accuracy 초기값 0으로 설정\n",
    "    \n",
    "    for X, Y in test_loader:\n",
    "        X=X.view(-1,28*28)\n",
    "        Y=Y\n",
    "        \n",
    "        bn_prediction=bn_model(X)\n",
    "        bn_correct_prediction=torch.argmax(bn_prediction,1)==Y\n",
    "        bn_loss+=criterion(bn_prediction,Y)\n",
    "        bn_acc+=bn_correct_prediction.float().mean()\n",
    "    print(\"Accuracy: \", bn_acc.item())\n",
    "    \n",
    "    ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = bn_model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1-2) 지금까지는 Layer의 수를 바꾸거나, Batch Normalization Layer를 추가하는 등 Layer에만 변화를 주며 모델의 성능을 향상 시켰습니다.\n",
    "### 이번 문제에서는 위에서 만든 모델에서 있던 Layer 들의 Hidden node 수를 증가 또는 감소 (ex: 200, 300, 50...) 시켰을 때, train set에서의 cost와 test set에서 Accuracy가 기존 결과와 비교하였을 때 어떻게 달라졌는지 비교해주시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) hidden node 수 증가 (100->200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.477384955\n",
      "Epoch: 0002 cost = 0.331772596\n",
      "Epoch: 0003 cost = 0.292640299\n",
      "Epoch: 0004 cost = 0.278033406\n",
      "Epoch: 0005 cost = 0.262251168\n",
      "Epoch: 0006 cost = 0.256022990\n",
      "Epoch: 0007 cost = 0.239788041\n",
      "Epoch: 0008 cost = 0.236049607\n",
      "Epoch: 0009 cost = 0.228607759\n",
      "Epoch: 0010 cost = 0.222600222\n",
      "Epoch: 0011 cost = 0.216132000\n",
      "Epoch: 0012 cost = 0.213803470\n",
      "Epoch: 0013 cost = 0.204192325\n",
      "Epoch: 0014 cost = 0.210139528\n",
      "Epoch: 0015 cost = 0.198376372\n",
      "Learning finished\n",
      "Accuracy:  97.34000396728516\n",
      "Label:  9\n",
      "Prediction:  4\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 설정 (learning rate, training epochs, batch_size)\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# train과 test set으로 나누어 MNIST data 불러오기\n",
    "mnist_train=dsets.MNIST(root='MNIST_data/',\n",
    "                      train=True,\n",
    "                      transform=transforms.ToTensor(),\n",
    "                      download=True)\n",
    "mnist_test=dsets.MNIST(root='MNIST_data/',\n",
    "                      train=False,\n",
    "                      transform=transforms.ToTensor(),\n",
    "                      download=True)\n",
    "\n",
    "# dataset loader에 train과 test할당하기\n",
    "# (batch size, shuffle, drop_last 잘 설정할 것!)\n",
    "train_loader = DataLoader(dataset=mnist_train,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=mnist_test,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         drop_last=True)\n",
    "\n",
    "\n",
    "# Layer 쌓기 \n",
    "# (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n",
    "# 각 Layer의 Hidden node 수 : 1st Layer (784,200), 2nd Layer(200,200),3rd Layer(200,10)\n",
    "linear1=nn.Linear(784,200,bias=True)\n",
    "linear2=nn.Linear(200,200,bias=True)\n",
    "linear3=nn.Linear(200,10,bias=True)\n",
    "dropout=nn.Dropout(p=0.3)\n",
    "relu=nn.ReLU()\n",
    "bn1=nn.BatchNorm1d(200)\n",
    "bn2=nn.BatchNorm1d(200)\n",
    "\n",
    "#xavier initialization을 이용하여 각 layer의 weight 초기화\n",
    "nn.init.xavier_uniform_(linear1.weight)\n",
    "nn.init.xavier_uniform_(linear2.weight)\n",
    "nn.init.xavier_uniform_(linear3.weight)\n",
    "\n",
    "# torch.nn.Sequential을 이용하여 model 정의하기\n",
    "# (쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n",
    "bn_model=torch.nn.Sequential(linear1,bn1,relu,dropout,\n",
    "                         linear2,bn2,relu,dropout,\n",
    "                         linear3)\n",
    "\n",
    "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "criterion=nn.CrossEntropyLoss() # softmax는 안에 포함\n",
    "\n",
    "#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "optimizer=optim.Adam(bn_model.parameters(),lr=learning_rate)\n",
    "\n",
    "#cost 계산을 위한 변수 설정\n",
    "train_total_batch = len(train_loader)\n",
    "\n",
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
    "\n",
    "bn_model.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "\n",
    "#train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
    "    for X, Y in train_loader:\n",
    "        X=X.view(-1,28*28)\n",
    "        Y=Y\n",
    "        \n",
    "        hypothesis=bn_model(X)\n",
    "        bn_loss=criterion(hypothesis,Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        bn_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        avg_cost += bn_loss / train_total_batch\n",
    "        \n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')\n",
    "\n",
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "#accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "\n",
    "with torch.no_grad():\n",
    "    bn_model.eval() # Set the model to evaluation mode\n",
    "    bn_loss, bn_acc=0,0 # loss, accuracy 초기값 0으로 설정\n",
    "    \n",
    "    for X, Y in test_loader:\n",
    "        X=X.view(-1,28*28)\n",
    "        Y=Y\n",
    "        \n",
    "        bn_prediction=bn_model(X)\n",
    "        bn_correct_prediction=torch.argmax(bn_prediction,1)==Y\n",
    "        bn_loss+=criterion(bn_prediction,Y)\n",
    "        bn_acc+=bn_correct_prediction.float().mean()\n",
    "    print(\"Accuracy: \", bn_acc.item())\n",
    "    \n",
    "    ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = bn_model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accuracy가 조금 증가하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) hidden node 수 감소 (100->50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.586803973\n",
      "Epoch: 0002 cost = 0.439989895\n",
      "Epoch: 0003 cost = 0.402204126\n",
      "Epoch: 0004 cost = 0.393706650\n",
      "Epoch: 0005 cost = 0.379411131\n",
      "Epoch: 0006 cost = 0.377140313\n",
      "Epoch: 0007 cost = 0.359739065\n",
      "Epoch: 0008 cost = 0.361200511\n",
      "Epoch: 0009 cost = 0.351111501\n",
      "Epoch: 0010 cost = 0.335307926\n",
      "Epoch: 0011 cost = 0.340453863\n",
      "Epoch: 0012 cost = 0.330862135\n",
      "Epoch: 0013 cost = 0.334065765\n",
      "Epoch: 0014 cost = 0.328688413\n",
      "Epoch: 0015 cost = 0.327466607\n",
      "Learning finished\n",
      "Accuracy:  96.17997741699219\n",
      "Label:  5\n",
      "Prediction:  5\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 설정 (learning rate, training epochs, batch_size)\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# train과 test set으로 나누어 MNIST data 불러오기\n",
    "mnist_train=dsets.MNIST(root='MNIST_data/',\n",
    "                      train=True,\n",
    "                      transform=transforms.ToTensor(),\n",
    "                      download=True)\n",
    "mnist_test=dsets.MNIST(root='MNIST_data/',\n",
    "                      train=False,\n",
    "                      transform=transforms.ToTensor(),\n",
    "                      download=True)\n",
    "\n",
    "# dataset loader에 train과 test할당하기\n",
    "# (batch size, shuffle, drop_last 잘 설정할 것!)\n",
    "train_loader = DataLoader(dataset=mnist_train,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=mnist_test,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         drop_last=True)\n",
    "\n",
    "\n",
    "# Layer 쌓기 \n",
    "# (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n",
    "# 각 Layer의 Hidden node 수 : 1st Layer (784,50), 2nd Layer(50,50),3rd Layer(50,10)\n",
    "linear1=nn.Linear(784,50,bias=True)\n",
    "linear2=nn.Linear(50,50,bias=True)\n",
    "linear3=nn.Linear(50,10,bias=True)\n",
    "dropout=nn.Dropout(p=0.3)\n",
    "relu=nn.ReLU()\n",
    "bn1=nn.BatchNorm1d(50)\n",
    "bn2=nn.BatchNorm1d(50)\n",
    "\n",
    "#xavier initialization을 이용하여 각 layer의 weight 초기화\n",
    "nn.init.xavier_uniform_(linear1.weight)\n",
    "nn.init.xavier_uniform_(linear2.weight)\n",
    "nn.init.xavier_uniform_(linear3.weight)\n",
    "\n",
    "# torch.nn.Sequential을 이용하여 model 정의하기\n",
    "# (쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n",
    "bn_model=torch.nn.Sequential(linear1,bn1,relu,dropout,\n",
    "                         linear2,bn2,relu,dropout,\n",
    "                         linear3)\n",
    "\n",
    "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "criterion=nn.CrossEntropyLoss() # softmax는 안에 포함\n",
    "\n",
    "#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "optimizer=optim.Adam(bn_model.parameters(),lr=learning_rate)\n",
    "\n",
    "#cost 계산을 위한 변수 설정\n",
    "train_total_batch = len(train_loader)\n",
    "\n",
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
    "\n",
    "bn_model.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "\n",
    "#train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
    "    for X, Y in train_loader:\n",
    "        X=X.view(-1,28*28)\n",
    "        Y=Y\n",
    "        \n",
    "        hypothesis=bn_model(X)\n",
    "        bn_loss=criterion(hypothesis,Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        bn_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        avg_cost += bn_loss / train_total_batch\n",
    "        \n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')\n",
    "\n",
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "#accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "\n",
    "with torch.no_grad():\n",
    "    bn_model.eval() # Set the model to evaluation mode\n",
    "    bn_loss, bn_acc=0,0 # loss, accuracy 초기값 0으로 설정\n",
    "    \n",
    "    for X, Y in test_loader:\n",
    "        X=X.view(-1,28*28)\n",
    "        Y=Y\n",
    "        \n",
    "        bn_prediction=bn_model(X)\n",
    "        bn_correct_prediction=torch.argmax(bn_prediction,1)==Y\n",
    "        bn_loss+=criterion(bn_prediction,Y)\n",
    "        bn_acc+=bn_correct_prediction.float().mean()\n",
    "    print(\"Accuracy: \", bn_acc.item())\n",
    "    \n",
    "    ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = bn_model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accuracy가 조금 감소하였다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
