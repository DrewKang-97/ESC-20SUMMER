{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### week2_오다건_세션과제\n",
    "### 1. 아래에 주어진 주석을 기반으로 하여 코딩을 해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파라미터 설정\n",
    "\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd1d9670ed34870b633c631ede09f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a3844097f84d91a0270c05d559bc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910b532caa3347269fa180668d2eb687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8d959d4a6b47959f43a00a54f71c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/distiller/project/conda/conda-bld/pytorch_1591914925853/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    }
   ],
   "source": [
    "#train 과 test set으로 나누어 MNIST 데이터 불러오기\n",
    "\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=True,\n",
    "                         transform = transforms.ToTensor(),\n",
    "                         download =True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                        train=False,\n",
    "                        transform = transforms.ToTensor(),\n",
    "                        download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset loader에 train 과 test 할당하기.\n",
    "#batch size, shuffle(순서를 무작위로),drop_last(크기에 맞지 않는 데이터 drop할지 안할지)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train, \n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test, \n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          drop_last = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기) \n",
    "# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n",
    "\n",
    "linear1 = torch.nn.Linear(784,100, bias=True)\n",
    "linear2 = torch.nn.Linear(100,100, bias=True)\n",
    "linear3 = torch.nn.Linear(100,10, bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "bn1 = torch.nn.BatchNorm1d(100) #number of size\n",
    "bn2 = torch.nn. BatchNorm1d(100)\n",
    "\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \"\"\"\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n",
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.6294e-01,  2.1647e-01, -5.8171e-02,  1.8560e-02,  1.5128e-01,\n",
       "          1.9623e-01,  2.0730e-01,  2.1863e-01, -2.4716e-02,  1.1893e-01,\n",
       "         -1.1394e-02, -2.0730e-01,  2.0492e-01, -1.5781e-01, -1.7326e-01,\n",
       "         -1.7959e-01, -1.1447e-01, -1.4627e-02, -1.3109e-01,  2.9577e-02,\n",
       "         -1.4077e-01, -4.0010e-02,  1.9457e-01, -1.7065e-01, -1.6669e-02,\n",
       "         -2.0035e-01, -1.8170e-02,  2.2263e-02, -7.1494e-02,  1.5544e-02,\n",
       "          1.5608e-01, -1.2987e-01,  8.9370e-02, -5.8783e-02,  1.7699e-01,\n",
       "         -1.6194e-01,  1.0401e-01, -5.8772e-02, -1.2016e-01, -9.3409e-02,\n",
       "         -1.5245e-01,  1.9454e-01, -5.7787e-02, -1.1542e-01, -2.2468e-01,\n",
       "         -1.8898e-01, -3.0636e-02, -3.6729e-02,  1.4153e-01, -1.2008e-01,\n",
       "         -1.6520e-01,  1.1044e-01, -9.8183e-02,  1.7162e-01, -4.7478e-02,\n",
       "          1.7293e-02, -1.0792e-01,  1.1774e-01, -1.6785e-01,  1.4957e-01,\n",
       "         -1.5212e-01,  1.4670e-01,  1.3533e-01, -1.7918e-01, -2.3033e-01,\n",
       "         -1.3298e-01,  4.5211e-02,  1.0923e-01,  1.5489e-01, -2.1616e-01,\n",
       "          7.3558e-02,  2.0616e-01,  1.3114e-01,  2.1134e-01, -1.8669e-01,\n",
       "          2.1129e-01, -1.8441e-01,  2.1350e-01, -2.2376e-01,  1.4197e-01,\n",
       "         -2.2093e-01,  4.2362e-02,  1.0093e-01,  3.1979e-02,  8.7019e-02,\n",
       "          2.3070e-01,  3.9174e-03, -1.6081e-01,  2.2353e-01, -1.5219e-03,\n",
       "          3.6329e-02, -4.7009e-02,  1.4573e-01,  1.2049e-01, -1.7792e-01,\n",
       "         -2.3101e-01,  1.4947e-01, -4.4783e-03,  5.9926e-02, -6.5678e-02],\n",
       "        [-1.1125e-01, -1.2577e-01, -1.4394e-01,  2.0169e-01, -2.2842e-01,\n",
       "          2.1624e-01, -3.9178e-02, -1.1409e-01,  2.0374e-01, -4.5390e-02,\n",
       "         -1.3310e-01,  1.3214e-01, -1.7680e-01, -2.8654e-02,  6.3138e-02,\n",
       "         -1.8905e-01,  5.5275e-02,  2.4563e-02,  4.4239e-02,  1.3466e-02,\n",
       "          5.9914e-02, -2.1361e-01, -1.2284e-01,  1.5757e-01, -1.8274e-01,\n",
       "         -7.4159e-02, -1.4402e-01, -1.1935e-01, -2.2983e-01,  1.2376e-01,\n",
       "         -4.7481e-02,  1.2864e-01,  2.2490e-04,  1.7128e-01,  2.0706e-01,\n",
       "         -4.7832e-02,  6.3070e-03, -4.8220e-02,  4.4866e-03, -5.3488e-02,\n",
       "         -1.9133e-01,  6.6447e-02, -7.7296e-02,  1.9037e-01,  1.6256e-01,\n",
       "          1.5379e-01,  3.2337e-03, -1.0912e-01, -1.8245e-01,  1.9293e-01,\n",
       "          2.3016e-01,  2.2722e-01, -6.0312e-02,  2.1882e-02,  1.4584e-01,\n",
       "         -1.2267e-01, -1.0463e-01, -2.2847e-03,  1.6085e-02,  4.8438e-04,\n",
       "          1.6228e-01, -1.0632e-01, -1.9814e-01,  3.3315e-02,  3.4022e-02,\n",
       "          1.1323e-01, -6.9017e-02, -8.5522e-02, -9.1720e-02,  1.9817e-01,\n",
       "          8.7552e-02, -1.6930e-01,  2.3045e-01, -2.2718e-01, -1.2231e-01,\n",
       "         -4.1798e-02,  1.5845e-01, -1.0508e-01,  1.1348e-01,  2.1434e-01,\n",
       "         -1.9866e-01, -1.0628e-01, -4.7099e-03,  1.6769e-01, -1.8968e-01,\n",
       "         -1.4619e-01,  1.2225e-01,  1.6943e-01,  1.0947e-01,  6.0687e-02,\n",
       "          4.3452e-02,  4.8350e-02,  7.9910e-04,  1.6620e-01,  1.3609e-01,\n",
       "          7.6410e-02, -4.8713e-02, -5.5146e-02,  3.0819e-02, -6.4380e-03],\n",
       "        [ 6.3463e-03, -4.4443e-03, -1.3168e-01, -2.1115e-01, -2.3218e-01,\n",
       "          1.1573e-01,  4.0942e-02,  1.2517e-01,  7.8724e-02, -7.6315e-02,\n",
       "         -5.0963e-02,  1.6768e-01,  8.6382e-02,  2.1427e-01, -5.4558e-02,\n",
       "         -3.3145e-02, -1.5281e-01,  2.0475e-01,  1.2672e-01, -1.0707e-01,\n",
       "         -1.2047e-01, -7.5165e-02,  8.4836e-02,  1.9491e-01,  2.1299e-01,\n",
       "          8.7393e-02, -4.1979e-02, -3.2513e-02,  1.8847e-01,  2.2681e-01,\n",
       "         -2.2491e-01, -1.7095e-01, -1.7448e-02, -1.8363e-01,  1.5784e-01,\n",
       "         -1.0292e-01, -2.1545e-01, -5.2683e-02,  2.1071e-01, -9.6500e-02,\n",
       "         -2.2101e-01,  1.1291e-01, -1.8724e-02, -4.8431e-02, -8.3199e-02,\n",
       "          1.7092e-01, -7.2907e-02,  6.0671e-02,  1.5442e-01, -1.6070e-01,\n",
       "          1.2668e-01, -2.5352e-02, -1.0242e-01,  1.7733e-01, -2.2820e-01,\n",
       "          7.3814e-02,  4.7221e-02,  7.2432e-03, -3.5691e-02, -2.2901e-01,\n",
       "         -2.2955e-01, -8.4596e-02, -1.5978e-01, -2.3686e-02, -3.8911e-02,\n",
       "         -1.5769e-01,  1.7287e-01, -1.3215e-01,  5.2262e-02, -1.0047e-01,\n",
       "          8.3667e-02,  2.4695e-02,  3.7129e-02,  1.1884e-01, -2.2413e-01,\n",
       "          1.1006e-01,  2.1627e-01,  2.4184e-02,  7.2256e-02,  2.0132e-01,\n",
       "          1.0682e-02, -1.5522e-02,  8.8901e-02, -9.9780e-03,  6.4393e-02,\n",
       "          1.6742e-02,  7.0826e-02, -8.5129e-02, -1.7005e-01,  2.6103e-02,\n",
       "         -9.8678e-02,  1.0872e-01, -2.0062e-01, -6.2877e-02, -1.1796e-01,\n",
       "         -1.5509e-01,  1.8224e-01,  2.1159e-02,  7.6313e-02,  8.7398e-03],\n",
       "        [ 6.9326e-02, -5.7314e-02,  1.5075e-01,  7.0655e-02,  1.0542e-01,\n",
       "         -1.6286e-01,  2.8889e-02, -1.9409e-01, -2.2303e-01,  2.2401e-01,\n",
       "         -1.6411e-01,  1.6327e-01, -1.2836e-01,  2.1559e-02,  6.0723e-02,\n",
       "         -1.8758e-01, -2.0086e-01, -5.8772e-02, -2.9841e-02, -8.4354e-02,\n",
       "          7.9505e-02,  8.6081e-03, -1.9543e-01, -4.9134e-03,  1.4363e-01,\n",
       "         -2.9470e-04,  6.0565e-02,  1.1523e-01, -2.3323e-01,  1.8341e-01,\n",
       "          1.7343e-01,  7.3977e-02, -6.4158e-02,  1.7449e-01,  1.2346e-01,\n",
       "         -9.3651e-02, -1.7439e-01,  1.7787e-01, -1.2625e-01,  1.2746e-01,\n",
       "          6.3947e-02,  5.5097e-02,  1.3959e-01, -2.1695e-01,  3.5437e-02,\n",
       "          4.7536e-03, -2.2573e-01, -1.9764e-01, -7.0532e-02,  2.2864e-01,\n",
       "          1.6173e-01,  4.9122e-02,  2.2832e-01, -1.6548e-01,  5.7471e-02,\n",
       "         -1.5854e-02,  4.7253e-02,  2.3225e-01, -1.5460e-01, -1.4696e-01,\n",
       "          1.5370e-01,  3.6015e-02,  1.7840e-01,  1.2841e-01, -1.3597e-01,\n",
       "         -2.1894e-01, -1.5947e-01,  2.5339e-02,  8.4414e-02,  2.1105e-01,\n",
       "          6.6282e-02,  6.3256e-03,  1.5315e-01,  8.7114e-02,  2.2372e-01,\n",
       "         -3.2207e-02, -1.1763e-01, -5.2517e-02,  1.7784e-02,  2.9015e-02,\n",
       "         -5.0050e-02,  2.2285e-01, -1.2304e-01,  2.1711e-01, -7.3785e-02,\n",
       "          8.9695e-02,  6.4807e-02,  1.6721e-01, -6.6794e-02, -8.5476e-02,\n",
       "          1.4502e-01,  9.3701e-02, -1.9114e-01,  1.7173e-01, -4.6618e-02,\n",
       "         -8.7716e-02,  9.3835e-02,  1.5438e-01,  1.9969e-02,  1.7508e-02],\n",
       "        [-9.1730e-02,  2.8661e-02,  1.5667e-01, -1.9007e-01, -2.2777e-01,\n",
       "         -7.6947e-02, -1.1000e-01, -8.1007e-02,  1.0587e-01,  3.7741e-02,\n",
       "         -2.0367e-01,  6.3300e-02,  6.4447e-02,  1.4106e-03, -1.8976e-01,\n",
       "         -1.3093e-01,  1.6057e-01, -2.8100e-02, -1.7197e-01, -4.3790e-02,\n",
       "         -1.2150e-01,  1.5342e-01,  2.2498e-01, -2.0116e-01, -1.1546e-01,\n",
       "          1.7939e-02,  1.8556e-01,  4.0872e-02,  1.9071e-01, -2.2731e-01,\n",
       "          1.8046e-01, -1.3483e-01, -1.7169e-01, -1.5944e-01,  1.3782e-01,\n",
       "          2.2613e-01,  2.4516e-02, -1.5137e-01,  3.2183e-02, -1.0533e-02,\n",
       "          1.1649e-02, -2.5602e-02,  7.3796e-02,  9.3424e-02, -2.0787e-01,\n",
       "         -2.0170e-01,  1.1096e-01, -1.5460e-01, -2.4584e-02, -8.3763e-02,\n",
       "          2.2721e-01, -6.2217e-02,  3.9420e-03,  4.6314e-02,  2.1058e-01,\n",
       "          8.3893e-02, -2.2574e-01,  4.2567e-02, -1.7891e-01,  5.1167e-02,\n",
       "          2.2148e-02, -4.3599e-02, -1.5876e-01, -1.0788e-01, -1.7957e-01,\n",
       "          7.5727e-02, -1.1566e-01, -1.5373e-01,  5.3275e-02,  4.2078e-03,\n",
       "          1.7506e-01,  9.5893e-02,  2.3213e-01,  1.3852e-01, -4.0458e-03,\n",
       "          8.6874e-02,  1.2241e-01, -2.1373e-01,  1.4747e-01, -1.7014e-01,\n",
       "          1.2036e-01,  1.4602e-01, -3.2814e-02,  1.6456e-03, -3.8051e-02,\n",
       "         -2.2370e-01, -1.2879e-01,  6.4031e-02,  5.7505e-02, -6.7910e-03,\n",
       "          1.8158e-01, -8.7761e-02,  1.2654e-01, -1.1892e-01,  2.2650e-01,\n",
       "          1.6894e-01, -1.0313e-01,  8.9491e-02, -9.7762e-03,  8.2655e-02],\n",
       "        [-9.0067e-02, -1.0680e-01,  8.9372e-02, -1.3602e-01,  1.0414e-01,\n",
       "          6.6022e-03,  1.8343e-01, -1.6753e-01,  1.6893e-01,  4.7631e-02,\n",
       "          2.7375e-02,  2.5016e-02, -4.6827e-02, -4.6535e-02, -1.8545e-01,\n",
       "         -1.6654e-02,  2.3441e-02, -1.7546e-01, -4.1924e-02,  2.1914e-01,\n",
       "          7.0859e-02,  4.5009e-02, -3.5970e-02,  6.0495e-02,  3.3340e-03,\n",
       "         -7.2777e-02,  1.1412e-01,  2.1128e-01, -1.9294e-01,  9.5276e-03,\n",
       "          8.4563e-02,  1.8289e-01,  1.0584e-01, -1.3392e-01,  1.3377e-01,\n",
       "          1.0619e-01, -1.3860e-01, -2.1298e-01,  1.9365e-01, -9.7131e-02,\n",
       "         -4.5661e-02,  1.9010e-01,  1.7942e-01,  1.8947e-01, -9.1880e-03,\n",
       "         -1.1662e-01,  1.5874e-01, -1.0537e-01, -1.6644e-01,  2.2850e-01,\n",
       "         -2.8345e-02,  1.8480e-01,  9.1874e-02,  2.0437e-01, -6.4556e-02,\n",
       "          5.5338e-02, -1.2345e-01, -7.6160e-02, -4.3978e-02,  1.6283e-01,\n",
       "          2.7540e-02,  1.4122e-01, -8.6939e-02,  7.7032e-02, -1.2846e-01,\n",
       "         -1.6003e-01,  5.4092e-02,  9.8840e-02, -2.2381e-01, -3.0129e-03,\n",
       "          4.1352e-02, -1.1111e-01,  1.3139e-01, -1.8724e-01, -6.8572e-02,\n",
       "         -1.7579e-01,  1.2085e-01, -1.0214e-01, -2.0769e-01, -2.1152e-01,\n",
       "          1.9197e-01, -8.3844e-02, -8.5321e-02, -3.7498e-02, -6.4914e-02,\n",
       "          7.8267e-02,  2.2140e-01, -9.9567e-02, -1.5593e-01, -9.6449e-02,\n",
       "         -1.7228e-01, -1.9513e-01,  2.3022e-01,  3.3311e-02, -4.0524e-02,\n",
       "          1.6350e-02,  7.9878e-02, -1.4208e-01,  1.2973e-01, -1.1103e-01],\n",
       "        [ 1.9749e-01,  5.9646e-02, -1.6118e-01,  1.6827e-01,  1.8994e-01,\n",
       "          2.0258e-01, -2.0472e-01, -2.1561e-01, -3.4116e-02, -6.3781e-02,\n",
       "         -1.5267e-01, -1.3277e-01, -5.8047e-02,  4.1660e-02, -1.1828e-01,\n",
       "         -1.0093e-01, -1.2560e-01, -2.2081e-01,  2.2934e-01, -2.6988e-02,\n",
       "         -2.0082e-01, -1.4446e-02,  2.0321e-01,  1.0520e-01,  1.0256e-01,\n",
       "          9.3771e-02,  8.8112e-02,  6.7009e-02, -1.8474e-01,  2.1906e-01,\n",
       "         -1.1217e-01, -1.6994e-01, -8.6830e-03, -1.5594e-01, -1.4056e-01,\n",
       "         -1.1247e-01, -6.8816e-02,  1.4295e-01, -2.6286e-02, -7.9098e-02,\n",
       "          3.4859e-02, -1.0187e-01, -2.4386e-02, -1.1897e-01, -3.6915e-02,\n",
       "          1.2502e-01,  1.5898e-01,  2.2698e-01,  4.3041e-03, -2.0571e-01,\n",
       "         -3.4939e-02, -8.0476e-02,  1.7244e-01, -1.8685e-01, -3.5987e-02,\n",
       "         -8.7061e-02,  1.9101e-01,  1.5281e-01,  1.7472e-01,  1.2952e-02,\n",
       "         -7.7906e-02,  5.8597e-02, -1.8831e-01, -1.0050e-01,  1.1529e-01,\n",
       "          1.6722e-01,  2.8569e-03, -6.7248e-03,  7.1689e-02, -3.9117e-02,\n",
       "         -1.3703e-01,  1.9907e-01, -9.1015e-02, -3.8784e-02, -2.1512e-01,\n",
       "         -1.2573e-01, -1.2291e-01,  1.0303e-01,  2.3199e-01,  2.2548e-02,\n",
       "          9.3932e-02,  9.6490e-02, -1.2520e-01,  1.4483e-02, -1.9323e-01,\n",
       "         -1.2480e-01, -1.1425e-01,  4.2859e-02, -1.6966e-01, -1.0130e-01,\n",
       "          1.0795e-01, -6.5719e-02,  1.4913e-01,  2.6225e-02, -1.9792e-01,\n",
       "         -7.9748e-02, -1.8448e-01, -4.1580e-02, -1.0461e-01, -8.8836e-02],\n",
       "        [ 1.4354e-01,  1.7790e-01, -6.9677e-02,  1.9075e-01, -2.9805e-03,\n",
       "          1.2459e-01,  1.0494e-02, -9.2631e-02,  2.3115e-01,  1.1625e-03,\n",
       "         -1.9934e-01,  2.0058e-01, -2.0161e-01,  2.1696e-01, -8.4088e-02,\n",
       "          2.3325e-01,  5.2222e-03,  2.2212e-01, -1.9358e-01,  1.1799e-01,\n",
       "         -4.6835e-02, -3.9629e-02,  2.2961e-01, -4.0400e-02,  8.4030e-02,\n",
       "          1.1175e-01,  1.0416e-01, -7.8337e-02, -1.9187e-01,  9.1491e-02,\n",
       "          1.3673e-01,  1.4262e-01, -2.2609e-01,  5.4460e-02, -1.1640e-01,\n",
       "         -3.1257e-02, -1.8293e-01,  4.7227e-02, -7.9264e-02,  2.2871e-01,\n",
       "         -1.4000e-01,  1.3399e-01, -2.2162e-01,  1.6304e-01,  1.2186e-01,\n",
       "         -1.9575e-01,  1.3729e-02, -2.1971e-01,  6.0993e-02,  1.8720e-01,\n",
       "         -1.7546e-02,  1.3995e-01, -6.0062e-02,  3.9581e-02, -1.1916e-01,\n",
       "          8.9068e-02,  8.7118e-02,  7.8645e-02,  1.7679e-01,  7.7870e-02,\n",
       "          3.6383e-02,  1.1219e-01,  6.6484e-02,  1.9328e-01,  4.9941e-02,\n",
       "          1.9512e-02,  1.7437e-01,  3.2118e-02, -9.8666e-02,  1.6278e-01,\n",
       "          6.7523e-02, -5.7767e-03, -1.3801e-01,  1.7626e-01,  1.3335e-01,\n",
       "         -1.1435e-01,  1.7026e-01,  1.3497e-01, -1.9211e-01, -8.8141e-02,\n",
       "          2.0729e-01, -1.1277e-01,  6.6909e-02,  1.6116e-01, -4.6958e-02,\n",
       "          1.7344e-01,  3.9358e-02, -1.5685e-01,  1.2264e-02, -2.9658e-02,\n",
       "          3.6386e-02, -7.4034e-03, -1.0562e-01,  2.2785e-02,  1.4873e-01,\n",
       "          1.9482e-01,  2.3184e-01, -3.4050e-02,  3.0148e-02, -1.1318e-01],\n",
       "        [ 9.6346e-02, -8.7928e-02, -1.8817e-01, -5.7525e-02,  1.5465e-01,\n",
       "         -7.6472e-02,  1.0025e-01,  3.9711e-02,  2.0616e-01, -2.2073e-01,\n",
       "          2.7310e-02, -2.0909e-01, -1.0381e-01, -3.1619e-02, -1.5169e-02,\n",
       "         -1.1687e-01, -1.2331e-01,  1.7220e-01, -4.4312e-02, -1.4104e-01,\n",
       "         -1.0128e-01, -3.9581e-03, -2.2268e-01,  1.3134e-01, -1.7932e-01,\n",
       "          1.6778e-01,  1.9553e-01, -1.3636e-01, -1.5042e-01, -2.1985e-01,\n",
       "         -2.0368e-01, -4.5560e-03, -2.0172e-01, -1.4509e-01, -6.4522e-02,\n",
       "          2.2490e-03, -5.3126e-02, -2.1411e-01,  2.0008e-01, -6.3254e-02,\n",
       "          1.6982e-01, -1.2696e-01,  2.0324e-01,  1.9684e-01,  1.5580e-01,\n",
       "          4.4028e-02, -1.6809e-01, -1.3113e-01,  7.9028e-02, -8.9934e-02,\n",
       "          1.0201e-01,  1.8755e-01,  1.5472e-01, -9.2698e-02, -1.6148e-01,\n",
       "          1.1856e-01, -4.2362e-02,  7.1121e-02,  6.4316e-02,  1.8186e-01,\n",
       "          1.2549e-01,  9.7389e-02,  3.0820e-03,  2.2283e-01,  1.7827e-01,\n",
       "          5.4958e-02,  1.8103e-02, -7.1822e-02,  1.4654e-01,  4.6728e-02,\n",
       "         -1.1412e-01,  2.1273e-01,  7.4004e-02, -1.2499e-01,  7.7307e-02,\n",
       "          2.1355e-01,  1.3860e-01, -1.9607e-01, -4.9615e-02, -1.2378e-01,\n",
       "          1.4554e-01, -6.7004e-02, -1.6350e-02,  8.8167e-03,  2.7514e-02,\n",
       "         -1.1251e-01,  2.2509e-01,  2.1170e-01,  1.9510e-01, -1.0468e-01,\n",
       "         -2.2655e-01,  1.2684e-01,  1.5842e-01, -1.7107e-01, -1.5009e-01,\n",
       "         -3.7658e-02, -2.7537e-02,  9.7673e-02,  1.8699e-01, -1.6583e-01],\n",
       "        [-5.7067e-02,  6.3331e-02,  1.9712e-01,  1.0962e-02, -3.6968e-02,\n",
       "         -4.0001e-02,  2.3181e-01,  2.5924e-02, -2.1205e-02,  7.5941e-02,\n",
       "         -1.8936e-01,  3.2359e-02, -2.0375e-01, -1.8274e-01, -1.6082e-01,\n",
       "          5.8115e-02,  1.0875e-01, -1.1456e-01,  1.1673e-01,  1.9485e-01,\n",
       "         -1.3432e-01, -9.6891e-02, -4.8357e-04,  6.9646e-02,  2.0760e-01,\n",
       "          1.0687e-01,  1.9853e-01, -2.0596e-01, -1.5475e-01, -4.7879e-02,\n",
       "          7.4589e-02, -8.2925e-02, -5.6936e-02,  3.7007e-02,  1.0249e-01,\n",
       "         -6.0291e-02,  1.1769e-01,  1.1736e-01,  1.5636e-01,  2.3244e-01,\n",
       "         -1.7756e-01,  1.6164e-01,  2.0346e-01,  1.0125e-01,  1.7313e-01,\n",
       "         -8.9706e-02, -1.9872e-01,  1.4609e-01, -3.5154e-03,  2.0866e-01,\n",
       "          1.6225e-01,  1.7673e-01,  5.2007e-02, -1.7698e-01, -9.3011e-02,\n",
       "          2.2211e-01, -1.6664e-01,  1.4595e-01,  1.8799e-01,  1.7354e-01,\n",
       "         -2.1125e-01,  1.4428e-02, -1.3522e-01,  8.4518e-02,  3.0776e-03,\n",
       "          1.3664e-01, -1.5135e-01,  4.0446e-02,  1.5726e-01,  1.9651e-01,\n",
       "          1.7117e-01, -4.6706e-02,  4.1965e-02,  9.9674e-02,  2.7899e-02,\n",
       "          4.6581e-02, -1.0297e-01,  7.0085e-02, -1.3265e-01,  9.6589e-02,\n",
       "         -1.0332e-01, -1.1425e-01, -1.8614e-01, -6.7243e-02,  2.2773e-01,\n",
       "         -1.8214e-01,  6.6846e-02,  1.7872e-01, -1.0599e-01, -1.6204e-01,\n",
       "          6.6005e-02, -1.2147e-01,  1.9836e-01,  2.1603e-01, -1.8565e-02,\n",
       "          2.1551e-01,  2.0886e-01,  1.8160e-01, -1.4043e-01,  2.2912e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xavier initialization 이용해서 각 layer weight 초기화\n",
    "#xavier initialization => 시그모이드의 양 극단이 0으로 수렴해서 역전파의 과정에서\n",
    "#기울기 소실의 문제가 발생할 수  있음.\n",
    "\n",
    "torch.nn.init.xavier_uniform(linear1.weight)\n",
    "torch.nn.init.xavier_uniform(linear2.weight)\n",
    "torch.nn.init.xavier_uniform(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# CUDA 장치 객체(device object)로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#torch.nn.Sequential 을 이용하여  model 정의하기\n",
    "#쌓는 순서에 주의.\n",
    "#linear-batch normalization-relu-dropout 순으로 반복.\n",
    "\n",
    "model = torch.nn.Sequential(linear1,  bn1, relu, dropout, \n",
    "                           linear2, bn2, relu,  dropout,\n",
    "                            linear3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function 정의 하기 (Cross entropy)\n",
    "#optimzer - Adam optimizer\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.497178823\n",
      "Epoch: 0002 cost= 0.375539780\n",
      "Epoch: 0003 cost= 0.336459905\n",
      "Epoch: 0004 cost= 0.313253313\n",
      "Epoch: 0005 cost= 0.305459440\n",
      "Epoch: 0006 cost= 0.294139832\n",
      "Epoch: 0007 cost= 0.281584591\n",
      "Epoch: 0008 cost= 0.277942628\n",
      "Epoch: 0009 cost= 0.266553968\n",
      "Epoch: 0010 cost= 0.263346672\n",
      "Epoch: 0011 cost= 0.259189874\n",
      "Epoch: 0012 cost= 0.244044974\n",
      "Epoch: 0013 cost= 0.254103214\n",
      "Epoch: 0014 cost= 0.240941405\n",
      "Epoch: 0015 cost= 0.238476470\n",
      "Learning finished.\n"
     ]
    }
   ],
   "source": [
    "#cost 계산 위한 변수 설정\n",
    "train_total_batch  = len(train_loader)\n",
    "\n",
    "#Training epoch(cost 값 초기 설정0으로, model의 train 설정 꼭 할것.\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "#train dataset을 불러오고(X,Y 불러오기), \n",
    "#back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
    "\n",
    "    for X,Y in train_loader:\n",
    "        X= X.view(-1,28*28).to(device)\n",
    "        Y= Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost +=cost/ train_total_batch\n",
    "        \n",
    "    print('Epoch:','%04d' % (epoch+1),'cost=', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished.')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9269999861717224\n",
      "Label:  7\n",
      "Prediction:  7\n"
     ]
    }
   ],
   "source": [
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것) \n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것\n",
    "#Y_test를 불러올때 labels사용 #accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval() #예측할때는 사용하지 않으니까.\n",
    "    \n",
    "    #test dataset의 데이터 형태를 batch*784로 바꾸어줌\n",
    "    X_test = mnist_test.test_data.view(-1,28*28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = model(X_test)\n",
    "    \n",
    "    #각 batch별로 가장 높은 가능성의 숫자 클래스를 뽑아주기.\n",
    "    correct_prediction = torch.argmax(prediction,1)==Y_test\n",
    "    accuracy = correct_prediction.float().mean() #맞는 개수의 평균을 내면\n",
    "    print('Accuracy:', accuracy.item()) #정확도가 나온다. \n",
    "    \n",
    "    #get one and predict 랜덤으로 뽑아서 하나만 예측해보자\n",
    "    \n",
    "    r = random.randint(0,len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r+1].view(-1,28*28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r+1].to(device)\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Layer의 hidden node 수를 증가 혹은 감소시켰을 때, train set에서의 cost와 test set에서의 Accuracy가 기존 결과와 비교하였을 때 어떻게 달라졌는지 비교\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 784-> 200-> 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0879, -0.0918,  0.0112,  0.0019, -0.2209, -0.1594, -0.0656,  0.1329,\n",
       "         -0.0562, -0.2196, -0.2279,  0.1385,  0.0089, -0.1217,  0.0519, -0.1211,\n",
       "          0.1869,  0.2175,  0.2267,  0.1387, -0.1145,  0.0657,  0.1789,  0.2026,\n",
       "          0.0292, -0.0987, -0.0872,  0.0947, -0.1448, -0.2189, -0.1077, -0.1147,\n",
       "         -0.1978,  0.1763,  0.0064,  0.0233,  0.1345, -0.1491,  0.1576, -0.1130,\n",
       "          0.0499,  0.1779, -0.1573, -0.1153,  0.0318, -0.1944,  0.2128, -0.1594,\n",
       "          0.2301,  0.1047, -0.1202, -0.0605, -0.2318, -0.2067,  0.1726, -0.1689,\n",
       "         -0.1225, -0.1825,  0.0040,  0.1257,  0.1591, -0.0585,  0.1599,  0.2235,\n",
       "          0.0474, -0.1111, -0.1522,  0.0158,  0.2093,  0.1773,  0.1766,  0.2012,\n",
       "         -0.1817, -0.0970, -0.1383,  0.2292, -0.2226, -0.0837,  0.1164,  0.0331,\n",
       "         -0.0129, -0.1305,  0.0035,  0.1715,  0.1862,  0.1772, -0.1008, -0.0050,\n",
       "          0.1602,  0.0857,  0.1926,  0.0030,  0.0464,  0.2316, -0.1492,  0.1802,\n",
       "         -0.0209,  0.0937, -0.2126,  0.2273],\n",
       "        [ 0.1696, -0.2125,  0.1107, -0.0609,  0.1067, -0.0768, -0.1221, -0.0142,\n",
       "          0.1349,  0.0157, -0.0598,  0.0610,  0.0205,  0.2308, -0.0664, -0.1415,\n",
       "          0.1795,  0.0120,  0.1937, -0.1767, -0.1536,  0.0435, -0.0915,  0.0974,\n",
       "          0.0977, -0.0925,  0.2211, -0.0405,  0.1642,  0.0486,  0.0986,  0.1223,\n",
       "          0.0345, -0.0342, -0.2003,  0.0867,  0.0455, -0.0871, -0.0448, -0.2041,\n",
       "          0.1289, -0.1386, -0.0369,  0.0303,  0.1723, -0.0160, -0.1844, -0.0414,\n",
       "          0.1745,  0.2151, -0.0153, -0.2071,  0.1892,  0.0021,  0.2271, -0.1570,\n",
       "         -0.2305,  0.1553,  0.0773,  0.1927, -0.0235, -0.0053, -0.1295, -0.1502,\n",
       "          0.0770,  0.1889,  0.1412,  0.0261, -0.2027, -0.0500,  0.1437,  0.1388,\n",
       "          0.0635,  0.1727,  0.0479, -0.2186,  0.0261, -0.1909, -0.0780,  0.0103,\n",
       "         -0.0806, -0.0748, -0.0287, -0.0637, -0.2298, -0.1939,  0.1064,  0.2162,\n",
       "          0.2182,  0.1891, -0.0876, -0.0051,  0.1777,  0.1924, -0.0833,  0.1610,\n",
       "         -0.2099, -0.0431, -0.1810,  0.1622],\n",
       "        [-0.1670, -0.0833, -0.0193,  0.1909,  0.1987, -0.2248,  0.0634,  0.0471,\n",
       "         -0.1911,  0.1229, -0.1049, -0.1995, -0.1267, -0.1137, -0.0978,  0.0009,\n",
       "          0.0992,  0.1463, -0.2202, -0.1641, -0.2278, -0.1416, -0.1152, -0.0769,\n",
       "         -0.0558,  0.0895,  0.0778,  0.0129, -0.1372, -0.1289, -0.1661,  0.0749,\n",
       "         -0.0797,  0.0125, -0.0055,  0.0428, -0.0480,  0.1718,  0.1790,  0.1601,\n",
       "          0.0532,  0.1950,  0.1978,  0.1008, -0.0937, -0.2130,  0.0331, -0.1022,\n",
       "         -0.0447, -0.0100, -0.1450, -0.0326,  0.1760,  0.1545,  0.1955, -0.1039,\n",
       "          0.2064, -0.1100, -0.2207,  0.2317,  0.0741,  0.2235,  0.1940,  0.1733,\n",
       "         -0.0023,  0.1390,  0.2218, -0.0848, -0.2273,  0.2208, -0.1496,  0.0519,\n",
       "          0.0653,  0.1517, -0.2127,  0.0141, -0.0752, -0.1721,  0.0654, -0.1162,\n",
       "          0.0118,  0.0769,  0.1375,  0.0045,  0.1152,  0.0354, -0.1711, -0.1000,\n",
       "         -0.0133,  0.2202, -0.0111, -0.0762, -0.0931, -0.0594, -0.0384,  0.2135,\n",
       "          0.0409, -0.0764, -0.0047, -0.0932],\n",
       "        [ 0.0735, -0.1600, -0.1421,  0.0032,  0.1458,  0.0459, -0.0893, -0.0237,\n",
       "          0.1130,  0.0118, -0.0341,  0.1263,  0.0871, -0.1231,  0.1421,  0.1412,\n",
       "         -0.0552,  0.0946, -0.2091, -0.0413, -0.0097,  0.1712,  0.1647, -0.0761,\n",
       "         -0.2210, -0.0257,  0.0110,  0.0893,  0.2288,  0.1890,  0.1367, -0.0293,\n",
       "          0.0898, -0.1478, -0.2071,  0.0892,  0.2211, -0.0276, -0.2278, -0.2063,\n",
       "         -0.1153, -0.2062, -0.1798,  0.0633,  0.1522,  0.0273, -0.1236,  0.0371,\n",
       "         -0.1751,  0.1584,  0.0647,  0.2218, -0.0103, -0.0616, -0.1325,  0.0473,\n",
       "         -0.1725,  0.1135,  0.1399, -0.0068, -0.2041, -0.0580, -0.1264, -0.0483,\n",
       "         -0.0095, -0.0831,  0.0334, -0.1699, -0.0063,  0.1523,  0.0581, -0.1881,\n",
       "         -0.2092,  0.1724,  0.0249, -0.1246,  0.0251,  0.2310, -0.0778, -0.2197,\n",
       "          0.2295,  0.2216,  0.2173,  0.0240,  0.2066, -0.0238,  0.1010,  0.0088,\n",
       "          0.1570, -0.0581,  0.0966, -0.0882,  0.1940, -0.0199, -0.0517, -0.0738,\n",
       "         -0.0893,  0.1822,  0.0024,  0.0154],\n",
       "        [ 0.0299, -0.2171, -0.2334, -0.0460,  0.1792, -0.1636, -0.0748,  0.0594,\n",
       "          0.1068,  0.1672,  0.2056,  0.0919,  0.0582,  0.0935,  0.2006,  0.0048,\n",
       "         -0.1157,  0.0094, -0.0420,  0.1960,  0.0747, -0.2206,  0.0359, -0.2317,\n",
       "         -0.0215,  0.1144,  0.1579,  0.1775, -0.0357, -0.1146,  0.1738,  0.1894,\n",
       "          0.1972,  0.1877, -0.1774,  0.2264, -0.2218,  0.1567, -0.1912,  0.0377,\n",
       "         -0.0421, -0.2017, -0.0800,  0.1338,  0.1050,  0.1274, -0.1374, -0.1090,\n",
       "          0.1640, -0.0075,  0.2123,  0.0155, -0.0631, -0.0961, -0.0095, -0.1470,\n",
       "         -0.2168,  0.2211, -0.1118, -0.1233,  0.2195, -0.1861, -0.1301,  0.1905,\n",
       "          0.1779,  0.0266,  0.1915, -0.1553,  0.1154,  0.1382,  0.0099, -0.1431,\n",
       "          0.1562, -0.0406, -0.1754,  0.1331,  0.1196, -0.0378, -0.1520, -0.0493,\n",
       "          0.1206,  0.2066, -0.1198, -0.0024,  0.0968,  0.0083,  0.0929,  0.2238,\n",
       "          0.0426, -0.1060, -0.0277,  0.1526, -0.1250, -0.1267, -0.0472, -0.0969,\n",
       "         -0.1927,  0.1678,  0.1689,  0.2157],\n",
       "        [ 0.2205, -0.0502, -0.1867,  0.2054,  0.1356,  0.1115,  0.0568, -0.0836,\n",
       "          0.0855,  0.0796, -0.0842,  0.1151,  0.0469, -0.2049, -0.0074, -0.1066,\n",
       "         -0.0448,  0.0562, -0.1017, -0.1860,  0.0457,  0.0696, -0.0388, -0.1663,\n",
       "          0.1660, -0.0589, -0.1898,  0.1542,  0.0099, -0.0277, -0.2150, -0.2273,\n",
       "         -0.0482,  0.0020,  0.0747, -0.1857, -0.2079, -0.1096, -0.0239,  0.1031,\n",
       "         -0.1931,  0.2266,  0.0792,  0.1836,  0.2122, -0.1872,  0.0660,  0.1710,\n",
       "         -0.0550, -0.0195, -0.0654, -0.1959,  0.1240, -0.0922,  0.2307, -0.1285,\n",
       "         -0.1382, -0.0990, -0.1951, -0.0510,  0.2246,  0.0887,  0.0956,  0.2121,\n",
       "          0.0988,  0.1302, -0.1063, -0.0174,  0.1461, -0.2028,  0.1781,  0.1918,\n",
       "         -0.0429, -0.1569,  0.0985,  0.1774, -0.1679, -0.1571,  0.1551, -0.1629,\n",
       "          0.0136,  0.0150, -0.1950, -0.0219, -0.0875, -0.2210, -0.1499, -0.2197,\n",
       "         -0.1162,  0.0348,  0.2216,  0.0219, -0.0626,  0.1175,  0.2310,  0.0595,\n",
       "         -0.0483, -0.1230,  0.2013, -0.1432],\n",
       "        [-0.0961,  0.1021, -0.2169, -0.1589, -0.1613,  0.0524, -0.1914,  0.1679,\n",
       "          0.1822, -0.0373, -0.0472, -0.2136,  0.0102, -0.0815,  0.1094, -0.1623,\n",
       "         -0.1515, -0.0956, -0.0071, -0.1391,  0.0440, -0.0850, -0.0272,  0.1443,\n",
       "          0.0313, -0.2252, -0.1832,  0.0947, -0.2055, -0.1525, -0.0209, -0.1852,\n",
       "          0.1255,  0.1064,  0.1036,  0.1605,  0.1198,  0.1454,  0.1328,  0.1432,\n",
       "         -0.1657,  0.1320,  0.1678,  0.1527,  0.1991, -0.1991, -0.1550,  0.0394,\n",
       "         -0.0359,  0.1126,  0.1867,  0.1387,  0.1104,  0.0122, -0.2334, -0.2249,\n",
       "          0.0410, -0.1617,  0.1650, -0.1202, -0.1801,  0.1544, -0.1427, -0.1952,\n",
       "         -0.0193,  0.1099, -0.1532,  0.0510, -0.1780,  0.2079, -0.2202,  0.0510,\n",
       "          0.2036,  0.0566,  0.2238,  0.0689,  0.1293,  0.2164, -0.1166, -0.2267,\n",
       "          0.0623, -0.1412, -0.1330,  0.0359,  0.1936,  0.0062, -0.0625,  0.0846,\n",
       "          0.0546, -0.1143,  0.1129,  0.1007, -0.1495,  0.1758, -0.1472, -0.0876,\n",
       "          0.0375,  0.0632,  0.0321, -0.0869],\n",
       "        [-0.1912, -0.1263, -0.0660, -0.0208, -0.0171, -0.0825, -0.1759, -0.0392,\n",
       "         -0.0740,  0.1904, -0.1756, -0.0679, -0.0262,  0.2276, -0.0563, -0.1094,\n",
       "         -0.1874, -0.2056, -0.1401, -0.2283,  0.0656,  0.0032,  0.1888, -0.1169,\n",
       "         -0.2192, -0.1446, -0.0136,  0.0937, -0.0944,  0.2113,  0.2232, -0.1281,\n",
       "         -0.1267, -0.0867,  0.0557, -0.1877,  0.1234,  0.2072, -0.1557, -0.0165,\n",
       "          0.1425,  0.1373, -0.1622,  0.1819,  0.0984,  0.2074,  0.0661, -0.1749,\n",
       "         -0.1718, -0.1384, -0.0450, -0.0696, -0.1332, -0.1876, -0.1545,  0.0972,\n",
       "         -0.0399,  0.0582, -0.1531, -0.1835, -0.1054,  0.0290,  0.0080, -0.2178,\n",
       "         -0.1347,  0.0411, -0.1876,  0.0967,  0.1811, -0.0668,  0.2235,  0.2019,\n",
       "          0.1575, -0.1814, -0.0901, -0.0424, -0.1102, -0.1006,  0.0437, -0.0454,\n",
       "         -0.1973,  0.0403, -0.0572, -0.1610,  0.0861, -0.0549, -0.0634, -0.1313,\n",
       "         -0.2049,  0.1099,  0.1687, -0.0736,  0.1136, -0.1002,  0.0588,  0.2212,\n",
       "          0.1217, -0.2003,  0.2280, -0.1520],\n",
       "        [-0.1356,  0.1765,  0.0198, -0.1638, -0.0435,  0.1297, -0.1773, -0.0570,\n",
       "          0.0702, -0.0965,  0.0323,  0.0507,  0.2081,  0.0142, -0.1863,  0.0961,\n",
       "          0.1293,  0.1000, -0.1243,  0.0100, -0.1136,  0.1482, -0.0468,  0.0198,\n",
       "         -0.0954, -0.0160, -0.1506, -0.2085, -0.0472,  0.0017, -0.1575, -0.1545,\n",
       "         -0.1237,  0.1440,  0.0617, -0.0024, -0.1842, -0.1711,  0.0058,  0.2233,\n",
       "          0.0130,  0.0611, -0.1034, -0.1767, -0.0600, -0.0573, -0.1262,  0.0135,\n",
       "          0.1656, -0.1045,  0.1682,  0.1492, -0.0813,  0.1455,  0.1402, -0.0866,\n",
       "         -0.0014, -0.1630, -0.0404, -0.1648,  0.0375, -0.0111,  0.0982,  0.1076,\n",
       "         -0.0604, -0.1470,  0.2222,  0.0528,  0.2155,  0.1146, -0.0040,  0.1530,\n",
       "          0.0743,  0.0727, -0.2220, -0.1998, -0.1728,  0.1151, -0.0574, -0.2288,\n",
       "         -0.0295,  0.1499, -0.2081, -0.0404,  0.1715,  0.0628,  0.0845,  0.1719,\n",
       "         -0.2062, -0.1288,  0.0956,  0.1625, -0.0698,  0.1106,  0.1783, -0.0795,\n",
       "          0.1803,  0.2093, -0.1455,  0.1224],\n",
       "        [-0.0503, -0.1043,  0.1421,  0.1455, -0.2193,  0.0785, -0.1058, -0.1210,\n",
       "         -0.0066,  0.0576, -0.0119, -0.0892,  0.1271, -0.1810, -0.1409, -0.2135,\n",
       "         -0.2071,  0.2062, -0.1585,  0.1748,  0.1570, -0.1986,  0.0599,  0.0070,\n",
       "         -0.0343, -0.1335, -0.0100, -0.2287, -0.1883, -0.0742, -0.1232,  0.1144,\n",
       "          0.0136, -0.1868, -0.0448, -0.0404,  0.1127,  0.1528,  0.0559, -0.0090,\n",
       "         -0.2145,  0.0411, -0.1609, -0.1666, -0.0546,  0.1548,  0.0206, -0.1724,\n",
       "          0.0489, -0.1198,  0.2254, -0.0703,  0.0891, -0.0693,  0.1119, -0.1096,\n",
       "          0.1692,  0.0815, -0.1836, -0.0621,  0.0421,  0.1890, -0.0651, -0.0974,\n",
       "          0.0979, -0.0909,  0.0074,  0.0073,  0.1608,  0.2321,  0.1622,  0.1144,\n",
       "          0.1025,  0.1788,  0.0447, -0.2167, -0.0484, -0.0961,  0.0119, -0.2010,\n",
       "         -0.0143, -0.1247,  0.0686, -0.0455,  0.0014,  0.2191, -0.0282, -0.1551,\n",
       "          0.1917, -0.0778, -0.1667,  0.1398, -0.1372,  0.1492, -0.1199, -0.1784,\n",
       "         -0.0699,  0.1631, -0.2051, -0.2310]], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear4= torch.nn.Linear(784,200, bias=True)\n",
    "linear5= torch.nn.Linear(200,100, bias=True)\n",
    "linear6 = torch.nn.Linear(100,10, bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "bn3 = torch.nn.BatchNorm1d(200)\n",
    "bn4 =  torch.nn.BatchNorm1d(100)\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "torch.nn.init.xavier_uniform_(linear4.weight)\n",
    "torch.nn.init.xavier_uniform_(linear5.weight)\n",
    "torch.nn.init.xavier_uniform_(linear6.weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1= torch.nn.Sequential(linear4,bn3,relu,dropout,linear5,bn4, relu,dropout,linear6).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model_1.parameters(),lr=learning_rate)\n",
    "train_total_batch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.453175068\n",
      "Epoch: 0002 cost= 0.323464662\n",
      "Epoch: 0003 cost= 0.303170592\n",
      "Epoch: 0004 cost= 0.272519827\n",
      "Epoch: 0005 cost= 0.251503080\n",
      "Epoch: 0006 cost= 0.247146651\n",
      "Epoch: 0007 cost= 0.232310325\n",
      "Epoch: 0008 cost= 0.227555573\n",
      "Epoch: 0009 cost= 0.218427002\n",
      "Epoch: 0010 cost= 0.226032585\n",
      "Epoch: 0011 cost= 0.211749524\n",
      "Epoch: 0012 cost= 0.198414698\n",
      "Epoch: 0013 cost= 0.210980415\n",
      "Epoch: 0014 cost= 0.195357725\n",
      "Epoch: 0015 cost= 0.192274213\n",
      "Learning finished.\n"
     ]
    }
   ],
   "source": [
    "model_1.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X,Y in train_loader:\n",
    "        X = X.view(-1, 28*28).to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model_1(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost/ train_total_batch\n",
    "    \n",
    "    print('Epoch:','%04d' % (epoch+1),'cost=', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished.')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9247000217437744\n",
      "Label:  3\n",
      "Prediction:  3\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model_1.eval() #예측할때는 사용하지 않으니까.\n",
    "    \n",
    "    #test dataset의 데이터 형태를 batch*784로 바꾸어줌\n",
    "    X_test = mnist_test.test_data.view(-1,28*28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = model_1(X_test)\n",
    "    \n",
    "    #각 batch별로 가장 높은 가능성의 숫자 클래스를 뽑아주기.\n",
    "    correct_prediction = torch.argmax(prediction,1)==Y_test\n",
    "    accuracy = correct_prediction.float().mean() #맞는 개수의 평균을 내면\n",
    "    print('Accuracy:', accuracy.item()) #정확도가 나온다. \n",
    "    \n",
    "    #get one and predict 랜덤으로 뽑아서 하나만 예측해보자\n",
    "    \n",
    "    r = random.randint(0,len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r+1].view(-1,28*28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r+1].to(device)\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model_1(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 784->150->50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2821,  0.2988, -0.2087, -0.2392,  0.0100,  0.2381,  0.2699,  0.2631,\n",
       "          0.2873, -0.1684, -0.1720, -0.3120, -0.2315, -0.2136, -0.2963,  0.0095,\n",
       "         -0.2428,  0.2021,  0.0036, -0.1789, -0.0574,  0.2057,  0.2784, -0.2236,\n",
       "         -0.1016, -0.0589, -0.2068,  0.1926,  0.1011, -0.0814, -0.1486, -0.1977,\n",
       "          0.1376,  0.3112,  0.1931,  0.2115, -0.2705, -0.1597, -0.0578,  0.1773,\n",
       "          0.2604, -0.2885, -0.0566, -0.0798,  0.1289, -0.2123,  0.0270,  0.2695,\n",
       "         -0.0945,  0.1202],\n",
       "        [ 0.0057,  0.0930, -0.2098, -0.0623, -0.3081,  0.2761,  0.0706,  0.1768,\n",
       "         -0.0204, -0.1397,  0.2735, -0.1565, -0.2934,  0.2608, -0.0427,  0.0917,\n",
       "          0.1451, -0.1644,  0.0518,  0.1837,  0.0575, -0.2443,  0.0947, -0.0735,\n",
       "         -0.0586, -0.1019, -0.1296,  0.0569, -0.1884, -0.1935,  0.1656,  0.0816,\n",
       "          0.0009, -0.2533,  0.3006, -0.2782, -0.0441, -0.1077, -0.0186,  0.2070,\n",
       "         -0.0375, -0.2339, -0.3158,  0.2258,  0.2048, -0.2742,  0.1117,  0.0163,\n",
       "         -0.0042, -0.2336],\n",
       "        [ 0.1676, -0.0994,  0.2601, -0.0416,  0.0882, -0.1463,  0.2720,  0.0570,\n",
       "         -0.1068,  0.0595, -0.1577, -0.3075, -0.2142,  0.1146, -0.2909,  0.0700,\n",
       "          0.0561,  0.0318,  0.0218,  0.0521, -0.1492,  0.2534, -0.1861,  0.0254,\n",
       "         -0.0699, -0.0701, -0.2086,  0.2288, -0.0746, -0.1680, -0.0474, -0.2665,\n",
       "          0.0096, -0.0469,  0.0882, -0.0596, -0.0484, -0.1430, -0.0587, -0.1094,\n",
       "          0.1145, -0.0100, -0.1336, -0.0408, -0.2522,  0.0786,  0.3000, -0.1038,\n",
       "          0.1549,  0.2651],\n",
       "        [ 0.1271,  0.1286,  0.1375,  0.2905, -0.2971,  0.1180,  0.2396,  0.0544,\n",
       "         -0.1793,  0.1418,  0.2993,  0.2346, -0.2287,  0.2860,  0.1665,  0.1269,\n",
       "         -0.2549, -0.0186, -0.3077,  0.1225, -0.0313,  0.0375, -0.2832, -0.2879,\n",
       "         -0.2466, -0.2761,  0.2719,  0.1825, -0.3099, -0.0834, -0.1381, -0.1422,\n",
       "         -0.2326, -0.0181, -0.1817, -0.1842, -0.0577,  0.0460,  0.2466, -0.2157,\n",
       "          0.1580,  0.0716,  0.3109, -0.2184, -0.2134, -0.3130, -0.2915, -0.2282,\n",
       "          0.2906, -0.0382],\n",
       "        [-0.2074, -0.0086,  0.0500, -0.2453,  0.2450, -0.2512,  0.2625, -0.2595,\n",
       "          0.2818,  0.1493, -0.1385,  0.0931,  0.2216, -0.1777, -0.0443,  0.1056,\n",
       "          0.3110, -0.2786,  0.0710, -0.1583,  0.0488,  0.1566,  0.1669, -0.2500,\n",
       "         -0.2076,  0.2959,  0.2848, -0.1554,  0.3124, -0.2332,  0.0886, -0.0683,\n",
       "          0.1645,  0.2548,  0.1298, -0.0771,  0.0621, -0.1187, -0.1014, -0.1320,\n",
       "         -0.0760, -0.2086, -0.2327, -0.2118,  0.0525,  0.0127,  0.1799, -0.2524,\n",
       "         -0.1723,  0.0233],\n",
       "        [ 0.2996,  0.0084,  0.0298,  0.3130, -0.2801, -0.0969,  0.1596,  0.1422,\n",
       "          0.1588, -0.2489,  0.0728, -0.2246, -0.0641, -0.2190, -0.1480,  0.0129,\n",
       "         -0.1527, -0.1610,  0.0132, -0.0681,  0.2253, -0.0493,  0.3020, -0.2894,\n",
       "          0.0210, -0.0339, -0.0651, -0.2873,  0.0821, -0.0120,  0.2148,  0.2482,\n",
       "         -0.1614,  0.0758, -0.2057,  0.1893, -0.1895, -0.2604, -0.0459,  0.0357,\n",
       "         -0.1142,  0.2830,  0.0899, -0.3090, -0.0248,  0.0419,  0.0991,  0.0294,\n",
       "         -0.2865,  0.1279],\n",
       "        [-0.1902, -0.1925, -0.3095,  0.0386,  0.1002, -0.2155, -0.0809,  0.2727,\n",
       "         -0.1660, -0.2748, -0.2476, -0.1375, -0.1088,  0.0961, -0.1160, -0.1822,\n",
       "          0.0215,  0.1679,  0.0523,  0.3106,  0.2812,  0.0834,  0.2184, -0.0688,\n",
       "         -0.2773,  0.2826,  0.0778, -0.0006,  0.0828, -0.1721, -0.2333,  0.1940,\n",
       "          0.0771,  0.0193, -0.2213, -0.2026,  0.2253,  0.2615, -0.0837, -0.1322,\n",
       "         -0.2926,  0.1120,  0.0275,  0.2476, -0.1438,  0.2389,  0.1892,  0.2336,\n",
       "         -0.0627,  0.0928],\n",
       "        [-0.2833, -0.1862, -0.2025,  0.1826,  0.1756,  0.0050,  0.0853,  0.0468,\n",
       "         -0.2504,  0.1325, -0.2859,  0.2622,  0.1283,  0.2402, -0.1086, -0.2253,\n",
       "          0.0957, -0.0466, -0.0573, -0.0956, -0.0188,  0.1456,  0.1735, -0.1089,\n",
       "         -0.1977, -0.0704,  0.2532,  0.1655, -0.0384,  0.0026,  0.1922,  0.0180,\n",
       "          0.1875,  0.2823, -0.2058,  0.1421, -0.1042, -0.2692,  0.1115, -0.1347,\n",
       "         -0.0674,  0.0920, -0.1595,  0.2105, -0.0111,  0.2382, -0.1472,  0.2476,\n",
       "          0.0820,  0.1624],\n",
       "        [ 0.1137,  0.2680,  0.2528, -0.1155, -0.2333,  0.1510, -0.0642,  0.1833,\n",
       "          0.0977, -0.1613,  0.2349, -0.1073,  0.1948,  0.1789, -0.0745,  0.1779,\n",
       "         -0.2188,  0.0022,  0.1295,  0.1579,  0.1585, -0.0776, -0.1981,  0.0118,\n",
       "         -0.2352, -0.2819, -0.2270, -0.0446, -0.1822,  0.2276, -0.0414,  0.1366,\n",
       "         -0.1407, -0.2732, -0.0595, -0.0028, -0.2601,  0.0404,  0.2106, -0.2573,\n",
       "          0.0922,  0.2950,  0.0335, -0.2068, -0.0304,  0.2691,  0.0622, -0.2318,\n",
       "          0.1464, -0.2970],\n",
       "        [-0.0766,  0.0321,  0.2776,  0.1955,  0.1254,  0.1607, -0.0293,  0.0795,\n",
       "         -0.0784, -0.0067, -0.1263,  0.2631,  0.2569, -0.2021, -0.2359, -0.0249,\n",
       "         -0.2644,  0.2439,  0.1101, -0.2956,  0.0709, -0.0403,  0.0843,  0.1943,\n",
       "         -0.0606, -0.1987,  0.3054,  0.2925, -0.0077,  0.2003, -0.0211, -0.3052,\n",
       "          0.1485, -0.1318, -0.1881,  0.0180, -0.1242, -0.2271,  0.1289, -0.1189,\n",
       "          0.0586,  0.1550, -0.2132,  0.3011,  0.1612,  0.3011, -0.0202,  0.2603,\n",
       "          0.2764, -0.1396]], requires_grad=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear7= torch.nn.Linear(784,150, bias=True)\n",
    "linear8= torch.nn.Linear(150,50, bias=True)\n",
    "linear9 = torch.nn.Linear(50,10, bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "bn5 = torch.nn.BatchNorm1d(150)\n",
    "bn6 =  torch.nn.BatchNorm1d(50)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "torch.nn.init.xavier_uniform_(linear7.weight)\n",
    "torch.nn.init.xavier_uniform_(linear8.weight)\n",
    "torch.nn.init.xavier_uniform_(linear9.weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2= torch.nn.Sequential(linear7,bn5,relu,dropout,linear8,bn6, relu,dropout,linear9).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model_2.parameters(),lr=learning_rate)\n",
    "train_total_batch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.482527375\n",
      "Epoch: 0002 cost= 0.337119251\n",
      "Epoch: 0003 cost= 0.299585283\n",
      "Epoch: 0004 cost= 0.291280389\n",
      "Epoch: 0005 cost= 0.278631896\n",
      "Epoch: 0006 cost= 0.263238013\n",
      "Epoch: 0007 cost= 0.241790026\n",
      "Epoch: 0008 cost= 0.237411425\n",
      "Epoch: 0009 cost= 0.230461031\n",
      "Epoch: 0010 cost= 0.233719453\n",
      "Epoch: 0011 cost= 0.222534403\n",
      "Epoch: 0012 cost= 0.221500278\n",
      "Epoch: 0013 cost= 0.215621382\n",
      "Epoch: 0014 cost= 0.210736290\n",
      "Epoch: 0015 cost= 0.196814656\n",
      "Learning finished.\n"
     ]
    }
   ],
   "source": [
    "model_2.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X,Y in train_loader:\n",
    "        X = X.view(-1, 28*28).to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model_2(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost/ train_total_batch\n",
    "    \n",
    "    print('Epoch:','%04d' % (epoch+1),'cost=', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9269999861717224\n",
      "Label:  5\n",
      "Prediction:  5\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model_2.eval() #예측할때는 사용하지 않으니까.\n",
    "    \n",
    "    #test dataset의 데이터 형태를 batch*784로 바꾸어줌\n",
    "    X_test = mnist_test.test_data.view(-1,28*28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = model(X_test)\n",
    "    \n",
    "    #각 batch별로 가장 높은 가능성의 숫자 클래스를 뽑아주기.\n",
    "    correct_prediction = torch.argmax(prediction,1)==Y_test\n",
    "    accuracy = correct_prediction.float().mean() #맞는 개수의 평균을 내면\n",
    "    print('Accuracy:', accuracy.item()) #정확도가 나온다. \n",
    "    \n",
    "    #get one and predict 랜덤으로 뽑아서 하나만 예측해보자\n",
    "    \n",
    "    r = random.randint(0,len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r+1].view(-1,28*28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r+1].to(device)\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model_2(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
