{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4dxUx53QciTq"
   },
   "source": [
    "Q 1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FOFS47kLab9B"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SqOtCE0mapGZ"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PSGnqvl3asUz"
   },
   "outputs": [],
   "source": [
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eDSMppGdawoI"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ugFgZCUgazRG"
   },
   "outputs": [],
   "source": [
    "linear1 = torch.nn.Linear(784, 100, bias=True)\n",
    "linear2 = torch.nn.Linear(100, 100, bias=True)\n",
    "linear3 = torch.nn.Linear(100, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "bn1 = torch.nn.BatchNorm1d(100)\n",
    "bn2 = torch.nn.BatchNorm1d(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n3br7QDAa05g"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1563, -0.0935,  0.1054,  0.0467, -0.1430,  0.0056,  0.1075,  0.0812,\n",
       "          0.0571,  0.1108,  0.1240, -0.1626, -0.0292, -0.0810,  0.0082,  0.2266,\n",
       "         -0.0368,  0.1834,  0.0775, -0.2137, -0.0682, -0.1343,  0.1931, -0.0976,\n",
       "         -0.0970,  0.2235, -0.0346, -0.0337,  0.0989,  0.1930,  0.1656,  0.0767,\n",
       "         -0.1354,  0.1212, -0.0636,  0.1559, -0.0749,  0.1246, -0.2310,  0.1233,\n",
       "         -0.0399,  0.1057,  0.0362,  0.0362, -0.0462, -0.0024,  0.2250, -0.0211,\n",
       "         -0.1462,  0.0648, -0.1762, -0.1694, -0.1338, -0.1769, -0.1568, -0.1632,\n",
       "         -0.1192,  0.1742, -0.0936,  0.0624, -0.0997,  0.0917,  0.0901,  0.0850,\n",
       "          0.1219, -0.0019,  0.0233, -0.2280,  0.0971,  0.1113,  0.1541,  0.0456,\n",
       "          0.1698,  0.2170, -0.0885,  0.0783, -0.2115, -0.1456, -0.2177, -0.0755,\n",
       "          0.0039,  0.2213, -0.2069, -0.1256,  0.0257,  0.2333,  0.1777, -0.0940,\n",
       "          0.0034, -0.2281, -0.1773,  0.0006, -0.1559,  0.1368, -0.1223, -0.0203,\n",
       "         -0.1816, -0.0217,  0.1700, -0.1633],\n",
       "        [ 0.0678, -0.0649,  0.1128, -0.1643, -0.1647, -0.0945,  0.0983, -0.0284,\n",
       "         -0.0890, -0.1825, -0.0271,  0.1922,  0.1036,  0.0858, -0.0247,  0.1817,\n",
       "         -0.0064, -0.1512,  0.2076, -0.1140,  0.2244,  0.1894, -0.1505, -0.2110,\n",
       "          0.1069, -0.1786,  0.1129,  0.0638, -0.1108,  0.1328,  0.1808, -0.2023,\n",
       "         -0.0215, -0.1967, -0.1093, -0.2218,  0.0965, -0.0731, -0.0997, -0.1765,\n",
       "         -0.0111, -0.0371, -0.1469, -0.0857, -0.1035, -0.1070,  0.2121,  0.0596,\n",
       "         -0.2072, -0.2305, -0.1956, -0.1800,  0.2049,  0.0803,  0.1438, -0.1667,\n",
       "          0.0962,  0.1196, -0.0938,  0.2176, -0.2019, -0.2227,  0.0980, -0.2164,\n",
       "          0.2236, -0.1561, -0.0871,  0.0598, -0.0131, -0.1856, -0.1428, -0.1045,\n",
       "         -0.0743, -0.0265,  0.1464,  0.0359,  0.0980, -0.0237,  0.1594, -0.1850,\n",
       "         -0.1370,  0.0968,  0.1035, -0.0888, -0.0616,  0.1156, -0.1135,  0.1078,\n",
       "         -0.0570, -0.1850,  0.1373,  0.1478,  0.1917, -0.0273, -0.1531, -0.1189,\n",
       "          0.2198,  0.1121, -0.0342,  0.1174],\n",
       "        [-0.1025,  0.1974, -0.0782, -0.1785,  0.1414, -0.0093, -0.1059,  0.1236,\n",
       "         -0.1719,  0.1017,  0.1096, -0.0712,  0.0392, -0.1033, -0.0274, -0.0876,\n",
       "         -0.2251,  0.0279, -0.1983, -0.1444,  0.1865, -0.1406,  0.1388, -0.1753,\n",
       "         -0.1607,  0.0336, -0.0525,  0.0508, -0.1391, -0.1144, -0.1474, -0.1041,\n",
       "         -0.0806,  0.0287, -0.1319,  0.0917,  0.0572, -0.1975, -0.1488,  0.1123,\n",
       "          0.1423,  0.1774,  0.1712, -0.0134, -0.1275, -0.0628,  0.1266, -0.0216,\n",
       "         -0.0912,  0.2247, -0.1569, -0.1801, -0.0341,  0.1073, -0.1805, -0.1727,\n",
       "          0.1109,  0.0342, -0.1445, -0.1581, -0.1743, -0.1856, -0.0277,  0.1906,\n",
       "          0.1887, -0.1728,  0.0857,  0.1151, -0.0523,  0.1139, -0.0682,  0.1695,\n",
       "         -0.0440, -0.2131, -0.0814,  0.1947,  0.0550, -0.0344, -0.0454,  0.2036,\n",
       "          0.1798,  0.1398, -0.1091, -0.0755,  0.2259, -0.0657,  0.0451, -0.0731,\n",
       "         -0.1685, -0.0356, -0.1947,  0.1391,  0.0769,  0.2089, -0.0655,  0.2143,\n",
       "         -0.2087, -0.1068, -0.1008, -0.1537],\n",
       "        [ 0.2054, -0.2194, -0.1522,  0.0665, -0.0149, -0.0182, -0.0462, -0.0828,\n",
       "          0.1074,  0.2014,  0.2118, -0.0886,  0.2018,  0.1221, -0.1718, -0.0971,\n",
       "         -0.0645,  0.0673, -0.1479, -0.1060, -0.1917, -0.0605,  0.0210, -0.0962,\n",
       "         -0.1197, -0.1502, -0.2275, -0.1772,  0.1986, -0.1995,  0.0553,  0.2234,\n",
       "          0.1835, -0.1038,  0.1542,  0.0195,  0.0471, -0.2182,  0.1006,  0.0098,\n",
       "          0.1091,  0.1154, -0.0739, -0.1094, -0.0777, -0.1323,  0.0344,  0.0471,\n",
       "         -0.0414,  0.1775, -0.1773,  0.1931, -0.0449,  0.0991,  0.1931, -0.1529,\n",
       "          0.1660, -0.0572, -0.0845,  0.1078,  0.1150,  0.1535, -0.1617,  0.1474,\n",
       "         -0.1511, -0.0900,  0.1105, -0.1781, -0.0258,  0.0679, -0.1240, -0.1614,\n",
       "          0.0675,  0.1594,  0.0651,  0.2147, -0.1481,  0.1234,  0.1052, -0.2263,\n",
       "          0.0548,  0.0867, -0.0289,  0.2264, -0.0744, -0.2239, -0.1719, -0.0885,\n",
       "         -0.1472, -0.0940,  0.0618,  0.2271, -0.2163,  0.0897, -0.0552,  0.0853,\n",
       "         -0.1915, -0.1804, -0.0949,  0.1905],\n",
       "        [-0.1370,  0.1786,  0.1189,  0.1363, -0.0683,  0.1733, -0.1847, -0.1877,\n",
       "         -0.1282, -0.1655, -0.0582,  0.2200, -0.1805,  0.2107, -0.2060, -0.0361,\n",
       "          0.2068, -0.2182,  0.2123,  0.0476, -0.0398,  0.0812,  0.0702, -0.0335,\n",
       "          0.0408,  0.2210, -0.0638, -0.1520, -0.2118, -0.1535, -0.1388,  0.1524,\n",
       "         -0.0570, -0.0353,  0.0187,  0.0690, -0.0078,  0.0688,  0.1972, -0.1851,\n",
       "         -0.0580,  0.0223, -0.0115, -0.0042, -0.0307,  0.2070,  0.1682,  0.2323,\n",
       "          0.0191,  0.1611, -0.1749,  0.1673,  0.1725, -0.0269, -0.0919,  0.1443,\n",
       "         -0.2324,  0.1040,  0.1493, -0.0462,  0.1015, -0.1034, -0.2071,  0.0379,\n",
       "          0.1192,  0.1657, -0.1564,  0.2156, -0.0856, -0.0628,  0.2245,  0.1101,\n",
       "         -0.0751, -0.0745,  0.2032,  0.2212, -0.2131,  0.1218,  0.0283,  0.0496,\n",
       "          0.0782,  0.0567, -0.1173,  0.1462,  0.1030,  0.0254,  0.1803,  0.0014,\n",
       "          0.0071, -0.1300,  0.0219, -0.1648, -0.0504,  0.2020,  0.1778,  0.0237,\n",
       "         -0.0978, -0.0617, -0.1110,  0.0776],\n",
       "        [-0.1093, -0.1856,  0.1002,  0.0019, -0.0987, -0.2314, -0.0299,  0.0661,\n",
       "          0.1600, -0.1557, -0.0944, -0.2298, -0.2238,  0.2166,  0.2056, -0.0944,\n",
       "         -0.0167, -0.0705, -0.2334,  0.1737,  0.0100,  0.2141, -0.1264, -0.2154,\n",
       "         -0.1038,  0.1616,  0.1588, -0.0246, -0.0717,  0.0133, -0.2043,  0.1965,\n",
       "          0.1490,  0.1134, -0.0239, -0.0648, -0.1247,  0.0923,  0.0920,  0.1694,\n",
       "          0.0238,  0.1569,  0.1264,  0.0235, -0.0026, -0.0316,  0.0282,  0.1762,\n",
       "         -0.1773, -0.0236,  0.0918, -0.2220, -0.1339, -0.0261,  0.0360, -0.1416,\n",
       "          0.1330,  0.0571, -0.0981,  0.0658,  0.0491, -0.1792, -0.0598, -0.2086,\n",
       "          0.1652,  0.0933, -0.0530, -0.0991, -0.0238, -0.0523, -0.0825,  0.0322,\n",
       "         -0.0904,  0.1952,  0.1799, -0.0735, -0.0695, -0.1970,  0.0069, -0.0488,\n",
       "         -0.0011, -0.1227, -0.0399,  0.0385, -0.0096,  0.0369,  0.1352,  0.0199,\n",
       "          0.0626, -0.1516, -0.1202, -0.0014, -0.0872, -0.2328,  0.0765,  0.0714,\n",
       "         -0.2284, -0.1109, -0.0141,  0.1278],\n",
       "        [ 0.0829,  0.1194, -0.1532, -0.2203,  0.0715,  0.0433,  0.1223,  0.1735,\n",
       "          0.1444, -0.1090,  0.0288, -0.0368, -0.1392, -0.0110, -0.0159, -0.0093,\n",
       "         -0.0956,  0.0006, -0.1262, -0.0934, -0.0543, -0.1301,  0.0114,  0.0187,\n",
       "         -0.1211, -0.0744, -0.0195,  0.0710, -0.1882, -0.0759, -0.2032,  0.1848,\n",
       "          0.0643,  0.0603,  0.1174, -0.1377, -0.0770,  0.1936,  0.1377, -0.1844,\n",
       "          0.0746,  0.1855,  0.0575,  0.0352,  0.0386, -0.2248,  0.2319,  0.1259,\n",
       "          0.0534, -0.1056, -0.0661, -0.2163, -0.1128, -0.1204, -0.1628, -0.0492,\n",
       "         -0.0315,  0.1716,  0.0868,  0.0611, -0.1445, -0.2074, -0.2232, -0.2098,\n",
       "          0.2025,  0.0544,  0.0865, -0.1334, -0.0423, -0.1600, -0.2186,  0.0114,\n",
       "          0.2242, -0.1022, -0.0411,  0.1253, -0.0590, -0.2151, -0.0919,  0.0792,\n",
       "         -0.1906, -0.0544,  0.0077, -0.0253,  0.2210, -0.0227, -0.0714, -0.0545,\n",
       "          0.1414,  0.1466, -0.1497,  0.2254,  0.0782, -0.0519,  0.1088, -0.0518,\n",
       "         -0.0310,  0.2263, -0.1291,  0.1638],\n",
       "        [-0.0467, -0.1454, -0.0992, -0.2294,  0.0337, -0.1368,  0.0657, -0.0138,\n",
       "          0.1512, -0.2022,  0.0008,  0.2047,  0.0653, -0.1449, -0.0191, -0.0149,\n",
       "         -0.1483,  0.1935,  0.1914,  0.0586,  0.0186,  0.1484, -0.1751,  0.1965,\n",
       "          0.0275,  0.1679,  0.1307,  0.2211, -0.2093,  0.1983,  0.0787,  0.0378,\n",
       "         -0.0716, -0.2252,  0.1849,  0.0264, -0.1288,  0.0392,  0.0069,  0.0763,\n",
       "          0.0272,  0.1290,  0.0830,  0.2160,  0.0604,  0.1635,  0.1082, -0.0803,\n",
       "          0.2078,  0.0445,  0.1556, -0.0598, -0.2114, -0.1734, -0.0058,  0.1224,\n",
       "         -0.1274, -0.0332, -0.2108, -0.0469, -0.0512, -0.0531,  0.0978,  0.0300,\n",
       "         -0.1687,  0.1783, -0.1016, -0.0432,  0.0165,  0.0045,  0.0396,  0.1427,\n",
       "         -0.1617,  0.0512,  0.0554,  0.1451,  0.1079, -0.0569,  0.1118, -0.1139,\n",
       "         -0.0459, -0.1050, -0.1304, -0.0337, -0.0541,  0.1340, -0.0684,  0.0446,\n",
       "          0.0322, -0.1256,  0.1425,  0.1589,  0.0535,  0.1380, -0.0270, -0.1237,\n",
       "          0.0296,  0.1470,  0.0944, -0.1445],\n",
       "        [ 0.1277, -0.0532, -0.1233,  0.0701, -0.0506, -0.0795, -0.0340,  0.2279,\n",
       "          0.1307, -0.1297,  0.0861, -0.0590, -0.1493,  0.1714,  0.0706, -0.0160,\n",
       "         -0.1553,  0.0397,  0.0760, -0.1792, -0.1859, -0.1511, -0.1778, -0.0769,\n",
       "          0.0167, -0.0793,  0.2023,  0.2125,  0.2312, -0.0416, -0.2068,  0.1647,\n",
       "         -0.1069, -0.1505,  0.1879,  0.1132, -0.2119, -0.1083, -0.1312,  0.0477,\n",
       "         -0.1233,  0.1896, -0.0656, -0.0705,  0.1738, -0.0004,  0.1693,  0.1003,\n",
       "         -0.0982, -0.2052, -0.2324, -0.0356, -0.2244, -0.0201, -0.0142, -0.1992,\n",
       "          0.0317,  0.1684, -0.2196, -0.0344,  0.0874, -0.1919,  0.1211,  0.0549,\n",
       "         -0.2235,  0.0627,  0.1285, -0.2166,  0.1504,  0.0275,  0.2118, -0.0755,\n",
       "         -0.1116, -0.0607,  0.1362,  0.0887,  0.2006, -0.1821,  0.1460, -0.1162,\n",
       "          0.0096,  0.1522, -0.2290,  0.1315, -0.1011,  0.1808,  0.0184, -0.1522,\n",
       "         -0.0775, -0.1091, -0.1426,  0.0649, -0.1904,  0.0548,  0.1405,  0.1063,\n",
       "          0.0029, -0.0398,  0.0015,  0.0065],\n",
       "        [-0.1091,  0.1036, -0.2199,  0.1853,  0.1299,  0.1103,  0.1300, -0.0146,\n",
       "         -0.0180, -0.2257, -0.2214,  0.0038,  0.2322, -0.0703, -0.0148,  0.1292,\n",
       "          0.0347, -0.0828, -0.1000,  0.0683,  0.1996, -0.0293,  0.2329, -0.1115,\n",
       "          0.1194,  0.0088,  0.2090,  0.2307, -0.1541,  0.1433, -0.1097, -0.1763,\n",
       "         -0.1993, -0.0197, -0.2246, -0.2195, -0.1338, -0.1154, -0.0098,  0.0804,\n",
       "         -0.1289,  0.1725,  0.1276, -0.2112,  0.2325,  0.1421,  0.1539, -0.0463,\n",
       "         -0.0969, -0.1098,  0.0633, -0.0810, -0.0547, -0.0534,  0.0664, -0.1108,\n",
       "         -0.0645, -0.0226, -0.1271, -0.0691, -0.2281, -0.1515,  0.0069, -0.0310,\n",
       "         -0.0063,  0.1770, -0.2079,  0.1637,  0.0637, -0.0526,  0.0730, -0.2186,\n",
       "         -0.1808,  0.0195,  0.1211,  0.1841, -0.2093, -0.1820,  0.0154,  0.2134,\n",
       "          0.0179,  0.0473, -0.1294,  0.0668, -0.1805,  0.1897,  0.0565,  0.0816,\n",
       "          0.0152,  0.1105,  0.0969,  0.1169,  0.2234,  0.1978,  0.0740,  0.1854,\n",
       "         -0.0075,  0.0284, -0.0611, -0.0713]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cGx5lqZ2a3Qz"
   },
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n",
    "                            linear2, bn2, relu, dropout,\n",
    "                            linear3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1FMYPeBPbBHq"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16idzsoJbCRO"
   },
   "outputs": [],
   "source": [
    "train_total_batch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WsgRWSo5bEJj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.514677465\n",
      "Epoch: 0002 cost = 0.375588477\n",
      "Epoch: 0003 cost = 0.334664613\n",
      "Epoch: 0004 cost = 0.330641806\n",
      "Epoch: 0005 cost = 0.310495287\n",
      "Epoch: 0006 cost = 0.296072185\n",
      "Epoch: 0007 cost = 0.289916933\n",
      "Epoch: 0008 cost = 0.278093159\n",
      "Epoch: 0009 cost = 0.274405301\n",
      "Epoch: 0010 cost = 0.267669708\n",
      "Epoch: 0011 cost = 0.251140743\n",
      "Epoch: 0012 cost = 0.254799694\n",
      "Epoch: 0013 cost = 0.241282478\n",
      "Epoch: 0014 cost = 0.244392082\n",
      "Epoch: 0015 cost = 0.252659291\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    model.train()\n",
    "    avg_cost=0\n",
    "    \n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 28 * 28)\n",
    "        Y = Y\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(X)\n",
    "        loss = criterion(prediction, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += loss / train_total_batch\n",
    "\n",
    "    \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "    avg_cost_o=avg_cost\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJfuVvfobFfn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9700000286102295\n",
      "Label:  5\n",
      "Prediction:  5\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    for X_test, Y_test in test_loader:\n",
    "        X_test = X.view(-1, 28 * 28)\n",
    "        Y_test = Y\n",
    "\n",
    "        prediction = model(X_test)\n",
    "        correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "        accuracy = correct_prediction.float().mean()\n",
    "        \n",
    "    print('Accuracy:', accuracy.item())\n",
    "    accuracy_o= accuracy\n",
    "    \n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.data[r:r + 1].view(-1, 28 *28).float()\n",
    "    Y_single_data = mnist_test.targets[r:r + 1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w6UU4Ti9cbEq"
   },
   "source": [
    "Q.1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-HKvpjcbgkj"
   },
   "outputs": [],
   "source": [
    "a1 = torch.nn.Linear(784, 200, bias=True)\n",
    "a2 = torch.nn.Linear(200, 50, bias=True)\n",
    "a3 = torch.nn.Linear(50, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "a_bn1 = torch.nn.BatchNorm1d(200)\n",
    "a_bn2 = torch.nn.BatchNorm1d(50)\n",
    "\n",
    "torch.nn.init.xavier_uniform_(a1.weight)\n",
    "torch.nn.init.xavier_uniform_(a2.weight)\n",
    "torch.nn.init.xavier_uniform_(a3.weight)\n",
    "\n",
    "model_a = torch.nn.Sequential(a1, a_bn1, relu, dropout,\n",
    "                            a2, a_bn2, relu, dropout,\n",
    "                            a3)\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer_a= torch.optim.Adam(model_a.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8hsTXwvcZES"
   },
   "outputs": [],
   "source": [
    "b1 = torch.nn.Linear(784, 50, bias=True)\n",
    "b2 = torch.nn.Linear(50, 200, bias=True)\n",
    "b3 = torch.nn.Linear(200, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "b_bn1 = torch.nn.BatchNorm1d(50)\n",
    "b_bn2 = torch.nn.BatchNorm1d(200)\n",
    "\n",
    "torch.nn.init.xavier_uniform_(b1.weight)\n",
    "torch.nn.init.xavier_uniform_(b2.weight)\n",
    "torch.nn.init.xavier_uniform_(b3.weight)\n",
    "\n",
    "model_b=torch.nn.Sequential(b1, b_bn1, relu, dropout,\n",
    "                            b2, b_bn2, relu, dropout,\n",
    "                            b3)\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer_b= torch.optim.Adam(model_b.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HcEIXazcbluT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost a = 0.454322249 cost b = 0.595459640\n",
      "Epoch: 0002 cost a = 0.321224034 cost b = 0.445810199\n",
      "Epoch: 0003 cost a = 0.281915009 cost b = 0.399279535\n",
      "Epoch: 0004 cost a = 0.267496496 cost b = 0.390589982\n",
      "Epoch: 0005 cost a = 0.251137227 cost b = 0.363779634\n",
      "Epoch: 0006 cost a = 0.237777486 cost b = 0.353661001\n",
      "Epoch: 0007 cost a = 0.230094030 cost b = 0.348468542\n",
      "Epoch: 0008 cost a = 0.230931133 cost b = 0.358975500\n",
      "Epoch: 0009 cost a = 0.218487144 cost b = 0.337771207\n",
      "Epoch: 0010 cost a = 0.218673661 cost b = 0.335367590\n",
      "Epoch: 0011 cost a = 0.211706504 cost b = 0.332377315\n",
      "Epoch: 0012 cost a = 0.205053702 cost b = 0.328902960\n",
      "Epoch: 0013 cost a = 0.209226817 cost b = 0.326631099\n",
      "Epoch: 0014 cost a = 0.191822767 cost b = 0.310908943\n",
      "Epoch: 0015 cost a = 0.190380782 cost b = 0.305071056\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    model_a.train()\n",
    "    model_b.train()\n",
    "    avg_cost_a=0\n",
    "    avg_cost_b=0\n",
    "    loss_a=0\n",
    "    loss_b=0\n",
    "    \n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 28 * 28)\n",
    "        Y = Y\n",
    "\n",
    "        optimizer_a.zero_grad()\n",
    "        prediction_a = model_a(X)\n",
    "        loss_a = criterion(prediction_a, Y)\n",
    "        loss_a.backward()\n",
    "        optimizer_a.step()\n",
    "        \n",
    "        avg_cost_a += loss_a / train_total_batch\n",
    "                \n",
    "        optimizer_b.zero_grad()\n",
    "        prediction_b = model_b(X)\n",
    "        loss_b = criterion(prediction_b, Y)\n",
    "        loss_b.backward()\n",
    "        optimizer_b.step()\n",
    "        \n",
    "        avg_cost_b += loss_b / train_total_batch\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost a =', '{:.9f}'.format(avg_cost_a),'cost b =', '{:.9f}'.format(avg_cost_b))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zKI3TK_XbnnH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_a: 0.9900000095367432 Accuracy_b 0.9800000190734863\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model_a.eval()\n",
    "    model_b.eval()\n",
    "    \n",
    "    for X_test, Y_test in test_loader:\n",
    "        X_test = X.view(-1, 28 * 28)\n",
    "        Y_test = Y\n",
    "\n",
    "        prediction_a = model_a(X_test)\n",
    "        correct_prediction_a = torch.argmax(prediction_a, 1) == Y_test\n",
    "        accuracy_a = correct_prediction_a.float().mean()\n",
    "\n",
    "        prediction_b = model_b(X_test)\n",
    "        correct_prediction_b = torch.argmax(prediction_b, 1) == Y_test\n",
    "        accuracy_b = correct_prediction_b.float().mean()\n",
    "        \n",
    "    print('Accuracy_a:', accuracy_a.item(), 'Accuracy_b', accuracy_b.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6yrJl9xcH3Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_o: 0.9700000286102295 Accuracy_a: 0.9900000095367432 Accuracy_b 0.9800000190734863\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy_o:', accuracy_o.item(), 'Accuracy_a:', accuracy_a.item(), 'Accuracy_b', accuracy_b.item())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
