{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2\n",
    "### Minju Jo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정 (learning rate, training epochs, batch_size)\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "#train과 test set으로 나누어 MNIST data 불러오기\n",
    "\n",
    "mnist_train = dsets.MNIST(root = './data', train = True,\n",
    "                        transform = transforms.ToTensor(),\n",
    "                        download = True)\n",
    "mnist_test = dsets.MNIST(root = './data', train = False,\n",
    "                       transform = transforms.ToTensor(),\n",
    "                       download = True)\n",
    "\n",
    "\n",
    "#dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.0766e-01, -5.1050e-02, -1.1665e-01,  1.3228e-01, -1.1840e-01,\n",
       "          3.3563e-02, -1.0127e-01,  1.3994e-01,  1.4761e-01, -2.2170e-01,\n",
       "          2.5772e-02, -2.1666e-02,  1.8241e-02,  5.0133e-02, -2.2530e-01,\n",
       "         -2.2008e-01,  6.7254e-02,  1.1799e-01,  1.2317e-01,  3.9237e-02,\n",
       "         -1.4996e-01,  1.1901e-02,  5.8836e-02, -1.7138e-01,  2.1557e-01,\n",
       "         -1.9901e-01, -2.1536e-01,  5.0881e-02,  2.1389e-01, -2.6479e-02,\n",
       "         -2.0792e-01,  1.8087e-01, -1.5557e-01, -6.8209e-02, -1.6699e-01,\n",
       "         -8.6963e-02, -1.7932e-01,  1.8026e-01, -2.1254e-01, -6.1511e-02,\n",
       "          1.2330e-01,  6.0720e-02,  1.4240e-02, -2.0966e-01, -7.0852e-03,\n",
       "         -1.5043e-01,  3.0338e-02, -1.9515e-01, -1.1159e-01, -2.0493e-01,\n",
       "          2.0340e-01, -6.3082e-02,  1.7558e-01,  2.2384e-03,  1.8260e-02,\n",
       "          1.1779e-01, -1.7339e-01, -5.3052e-02,  1.6983e-01, -2.0256e-01,\n",
       "         -4.2403e-02, -6.7203e-02,  1.0743e-01,  9.9188e-02, -2.1178e-01,\n",
       "          7.0026e-02,  1.7788e-01, -6.0138e-02, -2.5247e-02, -1.8556e-01,\n",
       "         -1.4564e-01,  1.1567e-01, -1.2452e-01, -1.7558e-01,  4.5911e-02,\n",
       "         -1.6375e-01,  1.0458e-01,  1.6758e-01, -2.2795e-01,  1.6676e-01,\n",
       "          1.5807e-01, -1.3745e-01,  1.9239e-01, -1.5655e-01,  1.4573e-01,\n",
       "          7.0828e-02,  1.4641e-01,  1.4439e-01, -1.2336e-02,  7.6402e-02,\n",
       "          1.2567e-01, -5.9636e-02, -6.9238e-02,  1.1668e-01, -2.2096e-01,\n",
       "         -1.4223e-01, -1.0675e-01, -1.7289e-01, -2.2910e-01,  1.0992e-01],\n",
       "        [ 1.5402e-01, -4.2832e-02, -1.0003e-01,  1.5203e-01, -1.1518e-01,\n",
       "         -2.1810e-02,  2.9028e-02,  1.0413e-01, -9.4043e-02,  2.3344e-01,\n",
       "         -7.2751e-02,  1.9873e-01, -6.3108e-02, -1.2562e-01, -1.1717e-01,\n",
       "          4.8679e-02,  6.4402e-02, -8.9854e-02,  9.5936e-02, -1.7552e-01,\n",
       "         -9.6999e-03,  3.9616e-02,  3.0991e-02, -2.1781e-02,  5.0134e-02,\n",
       "         -5.4838e-02, -1.8099e-01,  3.3340e-02,  2.0823e-01, -9.9992e-02,\n",
       "          7.8404e-02,  1.0621e-01, -8.5531e-02, -1.4205e-01, -2.0374e-01,\n",
       "         -6.3365e-02,  6.5189e-02,  2.2525e-01, -9.4199e-03,  1.5863e-01,\n",
       "          1.3284e-01,  2.1940e-01,  2.0940e-01,  6.6292e-02,  6.1394e-02,\n",
       "          2.0055e-01,  2.0937e-02,  1.2125e-01, -1.5037e-01,  1.8445e-01,\n",
       "         -1.1007e-01, -5.0577e-02,  1.4661e-01,  1.2649e-01, -1.5954e-01,\n",
       "          1.1101e-01, -1.3596e-01, -1.3349e-01, -1.2557e-01, -5.7895e-02,\n",
       "         -1.3405e-01, -1.5240e-01, -7.8948e-02, -1.5520e-01, -1.4504e-01,\n",
       "         -1.1314e-01, -2.3973e-02, -2.1043e-01, -2.1648e-01, -1.6451e-01,\n",
       "         -6.9080e-03,  2.2216e-01,  1.6115e-01,  1.2205e-02,  1.3371e-01,\n",
       "         -2.0512e-01, -6.0597e-02,  1.5024e-01,  5.4154e-02,  2.0219e-01,\n",
       "          1.7683e-01,  1.8919e-01,  7.1314e-02, -1.8591e-01,  6.6484e-02,\n",
       "          1.7951e-01, -2.7590e-02, -1.2377e-01,  7.1643e-02, -1.3759e-01,\n",
       "         -1.1530e-01,  1.1832e-02,  2.0789e-01,  1.4014e-02,  8.4542e-02,\n",
       "          1.4767e-01,  1.9901e-03, -1.0847e-01,  1.3928e-01,  2.2569e-01],\n",
       "        [-1.6664e-01,  1.0876e-01,  3.5235e-02, -1.9807e-01, -1.6000e-01,\n",
       "         -1.7943e-01,  7.8687e-02,  2.0780e-01, -1.9939e-01,  5.2365e-02,\n",
       "          1.7657e-01, -8.2068e-02,  1.0109e-01,  1.4145e-01,  1.3544e-01,\n",
       "          2.7745e-02,  1.3810e-01, -8.4497e-02,  2.0470e-01, -2.9895e-02,\n",
       "         -8.9077e-02, -3.7367e-02,  1.5900e-02,  3.0267e-02,  8.2898e-02,\n",
       "          1.5435e-01,  1.6030e-01, -2.1544e-01,  7.0618e-02,  2.6150e-02,\n",
       "          1.9802e-01,  1.1250e-02, -1.3206e-01,  6.9038e-02, -1.0941e-01,\n",
       "         -1.5829e-01, -5.7388e-02, -2.2437e-01,  1.1793e-01,  1.6508e-02,\n",
       "         -1.1726e-01, -2.1488e-01, -4.8022e-02,  2.1456e-01, -1.5137e-01,\n",
       "         -1.5266e-01, -8.2745e-02, -6.2206e-02,  2.1895e-01,  2.1852e-03,\n",
       "         -4.9149e-02, -4.8964e-02, -2.0899e-01, -2.1044e-01,  4.4165e-02,\n",
       "          2.2821e-01, -8.5139e-02,  8.5508e-02, -1.0448e-01,  7.6795e-02,\n",
       "         -1.3367e-01,  8.8286e-03,  4.8872e-02, -1.3641e-01, -2.0535e-01,\n",
       "          1.6631e-01, -1.6770e-02, -1.7104e-01, -1.4051e-01, -4.5230e-02,\n",
       "         -1.7650e-01, -1.8940e-01,  2.0571e-01, -1.5484e-01, -1.3752e-01,\n",
       "          1.8365e-01, -2.3751e-02,  1.0546e-01, -1.1693e-01, -1.5770e-01,\n",
       "          8.4939e-02,  2.2026e-01,  1.0761e-01,  3.9742e-02,  3.4807e-02,\n",
       "         -1.8206e-01, -1.2120e-01, -9.7366e-02, -1.9421e-01, -1.4557e-02,\n",
       "         -9.7128e-02,  7.2454e-02,  3.0615e-02, -9.7502e-02, -1.7933e-01,\n",
       "          1.5150e-01,  1.1993e-01, -9.2556e-03, -4.2943e-02, -8.1500e-02],\n",
       "        [-1.5657e-02,  3.1758e-02,  1.7908e-01,  1.1897e-01,  2.0405e-01,\n",
       "          1.8755e-01, -1.7961e-01, -1.1989e-01,  1.5036e-02, -1.8690e-01,\n",
       "         -1.0723e-01,  8.4043e-02, -1.7240e-01,  1.4730e-01,  1.5688e-01,\n",
       "          2.4806e-02,  2.1985e-01,  1.8890e-01, -1.1607e-01,  2.0246e-01,\n",
       "         -2.6923e-02, -2.5155e-02,  1.4858e-01,  4.9755e-02,  2.0892e-01,\n",
       "         -1.9190e-01, -6.5137e-02, -1.4819e-01,  9.1450e-03, -2.3306e-01,\n",
       "          2.0261e-01, -3.1142e-02, -1.5523e-01,  2.7136e-03,  7.8072e-02,\n",
       "          1.4260e-01,  8.3520e-03,  2.1860e-01,  1.7257e-01, -2.2770e-01,\n",
       "          7.1411e-02,  6.5777e-03,  2.0035e-01,  1.9725e-01, -4.2125e-02,\n",
       "         -1.7448e-01, -6.7747e-02, -1.8901e-01, -1.7057e-01,  5.9916e-03,\n",
       "          1.0950e-01, -6.7863e-02,  2.2887e-01,  1.0280e-01, -7.5140e-02,\n",
       "         -1.5250e-01,  2.0565e-02,  1.8199e-01, -4.1619e-02,  2.3481e-02,\n",
       "          1.9944e-01, -1.4826e-02,  1.7837e-02, -2.5344e-02, -4.4850e-03,\n",
       "          1.5113e-01,  1.1082e-01, -3.2116e-02,  2.2495e-01,  1.5173e-01,\n",
       "         -1.7593e-01,  3.5185e-02,  2.1596e-01,  4.5104e-02, -4.1283e-02,\n",
       "          6.0360e-02, -1.2854e-01, -1.4212e-01, -2.2873e-01,  1.6861e-01,\n",
       "         -1.5138e-01, -1.9870e-01, -1.5735e-01, -6.5473e-02,  1.7342e-01,\n",
       "         -1.6089e-01,  1.1993e-01, -2.0494e-01, -9.0697e-03, -7.8060e-02,\n",
       "          6.4327e-03,  1.5342e-02, -2.4959e-02, -9.5969e-02, -1.3177e-02,\n",
       "         -1.7420e-01, -2.9898e-02, -1.1809e-01, -2.2505e-01, -8.1999e-02],\n",
       "        [-1.5478e-01, -2.3119e-01, -1.7371e-01,  4.4141e-02,  2.6069e-02,\n",
       "         -6.1889e-02,  2.0241e-01, -1.3975e-02,  8.4512e-02, -8.2937e-02,\n",
       "         -1.4665e-01,  2.2667e-01, -2.6047e-02, -2.2835e-01, -1.4588e-01,\n",
       "          1.6430e-01,  4.8545e-02,  1.7470e-01, -1.3964e-01,  1.6176e-01,\n",
       "          2.2126e-01,  2.2486e-01,  2.2007e-01,  1.3210e-01, -2.0408e-01,\n",
       "         -1.8445e-01, -2.9331e-02, -1.6993e-01, -9.1860e-02, -9.7552e-02,\n",
       "         -2.1012e-01, -2.1769e-01, -1.8001e-01, -1.7757e-01,  8.9771e-02,\n",
       "         -2.1221e-01,  4.7477e-02, -2.0140e-01,  1.8119e-01,  1.0897e-01,\n",
       "         -2.0195e-01, -1.9974e-01,  8.2940e-02, -1.2672e-01,  1.1900e-01,\n",
       "         -9.1528e-02, -1.0893e-01,  1.5321e-01, -8.5403e-02,  1.7259e-01,\n",
       "          9.2713e-02,  1.8459e-01, -2.6322e-02,  2.7163e-02, -1.7289e-02,\n",
       "          8.1285e-03,  1.9574e-01, -1.0297e-01, -8.5692e-04,  1.6445e-01,\n",
       "          1.6519e-01, -2.0407e-01,  7.7936e-02,  6.4904e-02,  1.7369e-01,\n",
       "         -1.3903e-01,  2.0049e-01, -5.1449e-02, -8.0355e-02, -1.9494e-01,\n",
       "         -1.1041e-01, -6.6118e-02, -4.0579e-02, -4.3642e-02, -6.0525e-02,\n",
       "         -6.3544e-02, -1.3449e-01, -2.1002e-01, -7.2464e-02, -1.5828e-01,\n",
       "          2.0091e-01,  2.0009e-01,  1.4327e-01,  6.8430e-02,  5.3945e-02,\n",
       "          5.8569e-02,  1.7076e-01, -1.1913e-01,  2.3254e-01,  3.8687e-02,\n",
       "         -7.9263e-03,  1.3883e-02, -4.4898e-02, -1.7708e-01,  1.6322e-01,\n",
       "          1.4461e-01, -2.1618e-01, -1.5399e-01,  5.6812e-02,  2.2818e-01],\n",
       "        [ 4.0504e-02, -1.6345e-02, -3.4039e-03, -8.0426e-02,  1.1360e-02,\n",
       "          2.1512e-01, -1.9457e-01, -7.3537e-03, -7.2844e-02,  7.3923e-02,\n",
       "         -1.9336e-02,  2.1157e-01,  7.4692e-02, -1.2855e-01, -9.3997e-02,\n",
       "          9.3719e-02,  3.9976e-02,  9.6052e-02,  1.4803e-01,  1.8003e-01,\n",
       "          8.9800e-02,  2.6390e-02, -1.0794e-01, -1.2938e-02,  2.2507e-01,\n",
       "          2.6437e-02,  2.1524e-01,  2.7615e-02,  1.8639e-01, -8.1455e-02,\n",
       "         -1.6912e-01, -1.0382e-01,  2.2476e-02,  1.1585e-01, -1.6371e-01,\n",
       "          2.7198e-02,  1.6156e-02,  2.2828e-01,  1.0701e-01, -1.2357e-01,\n",
       "          1.3811e-01,  5.9482e-02, -8.7504e-02, -3.8041e-03,  1.9239e-01,\n",
       "          2.1690e-01,  1.8020e-02,  5.1408e-02,  1.6118e-02,  9.8689e-03,\n",
       "         -1.5485e-01,  1.7933e-01, -1.2609e-01, -2.2338e-01,  5.7787e-02,\n",
       "          4.6314e-02,  8.3723e-02, -2.5844e-02,  1.0659e-01,  2.2934e-01,\n",
       "         -1.4177e-01,  1.6307e-01, -1.3856e-01,  8.3639e-02,  6.2037e-02,\n",
       "         -8.1737e-02,  1.2891e-01,  1.3204e-01, -1.9711e-01, -4.4259e-02,\n",
       "         -1.7405e-01, -2.3289e-01,  1.1842e-02, -4.3460e-02,  1.0329e-01,\n",
       "         -1.1524e-01, -8.6863e-02,  1.3175e-01, -8.5086e-02, -1.2587e-01,\n",
       "         -1.6366e-01, -2.0599e-01, -5.4685e-02,  1.6433e-01, -5.4352e-02,\n",
       "         -3.8432e-02, -8.9960e-02, -2.9988e-02, -3.3876e-02, -4.9784e-02,\n",
       "         -8.4397e-02, -3.3265e-02, -7.1378e-02,  2.3067e-01,  3.0496e-02,\n",
       "         -1.0847e-04,  1.3179e-01, -2.0126e-01, -2.1074e-01, -7.9702e-02],\n",
       "        [-7.3455e-02, -1.2727e-01, -2.1908e-01,  1.7045e-01,  5.4786e-02,\n",
       "         -1.6063e-01,  1.8119e-01,  5.4942e-02,  3.5127e-02,  1.4513e-01,\n",
       "         -1.2509e-01, -1.9197e-01, -3.1316e-02, -2.0068e-01,  2.1659e-01,\n",
       "          2.0423e-01,  1.3308e-01, -2.0370e-01,  6.4458e-02, -1.9063e-01,\n",
       "         -2.2452e-01, -1.2978e-01,  6.2512e-02,  1.0297e-02,  6.7421e-02,\n",
       "         -6.4483e-02, -1.5263e-01,  5.2245e-02, -1.7719e-01, -6.0114e-02,\n",
       "          8.4108e-02,  5.0306e-02, -1.5558e-01, -3.7921e-02,  7.2919e-02,\n",
       "         -1.0348e-01, -1.6230e-01,  2.3142e-01,  9.3778e-02,  2.0501e-02,\n",
       "          2.3129e-01, -1.8715e-01,  1.7106e-01, -8.8770e-02,  3.2367e-02,\n",
       "         -1.8262e-01,  8.8203e-02,  2.1559e-01,  1.4229e-01, -3.4010e-02,\n",
       "         -1.9101e-02,  8.6322e-02,  1.5905e-03, -2.2269e-01, -3.9407e-02,\n",
       "          1.0340e-01, -2.6976e-02, -1.2582e-01, -4.5612e-02,  1.6528e-01,\n",
       "          1.3832e-01, -3.3608e-03, -8.4948e-02, -2.7622e-02,  2.1961e-01,\n",
       "          1.9634e-01, -2.3066e-01, -4.5911e-02, -1.0276e-01,  9.8953e-02,\n",
       "          6.0304e-03,  2.3765e-02,  2.2131e-01,  1.5415e-01, -4.0508e-02,\n",
       "         -9.1040e-02,  1.6213e-01, -1.3370e-01, -1.6474e-01,  9.1416e-02,\n",
       "         -1.2318e-01,  3.2378e-02, -8.2655e-02,  4.0452e-02, -1.6998e-01,\n",
       "         -4.6822e-02, -3.3210e-02,  6.9317e-02, -1.5568e-01, -1.9295e-01,\n",
       "         -1.7780e-01, -4.6269e-02, -1.3961e-01,  2.7257e-02,  5.7016e-02,\n",
       "          1.7113e-01,  1.3159e-01,  1.1507e-01, -5.3745e-02,  1.3199e-01],\n",
       "        [ 1.2697e-01,  7.2409e-02,  1.3103e-01,  1.7405e-01, -1.0354e-01,\n",
       "         -7.3669e-02,  5.4964e-03,  2.2879e-01,  2.1602e-01, -2.2761e-01,\n",
       "          1.9932e-01,  1.1197e-01, -1.9946e-01,  1.8848e-01, -1.0813e-01,\n",
       "          2.0013e-01, -1.9903e-01, -4.5619e-03,  3.4396e-02, -1.0038e-01,\n",
       "          4.6188e-02, -9.5634e-02, -5.2341e-02,  7.2324e-03,  7.5245e-02,\n",
       "         -1.4034e-01, -1.6101e-01, -1.0994e-01, -7.2771e-02,  2.0505e-01,\n",
       "         -4.2157e-02,  1.7551e-01, -2.1938e-02, -1.4864e-01, -1.9741e-01,\n",
       "         -2.0833e-01,  3.7222e-02,  2.2884e-01, -8.4975e-02,  4.0592e-02,\n",
       "          1.5397e-01, -1.3100e-02,  1.8104e-01,  1.7256e-02,  3.0955e-02,\n",
       "          2.2312e-01, -1.7605e-01, -1.4709e-01,  1.9861e-01, -1.1108e-01,\n",
       "         -1.1768e-01,  8.8847e-02,  1.4672e-01, -1.7512e-01, -1.5564e-01,\n",
       "          1.0987e-01, -4.0797e-02, -4.5775e-02,  1.8864e-01,  1.7726e-01,\n",
       "         -1.2056e-01, -1.0268e-01,  2.6176e-02,  1.8827e-01, -8.2540e-02,\n",
       "         -1.4015e-01,  1.2451e-01,  5.4871e-02, -3.6036e-02,  7.9194e-02,\n",
       "          5.8545e-02,  9.6821e-02, -6.6441e-02, -1.7749e-01, -1.8772e-01,\n",
       "         -3.2972e-02, -1.1473e-01, -3.6178e-02,  3.3665e-02, -1.6669e-01,\n",
       "         -2.2648e-01,  1.7623e-01, -8.6313e-02,  7.4316e-02, -1.4128e-01,\n",
       "         -6.0815e-03, -1.5137e-01, -1.9465e-02, -2.2158e-01, -1.7289e-01,\n",
       "         -9.5721e-02,  1.8456e-02, -2.2670e-01, -2.2948e-01,  2.7367e-02,\n",
       "          4.4221e-02, -5.4674e-02,  1.8000e-01, -1.3421e-01, -1.9888e-01],\n",
       "        [-1.7917e-01, -2.1675e-01, -1.9745e-01, -2.2671e-01, -2.0104e-01,\n",
       "          7.2793e-02,  7.8005e-02, -9.2891e-02, -1.3579e-01, -1.7245e-01,\n",
       "         -1.6306e-01, -1.9167e-01, -1.5320e-01,  1.3828e-01,  8.6982e-02,\n",
       "         -7.7065e-02, -1.4577e-01,  7.4701e-02, -1.8000e-01,  5.0459e-02,\n",
       "         -1.0258e-01, -4.0822e-02,  2.2243e-01,  1.4973e-01, -2.0653e-01,\n",
       "          1.4460e-01, -2.4244e-02,  1.9367e-01,  6.8417e-02, -1.4613e-01,\n",
       "         -2.2773e-01,  1.7766e-01, -1.3854e-01, -2.2699e-01, -1.7669e-01,\n",
       "         -1.9591e-01,  1.7950e-01,  7.6645e-02,  1.2300e-01, -1.2528e-01,\n",
       "         -1.9825e-01,  2.2121e-01, -1.7949e-01, -1.5844e-01, -2.2604e-01,\n",
       "         -3.9611e-02, -1.7789e-01,  3.8770e-02,  1.8775e-01,  1.0475e-01,\n",
       "         -3.2726e-02, -1.5664e-01,  6.3181e-02,  5.7098e-03, -1.4543e-01,\n",
       "          5.4148e-03,  9.6420e-02, -9.8212e-02, -1.8717e-01, -2.2673e-01,\n",
       "         -1.8716e-01,  1.4313e-01,  1.1265e-01, -2.2970e-02,  9.7276e-02,\n",
       "         -2.1931e-01, -1.3358e-01, -1.2552e-01, -2.6243e-02,  1.4415e-01,\n",
       "          1.0920e-01,  7.1527e-02, -7.4412e-02, -7.5042e-02, -1.9353e-01,\n",
       "          2.2719e-01, -2.1567e-01, -2.3343e-01,  2.3124e-01,  2.1450e-01,\n",
       "         -2.0301e-01,  1.0443e-01,  1.7246e-01,  2.1838e-01, -2.2840e-01,\n",
       "          1.3785e-01, -1.8108e-01,  8.7227e-03, -1.1858e-03,  1.6794e-01,\n",
       "         -7.4200e-02, -9.8141e-02,  1.2913e-01,  2.1008e-01,  1.4901e-01,\n",
       "          2.1493e-01, -1.9557e-01,  6.9470e-02,  1.3292e-01,  7.7946e-02],\n",
       "        [-8.4179e-03, -1.2418e-01,  1.0667e-02, -2.1898e-01,  2.8078e-02,\n",
       "          1.7942e-01,  2.2365e-01, -1.3029e-02, -1.8270e-01, -1.8130e-02,\n",
       "          1.9116e-01,  7.7962e-02, -1.9026e-01, -5.2450e-02, -1.2777e-01,\n",
       "         -2.2131e-02,  8.4356e-02,  1.4159e-01,  4.6577e-02,  2.2804e-01,\n",
       "         -2.2271e-01, -1.4343e-01,  1.7248e-01,  2.9899e-02,  1.1878e-01,\n",
       "         -1.2077e-01,  2.1992e-01, -4.8827e-02,  5.2373e-02, -2.1962e-01,\n",
       "         -7.8248e-02,  7.6039e-02, -2.0668e-01, -8.5768e-03, -2.2668e-02,\n",
       "         -9.4524e-02, -1.0428e-01, -1.3643e-02, -1.5217e-01,  9.8187e-02,\n",
       "          1.4305e-01, -7.7785e-02,  8.3975e-02,  2.4293e-02, -7.9988e-02,\n",
       "          1.2946e-01, -7.4870e-02, -1.4470e-01, -1.6603e-01, -1.5767e-02,\n",
       "         -1.1935e-01, -2.0730e-02,  5.1158e-02, -5.0228e-02,  2.0828e-01,\n",
       "          1.9839e-01, -2.1444e-01,  9.4304e-02, -1.4798e-01, -1.1592e-01,\n",
       "          1.3435e-01,  9.2715e-02,  1.0669e-01, -2.8124e-02, -1.5809e-01,\n",
       "         -1.8066e-02,  2.5281e-02, -1.5550e-01,  1.1579e-01, -7.3583e-02,\n",
       "          2.1889e-01,  2.1880e-01,  1.6164e-01, -6.3612e-02, -1.1083e-01,\n",
       "         -7.4231e-02, -1.3050e-01, -5.6484e-02, -2.2974e-01, -3.5175e-02,\n",
       "          8.7221e-03, -1.2607e-01, -3.0981e-02,  7.8255e-02,  1.9339e-01,\n",
       "          6.2662e-02,  1.4412e-01,  3.2984e-02, -2.2690e-01,  6.9816e-02,\n",
       "         -1.3359e-01, -2.0040e-02,  7.3006e-02, -3.6160e-02,  5.0122e-02,\n",
       "         -1.7508e-01, -2.1991e-01,  1.7118e-01,  1.6056e-01, -1.9237e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n",
    "# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n",
    "\n",
    "input_size = 1*28*28\n",
    "output_size = 10 # one to ten\n",
    "\n",
    "l1 = nn.Linear(input_size, 100, bias = True)\n",
    "l2 = nn.Linear(100, 100, bias = True)\n",
    "l3 = nn.Linear(100, output_size, bias = True)\n",
    "dropout = nn.Dropout(p = 0.3)\n",
    "relu = nn.ReLU()\n",
    "bn1 = nn.BatchNorm1d(100)\n",
    "bn2 = nn.BatchNorm1d(100)\n",
    "\n",
    "#xavier initialization을 이용하여 각 layer의 weight 초기화\n",
    "\n",
    "nn.init.xavier_uniform_(l1.weight)\n",
    "nn.init.xavier_uniform_(l2.weight)\n",
    "nn.init.xavier_uniform_(l3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Sequential을 이용하여 model 정의하기(쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n",
    "\n",
    "model = nn.Sequential(l1, bn1, relu, dropout,\n",
    "                      l2, bn2, relu, dropout,\n",
    "                      l3).to(device)\n",
    "\n",
    "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.508165479\n",
      "Epoch: 0002 cost = 0.375495285\n",
      "Epoch: 0003 cost = 0.333090812\n",
      "Epoch: 0004 cost = 0.325469553\n",
      "Epoch: 0005 cost = 0.298735440\n",
      "Epoch: 0006 cost = 0.294380337\n",
      "Epoch: 0007 cost = 0.271437466\n",
      "Epoch: 0008 cost = 0.266944289\n",
      "Epoch: 0009 cost = 0.267397463\n",
      "Epoch: 0010 cost = 0.263606280\n",
      "Epoch: 0011 cost = 0.252812713\n",
      "Epoch: 0012 cost = 0.249431238\n",
      "Epoch: 0013 cost = 0.247606575\n",
      "Epoch: 0014 cost = 0.233987093\n",
      "Epoch: 0015 cost = 0.241846159\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "#cost 계산을 위한 변수 설정\n",
    "\n",
    "model.train()\n",
    "train_total_batch = len(train_loader)\n",
    "\n",
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    \n",
    "#train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
    "    for X, Y in train_loader:\n",
    "        # reshape input image and label is not one-hot encoded\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        train_pred = model(X)\n",
    "        bn_loss = criterion(train_pred, Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        bn_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += bn_loss / train_total_batch\n",
    "        \n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9362000226974487\n",
      "Label:  8\n",
      "Prediction:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jominju/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/Users/jominju/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "#accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    model.eval()\n",
    "    bn_loss = 0\n",
    "    bn_acc = 0\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    pred = model(X_test)\n",
    "    correct = torch.argmax(pred, 1) == Y_test\n",
    "    accuracy = correct.float().mean()\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy.item())\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer):\n",
    "    #Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "\n",
    "    #train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
    "        for X, Y in train_loader:\n",
    "            # reshape input image and label is not one-hot encoded\n",
    "            X = X.view(-1, 28 * 28).to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_pred = model(X)\n",
    "            bn_loss = criterion(train_pred, Y)\n",
    "            bn_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_cost += bn_loss / train_total_batch\n",
    "\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "    print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    #test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "    #X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "    #accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        bn_loss = 0\n",
    "        bn_acc = 0\n",
    "\n",
    "        X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "        Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "        pred = model(X_test)\n",
    "        correct = torch.argmax(pred, 1) == Y_test\n",
    "        accuracy = correct.float().mean()\n",
    "\n",
    "        print(\"Accuracy: \", accuracy.item())\n",
    "\n",
    "\n",
    "\n",
    "        ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n",
    "        r = random.randint(0, len(test_loader)-1)\n",
    "        X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "        Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "\n",
    "        print('Label: ', Y_single_data.item())\n",
    "        single_prediction = model(X_single_data)\n",
    "        print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bn1 : 200, bn2 : 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = nn.Linear(input_size, 200, bias = True)\n",
    "l2 = nn.Linear(200, 100, bias = True)\n",
    "l3 = nn.Linear(100, output_size, bias = True)\n",
    "dropout = nn.Dropout(p = 0.3)\n",
    "relu = nn.ReLU()\n",
    "bn1 = nn.BatchNorm1d(200)\n",
    "bn2 = nn.BatchNorm1d(100)\n",
    "\n",
    "#xavier initialization을 이용하여 각 layer의 weight 초기화\n",
    "\n",
    "nn.init.xavier_uniform_(l1.weight)\n",
    "nn.init.xavier_uniform_(l2.weight)\n",
    "nn.init.xavier_uniform_(l3.weight)\n",
    "\n",
    "model = nn.Sequential(l1, bn1, relu, dropout,\n",
    "                      l2, bn2, relu, dropout,\n",
    "                      l3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.457052320\n",
      "Epoch: 0002 cost = 0.347828001\n",
      "Epoch: 0003 cost = 0.308138520\n",
      "Epoch: 0004 cost = 0.280457348\n",
      "Epoch: 0005 cost = 0.261511773\n",
      "Epoch: 0006 cost = 0.245762274\n",
      "Epoch: 0007 cost = 0.236824229\n",
      "Epoch: 0008 cost = 0.236409068\n",
      "Epoch: 0009 cost = 0.233064726\n",
      "Epoch: 0010 cost = 0.221178889\n",
      "Epoch: 0011 cost = 0.216698870\n",
      "Epoch: 0012 cost = 0.212435573\n",
      "Epoch: 0013 cost = 0.214893490\n",
      "Epoch: 0014 cost = 0.196609765\n",
      "Epoch: 0015 cost = 0.195929185\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8988000154495239\n",
      "Label:  6\n",
      "Prediction:  6\n"
     ]
    }
   ],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bn1 : 100, bn2 : 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = nn.Linear(input_size, 100, bias = True)\n",
    "l2 = nn.Linear(100, 50, bias = True)\n",
    "l3 = nn.Linear(50, output_size, bias = True)\n",
    "dropout = nn.Dropout(p = 0.3)\n",
    "relu = nn.ReLU()\n",
    "bn1 = nn.BatchNorm1d(100)\n",
    "bn2 = nn.BatchNorm1d(50)\n",
    "\n",
    "#xavier initialization을 이용하여 각 layer의 weight 초기화\n",
    "\n",
    "nn.init.xavier_uniform_(l1.weight)\n",
    "nn.init.xavier_uniform_(l2.weight)\n",
    "nn.init.xavier_uniform_(l3.weight)\n",
    "\n",
    "model = nn.Sequential(l1, bn1, relu, dropout,\n",
    "                      l2, bn2, relu, dropout,\n",
    "                      l3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.495852977\n",
      "Epoch: 0002 cost = 0.357113242\n",
      "Epoch: 0003 cost = 0.326630712\n",
      "Epoch: 0004 cost = 0.311622858\n",
      "Epoch: 0005 cost = 0.277846158\n",
      "Epoch: 0006 cost = 0.286495715\n",
      "Epoch: 0007 cost = 0.275808871\n",
      "Epoch: 0008 cost = 0.261058152\n",
      "Epoch: 0009 cost = 0.256663710\n",
      "Epoch: 0010 cost = 0.249630675\n",
      "Epoch: 0011 cost = 0.250210762\n",
      "Epoch: 0012 cost = 0.239698932\n",
      "Epoch: 0013 cost = 0.248992592\n",
      "Epoch: 0014 cost = 0.236774951\n",
      "Epoch: 0015 cost = 0.240820155\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9537000060081482\n",
      "Label:  3\n",
      "Prediction:  3\n"
     ]
    }
   ],
   "source": [
    "test(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
