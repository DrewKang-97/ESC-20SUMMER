{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2_신예진"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 아래에 주어진 주석을 기반으로 하여 코딩을 해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65f1b207d984d3c85475860d784e5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8986818b301404e9d9386fe72f96801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70f39286ab141c882b7a8cbd988514f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ae1382f3dd4d549f04baacf73f09bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rstudio\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=True,\n",
    "                         transform = transforms.ToTensor(),\n",
    "                         download =True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                        train=False,\n",
    "                        transform = transforms.ToTensor(),\n",
    "                        download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train, \n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test, \n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = torch.nn.Linear(784,100, bias=True)\n",
    "linear2 = torch.nn.Linear(100,100, bias=True)\n",
    "linear3 = torch.nn.Linear(100,10, bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "bn1 = torch.nn.BatchNorm1d(100) #number of size\n",
    "bn2 = torch.nn. BatchNorm1d(100)\n",
    "\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rstudio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Rstudio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n",
      "C:\\Users\\Rstudio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.4327e-01, -1.0198e-01, -2.1631e-01, -5.3945e-02, -1.1043e-01,\n",
       "          1.7424e-01,  1.3707e-01, -6.6992e-02, -4.6227e-02, -1.2974e-02,\n",
       "         -3.6126e-02,  1.2323e-01,  2.1793e-01, -1.5587e-01,  2.0820e-01,\n",
       "          1.6229e-01, -5.0635e-02,  1.4373e-01, -1.5804e-02, -1.6831e-01,\n",
       "          1.5265e-01, -7.3248e-02,  1.5824e-01, -3.2106e-02,  8.2778e-02,\n",
       "          6.3402e-02,  2.1614e-02,  6.9557e-02, -2.2104e-01, -1.3861e-01,\n",
       "         -1.0080e-01, -7.8901e-02,  8.5597e-02, -9.7518e-02,  6.8865e-03,\n",
       "         -1.4864e-01, -4.8956e-03,  1.4469e-01,  1.7819e-01, -5.2978e-02,\n",
       "         -1.0098e-01, -8.2930e-02,  2.0266e-01, -1.8001e-01, -1.2747e-01,\n",
       "         -8.1690e-02,  5.6006e-02, -2.2113e-02, -1.8489e-01,  9.5834e-02,\n",
       "         -1.9355e-01,  2.0232e-01, -2.1460e-01, -1.0026e-01, -1.8142e-01,\n",
       "         -1.8594e-01, -9.2068e-02,  1.6128e-01, -5.9895e-02,  7.6397e-02,\n",
       "         -1.2061e-01, -1.2702e-01, -1.7871e-01, -4.3743e-03,  2.0347e-01,\n",
       "          2.1790e-01,  1.8047e-01,  1.6673e-01, -8.2581e-02, -3.0645e-02,\n",
       "          2.7544e-02, -1.9635e-01, -4.5950e-02, -1.2328e-01,  6.7585e-02,\n",
       "          1.4611e-01,  1.5962e-01, -1.1363e-01,  2.2413e-01, -1.4932e-01,\n",
       "         -1.8543e-01,  4.9979e-02,  9.9568e-02,  4.3366e-02,  1.1680e-01,\n",
       "          2.0361e-01,  2.5829e-02, -3.7866e-02,  1.7305e-01,  2.1784e-01,\n",
       "         -1.7189e-02,  4.7389e-02,  1.2970e-01, -1.0828e-01,  1.7199e-01,\n",
       "          9.7664e-02,  7.0276e-02, -1.2719e-01,  8.8311e-02,  1.1901e-01],\n",
       "        [ 6.3875e-02,  9.1521e-02, -3.8189e-02, -1.2489e-01, -2.0135e-01,\n",
       "         -3.8832e-03, -1.8879e-01, -9.6005e-04, -2.0873e-01,  9.3614e-02,\n",
       "         -3.7452e-02,  1.6418e-01,  9.7439e-05, -1.4752e-01, -1.2307e-01,\n",
       "          5.4524e-03, -3.4307e-02,  4.7650e-02,  3.8889e-02, -1.8020e-01,\n",
       "         -1.2798e-01,  1.3206e-02, -1.2375e-01,  8.3424e-02,  1.9779e-01,\n",
       "         -3.3755e-02, -1.6827e-01,  1.2771e-01, -1.8026e-01,  6.5306e-02,\n",
       "         -1.3827e-01, -1.7932e-01,  5.6331e-02,  7.9879e-02,  5.6205e-02,\n",
       "          4.7215e-03, -1.4209e-01,  1.8771e-02,  1.6974e-02,  2.6125e-02,\n",
       "          1.1374e-01, -1.6213e-01,  1.1024e-01,  4.3487e-02, -1.3490e-01,\n",
       "          1.8485e-02,  1.7721e-01, -3.2130e-02, -1.2539e-01,  9.0201e-02,\n",
       "         -2.0452e-01, -6.5944e-02,  1.1187e-01, -1.5282e-01, -1.3555e-01,\n",
       "          1.6488e-01,  6.1860e-02,  4.3944e-02,  1.4539e-01,  1.9592e-01,\n",
       "          1.4605e-01,  1.8960e-01, -1.0144e-02,  2.2806e-01, -1.6170e-01,\n",
       "          1.2059e-01, -2.3341e-01,  1.9511e-01, -4.5123e-02, -1.4724e-01,\n",
       "          1.6230e-01,  1.9573e-01, -1.3838e-01,  4.1357e-02, -8.8998e-02,\n",
       "          1.5054e-01,  1.4020e-01,  6.5039e-02,  9.4285e-02,  5.9601e-02,\n",
       "          6.9984e-02,  5.0059e-02, -3.5498e-02,  1.4541e-01,  1.0637e-01,\n",
       "         -2.0974e-01, -2.3146e-01, -1.3159e-01,  3.3328e-02,  1.3626e-01,\n",
       "         -1.8700e-01, -9.9971e-02,  2.1821e-01, -1.4216e-01,  9.6416e-02,\n",
       "         -8.9669e-03,  1.7216e-01,  7.2905e-02, -2.1571e-01, -2.1543e-01],\n",
       "        [-1.0733e-01, -2.3207e-01, -5.8339e-03,  1.1864e-01,  2.3439e-02,\n",
       "         -3.0045e-03, -2.2690e-02, -1.8510e-01, -2.6200e-02,  1.7971e-01,\n",
       "         -4.2210e-02, -1.0398e-01, -1.1957e-01, -6.0429e-02,  7.6720e-02,\n",
       "          4.2688e-02,  2.2546e-01, -7.9603e-02,  1.4184e-01,  7.6033e-02,\n",
       "         -5.4227e-03,  1.4623e-01, -1.0197e-01,  2.2087e-03, -1.8710e-02,\n",
       "         -1.3905e-01, -1.8199e-01,  1.0437e-01,  8.3311e-02, -1.0232e-01,\n",
       "         -1.4768e-01,  4.4712e-02,  9.5203e-02,  1.3265e-01, -1.4859e-01,\n",
       "          1.5086e-01, -1.3988e-01, -1.4976e-01, -1.9483e-01, -1.1302e-01,\n",
       "          9.4350e-02, -1.6014e-01,  2.1222e-01, -1.2111e-01, -1.3174e-01,\n",
       "         -1.8490e-01, -1.6618e-01, -7.2506e-02, -4.1234e-02,  1.9289e-01,\n",
       "          3.0273e-02, -1.5494e-01,  2.1163e-01, -1.2227e-02, -4.3404e-02,\n",
       "         -1.7692e-01, -1.1343e-01, -1.0872e-01, -2.4875e-02,  1.0875e-02,\n",
       "         -1.2956e-01,  4.9913e-02,  3.0933e-02,  2.0603e-01,  3.2964e-02,\n",
       "          1.0599e-01,  8.7125e-03,  5.4771e-02, -1.6091e-01,  1.2990e-01,\n",
       "          4.8339e-02, -2.0777e-01,  9.4133e-02, -1.4386e-01,  1.4516e-01,\n",
       "         -2.5255e-02, -2.0288e-01, -1.6740e-01,  2.1787e-01,  1.0492e-01,\n",
       "          6.6547e-02, -6.3260e-02, -1.7987e-01,  1.8391e-02,  1.5995e-01,\n",
       "         -1.4830e-01, -3.9753e-02,  1.3609e-01, -8.6166e-02,  8.8721e-02,\n",
       "         -3.9650e-02,  1.9545e-01,  7.0330e-02, -9.4341e-02, -1.4575e-01,\n",
       "         -2.3144e-01, -1.9864e-01,  5.1524e-02,  8.5455e-03,  4.8295e-02],\n",
       "        [-3.6486e-02, -6.3347e-02,  1.9534e-01, -5.0890e-02, -3.7511e-02,\n",
       "          1.3867e-02, -5.8763e-02,  1.8848e-01,  2.1543e-01,  1.6784e-01,\n",
       "         -2.0473e-01, -9.1361e-02, -1.6952e-01,  1.1982e-01, -1.3425e-01,\n",
       "         -1.2168e-01,  2.2446e-01,  2.0886e-01, -1.7867e-01, -2.1364e-01,\n",
       "         -2.3332e-01,  8.0060e-02, -1.5309e-01,  1.3161e-01, -7.1594e-02,\n",
       "          1.2989e-01,  1.0556e-01,  1.6386e-01, -2.2612e-01, -5.4414e-02,\n",
       "          1.9395e-01, -1.3112e-02, -9.1650e-02,  1.1408e-01, -9.3902e-02,\n",
       "          5.6547e-02,  2.0700e-01,  7.1182e-02, -2.1539e-01, -5.8794e-02,\n",
       "          1.8937e-01, -3.4332e-02, -1.7893e-02, -2.0155e-01,  3.1755e-02,\n",
       "          5.3664e-02, -1.0199e-01, -1.1729e-01,  1.5057e-01, -1.2974e-01,\n",
       "         -1.3690e-01, -2.1896e-01,  2.1618e-02,  1.2101e-01,  1.4531e-01,\n",
       "          7.5543e-02,  1.7175e-01,  8.3543e-03, -1.5436e-01,  1.2463e-02,\n",
       "         -1.3579e-01, -2.0887e-01, -1.5514e-01,  1.7339e-01, -7.4865e-02,\n",
       "          4.3855e-02, -1.7923e-01, -2.0374e-01,  4.3837e-02,  1.6349e-01,\n",
       "          1.9479e-01,  9.6357e-02, -1.9601e-01,  1.3504e-01,  2.3071e-01,\n",
       "         -9.9366e-02,  1.5130e-01,  2.1159e-01, -1.6550e-01,  1.5582e-01,\n",
       "          1.9058e-01,  6.5569e-02,  1.9132e-01, -2.3109e-03, -1.8614e-01,\n",
       "          1.3554e-01, -1.3346e-01,  1.5001e-02, -1.4485e-01,  2.4253e-02,\n",
       "          2.2211e-01, -2.2196e-01,  4.3224e-02,  5.3865e-02,  1.9772e-01,\n",
       "          4.8003e-03,  2.0456e-01,  1.7835e-01,  1.7996e-01, -1.3067e-01],\n",
       "        [-8.0101e-02,  2.2031e-01,  6.5219e-02, -1.9590e-01, -4.8007e-02,\n",
       "          1.5211e-01,  1.0311e-01,  1.7734e-01,  1.2011e-01,  1.1660e-01,\n",
       "          1.4601e-01, -3.6464e-02,  1.8934e-01, -6.7839e-02, -5.1137e-02,\n",
       "         -7.4737e-03, -1.5659e-01, -6.5504e-02,  2.1917e-01, -1.6115e-01,\n",
       "         -1.1037e-01,  1.1308e-01,  1.9656e-01, -7.6481e-02,  1.8512e-01,\n",
       "          7.4299e-02, -1.4356e-01,  2.1111e-01,  3.3185e-02, -2.0639e-01,\n",
       "         -1.1768e-01,  2.1052e-01, -5.7605e-02, -8.5181e-02,  2.1960e-01,\n",
       "          7.8338e-02, -7.9294e-02, -2.0884e-01,  2.8611e-02,  6.7058e-02,\n",
       "          2.2096e-02,  2.9304e-02,  8.2801e-02,  1.8560e-02,  1.5631e-02,\n",
       "          1.0878e-01,  1.6353e-01,  1.9128e-01, -8.2206e-03, -1.3584e-01,\n",
       "         -1.2796e-01, -3.6545e-02,  1.6014e-01,  2.7444e-02,  1.8386e-01,\n",
       "          2.3162e-01,  2.3222e-01,  1.1120e-01, -4.0993e-02, -1.5019e-02,\n",
       "          1.9588e-01, -7.4788e-02, -3.9228e-02, -4.3929e-04,  2.0631e-01,\n",
       "         -8.8101e-02, -1.8590e-01, -4.7975e-02,  9.3753e-02, -2.0852e-01,\n",
       "         -1.2287e-01,  1.9675e-01, -3.2704e-02, -1.0473e-01, -4.7366e-02,\n",
       "          8.1316e-02, -3.0313e-02, -3.8413e-03, -5.5841e-02,  1.8614e-01,\n",
       "         -1.4927e-02, -1.6821e-01, -1.4797e-01, -9.1795e-02,  1.2833e-01,\n",
       "         -7.6641e-02,  1.5020e-01,  2.0772e-01,  2.1564e-01,  4.2302e-02,\n",
       "          1.0399e-03, -1.3627e-02, -5.1717e-02, -1.9322e-01,  7.6629e-02,\n",
       "         -2.4375e-03, -2.2483e-01,  1.9291e-01,  1.3940e-01,  1.8382e-01],\n",
       "        [ 1.1140e-01, -1.1647e-01, -2.2747e-01,  1.6870e-01, -4.1775e-02,\n",
       "         -2.2906e-01, -7.2400e-02,  7.4177e-03, -1.8234e-01, -3.7252e-02,\n",
       "          2.0208e-01,  2.0695e-01, -1.8608e-01, -1.4135e-01,  2.1041e-01,\n",
       "         -1.9305e-01,  4.3984e-02, -5.6359e-02, -2.3202e-01, -1.6885e-01,\n",
       "         -1.4361e-01,  6.6258e-02,  8.0554e-02, -1.3400e-01, -5.2665e-02,\n",
       "         -1.9329e-01, -1.7923e-01,  1.0496e-01, -4.1241e-02, -7.6358e-03,\n",
       "          5.6666e-02,  3.4875e-02, -1.1965e-01, -1.5701e-01,  1.0280e-01,\n",
       "          2.1085e-01, -1.8741e-01, -2.2929e-01, -6.8229e-02,  7.0333e-02,\n",
       "          1.1489e-01, -1.0509e-01,  4.4868e-02, -1.8796e-01,  4.5566e-02,\n",
       "         -1.6786e-01,  1.0725e-01,  6.4054e-02, -1.2705e-01,  2.9175e-02,\n",
       "         -9.4762e-02,  1.1009e-01,  1.2550e-01, -1.2082e-01,  2.1473e-01,\n",
       "          7.6147e-02, -6.8772e-02, -1.0314e-02,  6.8797e-02,  9.0317e-02,\n",
       "         -7.5854e-02, -1.2598e-01, -2.1552e-01,  4.0491e-02,  4.3032e-03,\n",
       "         -1.4951e-01,  1.1954e-01, -1.5358e-01, -1.9942e-01, -1.3103e-01,\n",
       "          3.0970e-02,  1.1524e-01, -1.8042e-01, -2.7178e-02, -4.0849e-02,\n",
       "          1.4020e-02, -2.2400e-01, -2.2817e-01,  2.7396e-02,  5.8320e-02,\n",
       "          2.1854e-01,  1.6412e-01, -1.8135e-01, -2.0471e-01,  1.0235e-01,\n",
       "         -1.1259e-01, -1.1332e-01, -1.8015e-02,  9.8793e-02,  1.0388e-01,\n",
       "         -2.1326e-01,  8.8010e-02,  3.4986e-02, -5.4438e-03,  9.7537e-02,\n",
       "          1.2493e-01,  9.7568e-02, -5.4416e-02,  7.9712e-02,  1.9491e-01],\n",
       "        [ 2.0693e-01,  8.9600e-02,  1.8754e-01, -2.3138e-01,  1.3854e-01,\n",
       "         -4.3400e-02,  9.8895e-02,  1.5916e-01, -4.8112e-02,  5.2511e-02,\n",
       "          6.0468e-02, -1.7021e-01, -7.5747e-02,  1.4801e-01,  1.1681e-01,\n",
       "         -1.9069e-02,  5.1997e-02, -3.1639e-02,  1.0335e-01,  1.4043e-01,\n",
       "         -1.3823e-01,  1.4205e-01,  5.3367e-02,  2.2308e-02,  1.8749e-01,\n",
       "          8.0776e-02,  7.7863e-02, -7.6894e-02,  1.3164e-01,  1.0566e-01,\n",
       "          1.8541e-01, -2.0969e-01, -1.5483e-01, -2.0617e-01,  7.3145e-02,\n",
       "         -7.2029e-02, -1.0239e-01,  1.0234e-01,  7.3398e-02, -8.0371e-02,\n",
       "         -1.4597e-01,  2.0479e-01, -7.6177e-02,  7.8930e-02, -1.6621e-01,\n",
       "         -1.6475e-01,  2.1038e-01,  4.0486e-02,  1.0844e-03,  1.1961e-02,\n",
       "         -1.6051e-01, -1.6256e-01,  1.7698e-01, -2.1566e-01, -1.5798e-01,\n",
       "         -1.0091e-01, -1.1734e-01,  4.3970e-02, -2.2394e-01,  5.1607e-02,\n",
       "          6.7500e-02, -1.6551e-01,  5.1164e-02,  5.0238e-02, -1.6801e-01,\n",
       "          9.7512e-03, -6.2276e-02, -7.7749e-02,  1.2980e-01,  8.5985e-02,\n",
       "          1.2233e-01,  8.8462e-03, -1.9743e-01, -3.8180e-02,  1.1455e-01,\n",
       "          1.0199e-01, -5.3498e-02, -1.7116e-01,  8.0976e-02, -5.3573e-02,\n",
       "          7.9160e-02, -1.2769e-01,  2.1448e-01,  2.2190e-01, -2.3287e-01,\n",
       "          2.1567e-01,  9.7390e-02,  6.4240e-02,  1.4598e-01,  9.4690e-02,\n",
       "          2.0348e-01, -8.9534e-02, -1.3513e-01, -2.1388e-01,  1.5118e-01,\n",
       "          1.6570e-02, -6.9330e-02, -6.0626e-02, -1.3634e-01, -1.6205e-01],\n",
       "        [ 1.9751e-01, -2.1686e-01, -1.0039e-01,  7.0704e-02,  1.5222e-01,\n",
       "         -1.8690e-01,  1.1042e-01,  2.2181e-03, -1.5370e-01,  1.9538e-01,\n",
       "          1.3664e-01,  1.7995e-01, -2.8491e-02,  1.2837e-02,  2.2502e-01,\n",
       "         -8.8852e-02,  1.6160e-01,  7.5040e-02, -1.7738e-01, -1.2251e-01,\n",
       "          2.0400e-01, -1.9944e-02, -6.6233e-02, -4.5481e-02, -1.0945e-01,\n",
       "         -2.2175e-01, -5.2185e-04, -1.1523e-02,  1.9327e-01, -2.2250e-01,\n",
       "         -1.5989e-01, -3.1539e-03,  1.8918e-01, -1.6953e-01, -1.3844e-01,\n",
       "         -7.7888e-02,  2.2418e-01, -1.6729e-01,  1.5899e-01,  8.1008e-02,\n",
       "          6.2649e-02, -1.8280e-01,  1.1938e-02, -1.2967e-01,  2.1759e-02,\n",
       "          2.1980e-01,  3.2358e-02, -8.5437e-02,  1.2236e-02, -9.9403e-02,\n",
       "         -1.9439e-01, -5.3983e-02, -2.1764e-01, -1.7885e-01, -1.5539e-01,\n",
       "          3.9519e-02,  5.4895e-02, -4.3966e-02,  1.0532e-01, -9.6545e-02,\n",
       "         -1.9139e-01, -9.3334e-02, -3.9855e-02, -2.0277e-01, -2.0980e-01,\n",
       "          1.2680e-01,  5.5981e-02,  1.6097e-01, -2.2589e-02, -1.9027e-01,\n",
       "          2.4514e-02, -1.6908e-01,  5.5851e-02,  6.9613e-03, -8.4507e-02,\n",
       "          2.1515e-01, -3.4994e-03,  2.0326e-01, -1.5752e-01, -1.4337e-03,\n",
       "          1.7719e-01,  3.8979e-02, -2.3251e-01,  1.6029e-01,  1.0219e-01,\n",
       "          7.5011e-02,  3.8360e-03,  1.8792e-01,  2.1328e-01, -4.8054e-02,\n",
       "         -1.7757e-01, -1.8153e-01,  1.1295e-02, -7.2417e-02,  1.4691e-01,\n",
       "          2.1646e-01, -1.2058e-01, -1.7720e-01,  1.1821e-01, -1.6515e-01],\n",
       "        [ 7.8760e-02, -6.0823e-02, -3.2571e-02, -3.9914e-02, -2.2589e-01,\n",
       "          1.8157e-01, -3.5621e-02, -7.0263e-02, -1.2050e-01, -9.9083e-02,\n",
       "         -2.1110e-01, -8.3645e-02,  1.9693e-01, -1.3305e-01,  2.0400e-01,\n",
       "          6.5156e-02, -2.0997e-01, -2.3094e-01, -1.3485e-01,  8.9792e-02,\n",
       "         -6.0991e-02,  1.9151e-01, -2.1840e-01,  2.1296e-01, -1.1416e-01,\n",
       "         -5.7960e-02,  1.3942e-01, -8.4406e-04,  1.1983e-01,  1.4207e-01,\n",
       "         -1.9328e-01,  3.4583e-02, -1.6667e-01,  2.6096e-02, -7.5578e-03,\n",
       "         -2.0437e-01,  1.1504e-02, -1.1290e-01,  1.0683e-01,  2.1959e-01,\n",
       "         -2.2836e-01, -1.9731e-02,  1.9716e-02,  1.3742e-01, -6.5613e-02,\n",
       "         -2.2723e-01, -1.0973e-01,  1.8688e-01,  7.9781e-02,  1.0130e-01,\n",
       "          8.3009e-02, -1.8885e-01,  6.3282e-02, -4.9489e-02,  1.1583e-01,\n",
       "         -1.9579e-01,  1.9539e-01, -8.8840e-02, -1.7993e-01, -2.1625e-01,\n",
       "         -1.0503e-01, -1.0517e-02,  1.9077e-01,  6.5185e-02, -1.9257e-01,\n",
       "         -3.4615e-02,  1.5955e-01,  9.2004e-02,  7.6653e-02, -1.6472e-01,\n",
       "          2.9361e-02, -9.0858e-02, -1.0913e-01, -2.2103e-01,  1.1250e-01,\n",
       "          1.3564e-01, -6.4514e-02, -5.0512e-02, -1.5929e-02, -1.6047e-01,\n",
       "         -1.5544e-02,  7.8478e-02,  2.0863e-01,  7.1828e-02,  2.2892e-02,\n",
       "         -9.5936e-02, -1.2528e-01,  2.0502e-01,  7.6519e-02,  1.4262e-03,\n",
       "          6.8041e-02,  6.3838e-02, -1.8969e-01, -2.9137e-02,  1.3072e-01,\n",
       "          1.0974e-01,  3.1094e-02,  1.7600e-01, -4.7493e-02,  1.8313e-01],\n",
       "        [ 2.0721e-01, -1.6318e-01,  1.0936e-02,  1.3547e-01, -2.6633e-02,\n",
       "          1.6561e-01, -1.1964e-01, -5.0616e-02,  1.4400e-01, -1.6306e-01,\n",
       "          6.2054e-02,  2.4430e-02,  3.6728e-03, -1.0955e-01,  1.0812e-01,\n",
       "         -1.6498e-01,  3.2091e-02,  5.3034e-02, -2.2437e-03, -9.3198e-02,\n",
       "         -1.8786e-01,  2.0220e-01, -7.2528e-02, -2.3251e-01,  1.0179e-01,\n",
       "          7.3466e-03, -1.9881e-01,  1.5441e-01, -6.7326e-02, -9.9343e-02,\n",
       "          1.9076e-01, -1.8192e-01, -7.6254e-03,  1.6544e-01,  1.6641e-01,\n",
       "          2.1690e-02, -7.7661e-02,  1.4868e-01, -1.0161e-01,  1.5779e-01,\n",
       "          2.3011e-01,  1.1449e-01, -2.1693e-01,  4.0222e-02, -2.1724e-01,\n",
       "         -8.6812e-02, -1.3794e-01, -2.1146e-01,  1.1560e-01,  1.5895e-01,\n",
       "          8.5590e-02, -1.3908e-01,  1.2524e-01,  2.3022e-01,  1.3520e-01,\n",
       "          1.0219e-01,  1.8428e-02,  1.0039e-01, -3.4580e-02,  2.1737e-01,\n",
       "          1.5205e-01,  1.7442e-02, -4.9292e-02, -3.7258e-02, -7.6824e-02,\n",
       "         -2.1012e-01, -5.8437e-02, -6.5174e-02,  6.5033e-02, -1.7241e-02,\n",
       "         -1.9163e-01, -1.4096e-01,  1.6284e-01, -7.1141e-02, -8.4768e-02,\n",
       "          5.4346e-02,  1.1490e-01,  1.3958e-02, -1.1230e-02,  1.7817e-01,\n",
       "          1.0401e-01, -5.4579e-02,  1.1736e-01, -2.8639e-02, -1.6789e-01,\n",
       "         -1.0748e-01, -2.2641e-01, -1.6169e-01,  7.1867e-02, -2.1697e-01,\n",
       "         -1.0648e-01, -1.7434e-01, -4.3050e-02,  1.0827e-01, -4.8416e-02,\n",
       "          1.8264e-01, -6.2178e-02, -5.8011e-02, -1.3212e-01,  1.4556e-03]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform(linear1.weight)\n",
    "torch.nn.init.xavier_uniform(linear2.weight)\n",
    "torch.nn.init.xavier_uniform(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(linear1,  bn1, relu, dropout, \n",
    "                           linear2, bn2, relu,  dropout,\n",
    "                            linear3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.512815416\n",
      "Epoch: 0002 cost= 0.381618172\n",
      "Epoch: 0003 cost= 0.332564741\n",
      "Epoch: 0004 cost= 0.304156423\n",
      "Epoch: 0005 cost= 0.296574652\n",
      "Epoch: 0006 cost= 0.289087325\n",
      "Epoch: 0007 cost= 0.280850381\n",
      "Epoch: 0008 cost= 0.281081885\n",
      "Epoch: 0009 cost= 0.267590672\n",
      "Epoch: 0010 cost= 0.267209679\n",
      "Epoch: 0011 cost= 0.265150785\n",
      "Epoch: 0012 cost= 0.257766902\n",
      "Epoch: 0013 cost= 0.249146625\n",
      "Epoch: 0014 cost= 0.242462710\n",
      "Epoch: 0015 cost= 0.238580048\n",
      "Learning finished.\n"
     ]
    }
   ],
   "source": [
    "train_total_batch  = len(train_loader)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for X,Y in train_loader:\n",
    "        X= X.view(-1,28*28).to(device)\n",
    "        Y= Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost +=cost/ train_total_batch\n",
    "    \n",
    "    print('Epoch:','%04d' % (epoch+1),'cost=', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rstudio\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\Users\\Rstudio\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9254999756813049\n",
      "Label:  4\n",
      "Prediction:  4\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1,28*28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = model(X_test)\n",
    "    \n",
    "    correct_prediction = torch.argmax(prediction,1)==Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "    \n",
    "    r = random.randint(0,len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r+1].view(-1,28*28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r+1].to(device)\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Layer의 hidden node 수를 증가 혹은 감소시켰을 때, train set에서의 cost와 test set에서의 Accuracy가 기존 결과와 비교하였을 때 어떻게 달라졌는지 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 784 -> 200 -> 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1568,  0.0559, -0.1477, -0.1963, -0.1895,  0.2295,  0.1548,  0.1275,\n",
       "         -0.0152, -0.1530,  0.0960,  0.1581,  0.1658,  0.0917,  0.1718, -0.0548,\n",
       "          0.0048, -0.1054,  0.0206,  0.0599, -0.1970, -0.1890,  0.0432,  0.2218,\n",
       "          0.0488,  0.1208, -0.1950, -0.0136,  0.1183, -0.0832,  0.2099, -0.0098,\n",
       "         -0.2243,  0.1976, -0.0441, -0.0775,  0.1811, -0.1543, -0.1315,  0.2062,\n",
       "         -0.1905,  0.0354, -0.1678, -0.1398, -0.1353, -0.1014,  0.0170,  0.1560,\n",
       "          0.1909,  0.2211, -0.1806, -0.1392, -0.1733, -0.0152, -0.0197, -0.0351,\n",
       "          0.1811, -0.0211,  0.1278,  0.1197,  0.0897, -0.0974, -0.0454, -0.1297,\n",
       "         -0.1507, -0.0453, -0.0786,  0.1355, -0.0597, -0.2161, -0.2025,  0.0186,\n",
       "         -0.1721, -0.0139,  0.2284,  0.1528, -0.0136, -0.1459,  0.1721,  0.0652,\n",
       "         -0.2142, -0.2177,  0.1755, -0.1122, -0.0700,  0.0719, -0.0529, -0.1182,\n",
       "         -0.1994,  0.1758, -0.0755, -0.0522, -0.1932, -0.0667, -0.1134, -0.2245,\n",
       "          0.0301, -0.2278, -0.1604,  0.1131],\n",
       "        [-0.2212, -0.0589, -0.1820, -0.0521, -0.2015, -0.0192, -0.2088,  0.1639,\n",
       "          0.1358, -0.1138, -0.1417,  0.0229,  0.2223,  0.0987,  0.0743,  0.2280,\n",
       "         -0.1207, -0.1833,  0.1802,  0.1874,  0.0781,  0.1554,  0.0998,  0.0568,\n",
       "          0.1945, -0.1862, -0.0768, -0.0146,  0.1296, -0.1433, -0.0425, -0.1705,\n",
       "          0.1176, -0.2216, -0.0547, -0.1229,  0.2280,  0.0359, -0.0868, -0.2333,\n",
       "         -0.0277,  0.1760, -0.0224,  0.0886, -0.0578, -0.0647,  0.0617,  0.2042,\n",
       "          0.0548, -0.1879,  0.1082, -0.1061, -0.0829,  0.0169,  0.1995, -0.1129,\n",
       "         -0.0220,  0.0585, -0.0010, -0.1749,  0.1866,  0.0971,  0.0568, -0.0309,\n",
       "          0.0038,  0.2170,  0.1674, -0.0476, -0.1845,  0.0165,  0.2215, -0.2297,\n",
       "          0.0050, -0.0215,  0.1763,  0.1075,  0.0968,  0.1105, -0.1684, -0.1110,\n",
       "         -0.1152,  0.0346,  0.0550, -0.1549, -0.0105, -0.0922,  0.1081, -0.0295,\n",
       "         -0.0871, -0.1292, -0.0624, -0.1139, -0.1224,  0.1982, -0.0630,  0.0472,\n",
       "         -0.2184,  0.2002, -0.1935,  0.2020],\n",
       "        [ 0.2095,  0.2098, -0.0267,  0.1251, -0.1216,  0.0305,  0.0255, -0.0665,\n",
       "         -0.2185,  0.1659,  0.0715, -0.1381, -0.0090,  0.0791,  0.0081, -0.0029,\n",
       "         -0.2041,  0.1750, -0.1923,  0.1991, -0.2030,  0.2322, -0.0369,  0.1666,\n",
       "          0.2191,  0.1038, -0.1323, -0.0298, -0.1300, -0.0253,  0.1141,  0.1774,\n",
       "         -0.0616,  0.0684,  0.0245,  0.0305, -0.1888,  0.2037, -0.0448, -0.2220,\n",
       "          0.1802, -0.1757,  0.0752,  0.1868, -0.0577,  0.1597,  0.1581,  0.0488,\n",
       "         -0.1662, -0.1928,  0.0704,  0.0747,  0.0682, -0.0183, -0.1714, -0.1548,\n",
       "         -0.0651, -0.0026,  0.1809,  0.0190,  0.1893, -0.1363,  0.2268,  0.0250,\n",
       "          0.0993,  0.1060, -0.1127, -0.2300,  0.0152, -0.2018, -0.1909, -0.0119,\n",
       "         -0.0477, -0.1200, -0.1750,  0.0854,  0.0152,  0.2191,  0.0245,  0.1739,\n",
       "          0.2232,  0.0221,  0.0475, -0.1477,  0.1829, -0.1372,  0.1596,  0.0569,\n",
       "          0.1626,  0.1502,  0.1272,  0.2074, -0.1966,  0.0122,  0.1751, -0.0089,\n",
       "         -0.1138, -0.1419, -0.0450, -0.0040],\n",
       "        [-0.0222,  0.0362, -0.1659,  0.1189, -0.1923,  0.1103, -0.2322,  0.0670,\n",
       "         -0.0871,  0.0696,  0.1912, -0.1139, -0.1669, -0.1246, -0.1439, -0.0749,\n",
       "          0.0716,  0.2025, -0.0644,  0.1715, -0.1229,  0.2109,  0.1859, -0.1054,\n",
       "         -0.1848, -0.0210,  0.2153, -0.2205,  0.2326,  0.0760,  0.1717, -0.1245,\n",
       "          0.1619,  0.0801, -0.0675,  0.1391, -0.0184,  0.0958,  0.0363, -0.0665,\n",
       "          0.0200,  0.0803,  0.1562,  0.1158,  0.2056,  0.1482, -0.1369,  0.1570,\n",
       "         -0.0801, -0.0925,  0.1429,  0.0937, -0.0827, -0.1774,  0.1987, -0.1310,\n",
       "         -0.1348,  0.0565,  0.1612, -0.2081, -0.1636, -0.2078,  0.1723,  0.1475,\n",
       "          0.0807, -0.0784, -0.1418,  0.1898,  0.2180,  0.0800,  0.0465, -0.1112,\n",
       "         -0.0139,  0.0872, -0.0396, -0.1285,  0.1102,  0.1574, -0.2254,  0.1179,\n",
       "          0.1395,  0.2266, -0.1713, -0.0528,  0.0248, -0.2029, -0.0168, -0.0254,\n",
       "          0.1659,  0.2090, -0.0710,  0.2249,  0.1464, -0.2183, -0.1769,  0.0661,\n",
       "          0.0583,  0.0740,  0.0099, -0.2044],\n",
       "        [ 0.0140,  0.0316,  0.2120,  0.0609, -0.0624, -0.0915, -0.0892, -0.1446,\n",
       "          0.0508, -0.1748,  0.2124, -0.0356, -0.1519, -0.0041,  0.1837,  0.2162,\n",
       "         -0.0501,  0.1262,  0.2220, -0.0611,  0.1059, -0.0365,  0.2112, -0.1911,\n",
       "          0.0692,  0.1336, -0.0132,  0.1802,  0.2170, -0.1494,  0.0161, -0.0512,\n",
       "         -0.1639,  0.2244,  0.2000, -0.0591,  0.0846,  0.1430, -0.1628,  0.2071,\n",
       "          0.1418,  0.0805, -0.0924,  0.2228,  0.1246,  0.1699,  0.1702,  0.0541,\n",
       "         -0.0197,  0.1168, -0.0562, -0.0434, -0.1723,  0.0247,  0.0162,  0.0811,\n",
       "         -0.1446,  0.1332,  0.0605, -0.1885, -0.0268, -0.2107, -0.2181,  0.0519,\n",
       "          0.0220, -0.0390, -0.1355,  0.1626,  0.1045, -0.0774, -0.2146,  0.1576,\n",
       "          0.0466, -0.1768, -0.1993,  0.0566, -0.0567,  0.1740, -0.1850,  0.1025,\n",
       "         -0.0155,  0.1485, -0.2283, -0.1672, -0.2237, -0.0705,  0.1796,  0.0871,\n",
       "          0.1223,  0.0983,  0.0419, -0.0518, -0.1297, -0.1438,  0.0567,  0.0917,\n",
       "         -0.1119, -0.1207, -0.2290, -0.2054],\n",
       "        [-0.1170, -0.1705,  0.1873,  0.1995,  0.0087, -0.1440,  0.1667,  0.1232,\n",
       "         -0.0894,  0.0511, -0.0911, -0.1683, -0.1291, -0.0704, -0.0011, -0.0183,\n",
       "         -0.0456, -0.0200, -0.1232,  0.2317, -0.0563,  0.0700,  0.1048,  0.1818,\n",
       "         -0.1369,  0.1647,  0.0132,  0.0588,  0.0936, -0.1288,  0.1172, -0.0472,\n",
       "         -0.0127,  0.1906, -0.0156, -0.1062, -0.0081,  0.0214, -0.1893, -0.0312,\n",
       "         -0.2118,  0.1200, -0.0714,  0.0803,  0.1581,  0.2200,  0.0608,  0.2269,\n",
       "          0.1789,  0.2314,  0.1320,  0.0394,  0.1663,  0.2142,  0.1643,  0.1816,\n",
       "         -0.1252, -0.0345,  0.0145, -0.2048, -0.1528, -0.1449, -0.2067,  0.0148,\n",
       "         -0.0765, -0.0341,  0.0338, -0.1541,  0.0495,  0.1025, -0.1746, -0.1122,\n",
       "         -0.1846,  0.1229,  0.1964, -0.0323, -0.2045,  0.1931, -0.1247, -0.0541,\n",
       "         -0.2273,  0.1904,  0.0022, -0.2166,  0.1056,  0.1169,  0.1101,  0.2292,\n",
       "         -0.1465,  0.0238, -0.0404, -0.1674,  0.1884,  0.1376, -0.1286, -0.1355,\n",
       "         -0.0090, -0.1280,  0.1856, -0.1472],\n",
       "        [-0.0089, -0.0219, -0.0890,  0.1522, -0.0449,  0.1251, -0.0878,  0.0364,\n",
       "          0.2213,  0.0723, -0.1976, -0.0438, -0.1057, -0.1634, -0.1554,  0.0629,\n",
       "          0.1945, -0.1260,  0.0453, -0.1253,  0.0023, -0.1915,  0.1602,  0.2275,\n",
       "         -0.0946, -0.1176, -0.0742,  0.0094,  0.0272, -0.1729,  0.1459, -0.1398,\n",
       "          0.0775, -0.1617,  0.1137, -0.0379, -0.0820,  0.1041,  0.0397,  0.1746,\n",
       "          0.1570,  0.2112,  0.0690,  0.0380,  0.1000,  0.1994,  0.0366, -0.0619,\n",
       "         -0.0100,  0.0922,  0.1945,  0.1585, -0.0514,  0.0882,  0.2226,  0.2098,\n",
       "         -0.1774, -0.1509, -0.1750,  0.0490,  0.1077, -0.0892,  0.2061,  0.0882,\n",
       "         -0.2159, -0.0254,  0.0817, -0.1150, -0.0744, -0.2198,  0.2075, -0.0765,\n",
       "          0.1549, -0.2179,  0.1135, -0.0977, -0.2108, -0.0762, -0.0005,  0.1682,\n",
       "         -0.0053,  0.1026,  0.2025, -0.1016,  0.1411,  0.1577,  0.1819,  0.0929,\n",
       "          0.1114,  0.2016,  0.2184,  0.0556,  0.0118,  0.0844,  0.1505, -0.0404,\n",
       "         -0.2073,  0.2238,  0.0459,  0.0323],\n",
       "        [-0.1077,  0.1694, -0.2167,  0.1398,  0.0466, -0.1293,  0.0574,  0.0296,\n",
       "          0.0692, -0.1901, -0.0319,  0.2232,  0.2222, -0.0329,  0.0812, -0.2208,\n",
       "          0.2183, -0.2014, -0.0016,  0.2228, -0.2240,  0.1359,  0.0329,  0.2058,\n",
       "          0.2238, -0.1502,  0.2332,  0.1724, -0.1395,  0.1131,  0.2032,  0.1701,\n",
       "          0.0624,  0.1235, -0.2187, -0.0679, -0.1137, -0.0770,  0.1172, -0.0959,\n",
       "          0.1647,  0.2251, -0.0166,  0.0899,  0.2221, -0.0238, -0.1226,  0.1317,\n",
       "          0.1016, -0.1998, -0.0479, -0.0783, -0.2174,  0.0385,  0.1095, -0.1936,\n",
       "         -0.0538,  0.1455,  0.0538, -0.1563,  0.1045, -0.0313,  0.0460,  0.1170,\n",
       "          0.0464,  0.0420,  0.0445, -0.1623,  0.0945, -0.0829,  0.1768,  0.1362,\n",
       "          0.1479,  0.1276,  0.0836,  0.1874,  0.1700,  0.2117,  0.0622, -0.0602,\n",
       "          0.0266,  0.0231,  0.0504,  0.0935, -0.1898, -0.0115,  0.0270, -0.1767,\n",
       "          0.1831,  0.0119,  0.0484,  0.0642, -0.2202,  0.2214, -0.1510, -0.1246,\n",
       "         -0.0923,  0.0725,  0.0072, -0.0976],\n",
       "        [ 0.0372, -0.1276,  0.0264, -0.0113,  0.0323, -0.2045,  0.0429, -0.1966,\n",
       "         -0.2264, -0.1625,  0.2269, -0.1527, -0.0950,  0.1674, -0.1532, -0.2148,\n",
       "         -0.1695, -0.1649, -0.0314,  0.2175,  0.2198,  0.0876, -0.1367, -0.0979,\n",
       "          0.0559, -0.0731,  0.1023,  0.1633,  0.0743,  0.0798,  0.0777, -0.0125,\n",
       "         -0.1128,  0.2125, -0.1277, -0.1983,  0.0359,  0.0110, -0.0262,  0.1613,\n",
       "          0.2209, -0.0049,  0.0300, -0.0230, -0.1233,  0.0307, -0.1222,  0.0732,\n",
       "          0.1717,  0.1841,  0.0165,  0.0826, -0.1548,  0.0608, -0.1038,  0.2118,\n",
       "          0.0399,  0.2243,  0.1617, -0.1822, -0.1248, -0.2150,  0.0644, -0.1496,\n",
       "         -0.1192, -0.1732, -0.0240, -0.2147,  0.1268, -0.1287,  0.2137,  0.1392,\n",
       "         -0.0929,  0.1325, -0.0641, -0.1755,  0.1646, -0.0854,  0.1482,  0.0262,\n",
       "         -0.0465, -0.0635, -0.1113,  0.2297, -0.1729, -0.1277, -0.1362,  0.1211,\n",
       "         -0.1278,  0.2285, -0.0173, -0.0748,  0.0751,  0.0992,  0.2019, -0.1776,\n",
       "          0.0325, -0.0738,  0.0418, -0.0072],\n",
       "        [ 0.0666,  0.0413,  0.0366, -0.1187, -0.1063, -0.1190,  0.2155,  0.0512,\n",
       "          0.1441, -0.1041, -0.2003,  0.1134, -0.1601,  0.0759,  0.1475,  0.0605,\n",
       "         -0.0583, -0.0425,  0.2287,  0.0107, -0.0506, -0.1466,  0.0150,  0.2194,\n",
       "         -0.0220, -0.1763, -0.0436, -0.0620,  0.1208, -0.2245, -0.1515, -0.1198,\n",
       "         -0.0024,  0.1660,  0.0582,  0.0102, -0.1646, -0.1665,  0.0958,  0.2201,\n",
       "          0.0031,  0.0777,  0.1288,  0.1357,  0.1915,  0.2070, -0.1095, -0.1796,\n",
       "         -0.1685,  0.1603,  0.0935, -0.1546, -0.0393,  0.0041, -0.1576,  0.0704,\n",
       "          0.1178, -0.2130, -0.1582,  0.1239,  0.0377,  0.0326,  0.2123,  0.1259,\n",
       "         -0.1102,  0.1040,  0.0957,  0.0586,  0.1647,  0.1917,  0.0685,  0.1754,\n",
       "          0.1936,  0.2189, -0.0630,  0.0909,  0.1884,  0.2163,  0.1977, -0.1638,\n",
       "         -0.1969, -0.0171, -0.1637, -0.2202,  0.0113, -0.0675,  0.1354,  0.0842,\n",
       "         -0.0589,  0.0433, -0.0299, -0.0158,  0.1003,  0.1369,  0.1745, -0.0717,\n",
       "          0.0460,  0.0574, -0.1854, -0.0514]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear4= torch.nn.Linear(784,200, bias=True)\n",
    "linear5= torch.nn.Linear(200,100, bias=True)\n",
    "linear6 = torch.nn.Linear(100,10, bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "bn3 = torch.nn.BatchNorm1d(200)\n",
    "bn4 =  torch.nn.BatchNorm1d(100)\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "torch.nn.init.xavier_uniform_(linear4.weight)\n",
    "torch.nn.init.xavier_uniform_(linear5.weight)\n",
    "torch.nn.init.xavier_uniform_(linear6.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1= torch.nn.Sequential(linear4,bn3,relu,dropout,linear5,bn4, relu,dropout,linear6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model_1.parameters(),lr=learning_rate)\n",
    "train_total_batch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.454125047\n",
      "Epoch: 0002 cost= 0.328139603\n",
      "Epoch: 0003 cost= 0.288275033\n",
      "Epoch: 0004 cost= 0.266167164\n",
      "Epoch: 0005 cost= 0.248139113\n",
      "Epoch: 0006 cost= 0.250120431\n",
      "Epoch: 0007 cost= 0.233583078\n",
      "Epoch: 0008 cost= 0.229012609\n",
      "Epoch: 0009 cost= 0.218645975\n",
      "Epoch: 0010 cost= 0.220448241\n",
      "Epoch: 0011 cost= 0.219343126\n",
      "Epoch: 0012 cost= 0.205362886\n",
      "Epoch: 0013 cost= 0.211231396\n",
      "Epoch: 0014 cost= 0.202726185\n",
      "Epoch: 0015 cost= 0.183361217\n",
      "Learning finished.\n"
     ]
    }
   ],
   "source": [
    "model_1.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X,Y in train_loader:\n",
    "        X = X.view(-1, 28*28).to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model_1(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost/ train_total_batch\n",
    "    \n",
    "    print('Epoch:','%04d' % (epoch+1),'cost=', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9404000043869019\n",
      "Label:  8\n",
      "Prediction:  0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model_1.eval()\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1,28*28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = model_1(X_test)\n",
    "        \n",
    "    correct_prediction = torch.argmax(prediction,1)==Y_test\n",
    "    accuracy = correct_prediction.float().mean() \n",
    "    print('Accuracy:', accuracy.item()) \n",
    "    \n",
    "    r = random.randint(0,len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r+1].view(-1,28*28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r+1].to(device)\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model_1(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 784->150->50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.7485e-01, -2.0509e-01,  1.0968e-01, -2.4953e-02, -1.9736e-01,\n",
       "          1.5979e-01, -1.2590e-01,  8.8654e-03,  9.7382e-02,  1.3839e-01,\n",
       "         -3.0570e-01,  8.0246e-02, -3.1390e-01, -2.8424e-01, -9.8025e-02,\n",
       "          2.4052e-02, -1.7239e-01, -1.7766e-01, -4.8179e-02, -1.2388e-01,\n",
       "         -1.4764e-01, -1.6261e-01, -3.0202e-01,  1.4000e-01, -1.7158e-01,\n",
       "          1.9509e-01,  3.2688e-02,  3.0909e-01,  1.4583e-01, -4.5996e-02,\n",
       "          2.3496e-01,  1.3800e-01, -2.0479e-01, -1.8202e-01,  2.5202e-01,\n",
       "         -1.2104e-01,  1.2390e-01,  1.3205e-01, -2.6791e-01, -2.5703e-01,\n",
       "          2.8499e-01, -1.4580e-01, -2.9219e-01, -1.2449e-01, -2.1938e-01,\n",
       "         -2.6892e-02, -3.0421e-01, -3.0995e-01,  3.0628e-02,  1.2769e-01],\n",
       "        [-9.4942e-03,  5.8623e-02, -2.1349e-01, -9.0995e-02, -6.9170e-03,\n",
       "         -1.0219e-01,  2.8444e-01, -1.4533e-01, -2.8501e-01,  1.4398e-01,\n",
       "          2.9918e-01, -2.6231e-03, -2.8151e-01,  4.3534e-02, -2.3266e-01,\n",
       "          5.5223e-02,  2.2776e-01, -1.4258e-01,  8.2867e-02, -6.3958e-02,\n",
       "          2.0869e-01,  7.5547e-02,  2.9345e-01,  1.9604e-01, -2.8571e-01,\n",
       "         -2.3272e-01,  1.2675e-01,  1.4521e-01,  6.0176e-02, -2.6511e-01,\n",
       "          4.8913e-02,  2.1461e-01, -6.2007e-02, -1.1845e-01,  6.3997e-02,\n",
       "          9.3032e-02, -9.5825e-02, -2.8748e-01, -2.4449e-01,  3.0771e-01,\n",
       "         -2.7922e-01, -1.8779e-01, -2.3325e-01,  8.0841e-02,  1.3545e-01,\n",
       "         -1.2332e-01, -4.8009e-02,  2.0687e-01,  2.1374e-01,  2.8994e-01],\n",
       "        [ 2.9116e-01,  1.2915e-01,  3.1462e-01,  9.4916e-02,  6.0568e-02,\n",
       "         -8.8822e-03, -2.0682e-02, -1.0618e-01,  2.8202e-01,  7.2037e-02,\n",
       "         -8.0366e-02,  1.5683e-01, -1.8867e-02, -1.6988e-01,  2.9113e-01,\n",
       "          1.7702e-01, -8.2535e-02,  1.3438e-01, -4.8809e-03,  1.0115e-01,\n",
       "          1.8797e-01, -6.7253e-02, -1.2391e-01, -1.4114e-01, -2.9508e-01,\n",
       "         -1.2641e-01,  1.4430e-01,  2.5556e-01,  2.5042e-01, -2.7809e-01,\n",
       "          1.6986e-01,  2.3496e-01, -6.0397e-02,  3.1945e-02,  6.1855e-02,\n",
       "          1.8201e-01,  1.6922e-01, -4.8689e-03,  3.2326e-02,  6.7276e-02,\n",
       "          3.1413e-01,  9.6278e-02, -1.3206e-01, -8.1361e-02,  1.6705e-01,\n",
       "         -3.0458e-01, -1.4517e-02, -2.8623e-01, -2.0761e-01, -6.1498e-02],\n",
       "        [ 2.1449e-01, -2.6085e-02,  6.1279e-02,  1.6553e-01, -2.2326e-01,\n",
       "          1.1792e-01, -1.0695e-01,  1.6341e-01,  1.8394e-01, -1.6918e-01,\n",
       "          1.0362e-04, -1.5018e-02,  3.0374e-01, -1.8455e-01, -1.8184e-01,\n",
       "         -1.0972e-01, -2.5425e-01,  1.5823e-01, -3.3377e-02, -4.8216e-02,\n",
       "          1.0184e-01, -1.9257e-01,  2.5373e-01,  2.2920e-01,  1.3907e-01,\n",
       "          2.9929e-01, -2.6720e-01,  2.5322e-01,  2.7134e-02, -2.2497e-01,\n",
       "         -4.9078e-02,  2.3610e-01,  5.3139e-02,  2.3270e-01, -3.1189e-01,\n",
       "         -3.0665e-01, -2.7286e-01,  7.5409e-02,  2.6789e-01, -3.3982e-02,\n",
       "         -4.9357e-02,  8.4642e-02,  2.8966e-01,  6.8550e-02,  2.0092e-01,\n",
       "          1.0047e-01, -2.8235e-02,  2.3497e-01,  1.2811e-01,  1.2989e-02],\n",
       "        [-1.6769e-01,  3.0473e-01, -2.2611e-01,  2.1743e-01,  2.8351e-01,\n",
       "         -2.0800e-01,  2.1532e-01,  5.7887e-02,  1.0153e-02,  2.5257e-01,\n",
       "         -2.7671e-01,  1.3999e-01, -1.9841e-01, -2.5778e-01, -2.2301e-01,\n",
       "         -6.5933e-02, -1.5109e-01, -1.1650e-01,  2.8833e-03, -2.1478e-01,\n",
       "          7.3884e-02, -5.2390e-02,  2.6889e-01,  9.4074e-03, -2.2109e-01,\n",
       "          7.4640e-02,  9.2829e-02,  8.3046e-03,  5.2569e-02,  5.4636e-02,\n",
       "         -7.5819e-02, -7.5993e-02, -2.1749e-01, -1.1011e-01,  2.7045e-01,\n",
       "         -2.1823e-01, -7.2942e-03,  3.4054e-02, -2.1231e-01,  3.9698e-02,\n",
       "          3.0705e-01, -1.8007e-01,  2.7847e-01,  7.1559e-02,  1.7066e-01,\n",
       "          5.1181e-02, -1.7100e-01,  2.5110e-01, -3.0029e-01,  4.3223e-03],\n",
       "        [ 1.8861e-01, -2.1742e-01, -1.2419e-01, -5.0801e-03, -8.2057e-02,\n",
       "          2.2122e-01,  1.5849e-01, -1.4931e-01, -2.4599e-01,  4.1982e-02,\n",
       "         -8.3228e-02, -1.8165e-01,  5.2212e-02,  1.1732e-01,  3.9180e-02,\n",
       "          2.6261e-01, -2.6431e-01,  2.3136e-01,  7.8432e-02,  1.0106e-01,\n",
       "         -3.0446e-01,  1.5374e-01, -2.6378e-01,  2.3583e-01,  1.0008e-01,\n",
       "         -1.3734e-01,  1.6878e-02,  2.6868e-01, -3.0101e-01, -1.0079e-01,\n",
       "          6.9885e-02,  2.8153e-01,  2.5334e-01,  7.2134e-02,  2.0856e-01,\n",
       "          2.6086e-01, -8.6779e-02,  2.5297e-02, -3.8976e-02,  2.8608e-01,\n",
       "          1.5916e-01,  2.8008e-01,  9.7892e-03,  2.4494e-01,  3.1597e-01,\n",
       "         -6.1196e-02, -3.1880e-02,  1.2209e-01, -2.3604e-01, -2.3114e-02],\n",
       "        [-6.8334e-02, -3.8548e-02,  1.5551e-01, -2.8960e-01,  2.3153e-01,\n",
       "          1.4596e-01,  3.0544e-01,  1.0387e-01, -1.6870e-01,  1.6497e-01,\n",
       "          1.7556e-01,  1.1549e-01,  1.3893e-01,  1.5802e-01, -2.1593e-01,\n",
       "          4.9094e-02, -7.5546e-02,  7.5806e-03, -1.9372e-01, -4.0565e-02,\n",
       "          1.8433e-01, -1.1444e-01,  3.5981e-02,  2.8840e-01,  1.9356e-01,\n",
       "          1.6740e-01, -1.6461e-01, -2.7743e-01, -1.5496e-01,  2.4558e-01,\n",
       "          2.0413e-01, -2.9052e-01, -1.1376e-01,  6.7406e-02,  1.1975e-01,\n",
       "         -9.8499e-02, -1.9126e-01,  8.5715e-02, -2.3906e-01,  3.2838e-03,\n",
       "         -1.5025e-01, -2.4407e-01,  1.1330e-01, -2.2058e-01,  1.2904e-01,\n",
       "         -5.3432e-02, -2.6233e-01, -1.9716e-01,  2.4949e-01,  7.7860e-02],\n",
       "        [ 1.0561e-02,  1.3347e-01, -2.6343e-01,  2.5666e-01, -1.2318e-01,\n",
       "          2.0667e-01, -1.3264e-01,  1.4075e-01,  3.0545e-01,  2.1932e-01,\n",
       "         -7.8929e-02, -7.4608e-02,  1.7526e-01, -5.4374e-02, -2.7720e-01,\n",
       "         -2.5289e-01,  3.5733e-02, -1.6382e-01,  3.1495e-01,  2.8640e-01,\n",
       "         -3.0776e-01,  5.5152e-02, -1.8583e-01, -1.9204e-01,  1.9774e-01,\n",
       "          7.5880e-03, -2.5887e-02,  3.0488e-01, -2.4502e-01, -6.3471e-02,\n",
       "         -1.2011e-01,  6.6944e-02,  2.9919e-01, -2.2957e-01, -2.0299e-01,\n",
       "         -2.7975e-01, -9.1682e-02, -1.8066e-01,  2.4218e-01,  2.0915e-01,\n",
       "          5.5152e-02, -9.7222e-02,  3.7266e-02,  2.3500e-01,  9.9768e-02,\n",
       "          2.0976e-01, -1.7832e-01,  1.1447e-01, -1.3895e-01,  2.1182e-01],\n",
       "        [ 3.2550e-02,  7.3156e-02,  1.5987e-01,  1.7799e-01,  1.8810e-01,\n",
       "          1.0454e-01, -5.1367e-02,  2.1927e-01,  1.1318e-02, -2.1473e-01,\n",
       "         -1.2297e-01,  1.4066e-01,  1.9072e-01, -2.5372e-01, -3.1370e-01,\n",
       "          1.0718e-01, -3.0786e-01,  8.8578e-02, -5.2343e-02, -3.0085e-01,\n",
       "         -2.3923e-01, -3.6069e-02, -3.0361e-01,  2.6220e-01, -2.2400e-01,\n",
       "          2.9124e-03, -2.5611e-01,  3.0284e-01,  1.5796e-01, -8.2823e-02,\n",
       "          2.9080e-01, -2.2143e-01,  3.1078e-01, -2.5417e-01,  3.0699e-01,\n",
       "         -8.4501e-02, -2.7103e-01,  1.6826e-02, -2.1801e-02, -2.6010e-01,\n",
       "         -1.7770e-01,  2.0783e-01, -1.6497e-01, -3.3664e-02, -2.0190e-01,\n",
       "          1.7363e-01,  1.0776e-01, -2.3324e-01,  6.8277e-03,  4.7424e-03],\n",
       "        [ 1.3845e-01,  4.1223e-02,  1.8434e-01,  1.5435e-01,  6.4256e-02,\n",
       "         -1.5209e-02,  1.6478e-01, -6.3891e-02, -2.4463e-01,  2.6970e-01,\n",
       "         -1.6101e-01,  1.7943e-01,  1.2706e-01,  1.2010e-03, -7.0234e-02,\n",
       "          2.6770e-01,  8.1953e-02,  4.1226e-02, -1.2575e-01, -1.7004e-01,\n",
       "          7.2460e-02,  1.3594e-02,  9.7389e-02, -1.3204e-02, -7.8772e-02,\n",
       "         -1.8140e-01, -1.0910e-01,  1.1093e-01,  1.6272e-01,  8.1531e-02,\n",
       "         -2.1079e-01,  3.2766e-02,  9.1703e-02, -1.2708e-01, -1.2902e-01,\n",
       "          1.7503e-01,  7.6224e-02, -1.3918e-01,  5.3697e-02,  2.5241e-01,\n",
       "          6.5204e-02, -2.3331e-01,  3.5768e-02,  8.1939e-02,  2.0515e-01,\n",
       "         -1.4752e-02,  1.2311e-01, -1.4470e-01, -1.0287e-01, -2.5078e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear7= torch.nn.Linear(784,150, bias=True)\n",
    "linear8= torch.nn.Linear(150,50, bias=True)\n",
    "linear9 = torch.nn.Linear(50,10, bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "bn5 = torch.nn.BatchNorm1d(150)\n",
    "bn6 =  torch.nn.BatchNorm1d(50)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "torch.nn.init.xavier_uniform_(linear7.weight)\n",
    "torch.nn.init.xavier_uniform_(linear8.weight)\n",
    "torch.nn.init.xavier_uniform_(linear9.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2= torch.nn.Sequential(linear7,bn5,relu,dropout,linear8,bn6, relu,dropout,linear9).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model_2.parameters(),lr=learning_rate)\n",
    "train_total_batch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.481632590\n",
      "Epoch: 0002 cost= 0.332700282\n",
      "Epoch: 0003 cost= 0.304669112\n",
      "Epoch: 0004 cost= 0.286051154\n",
      "Epoch: 0005 cost= 0.260747403\n",
      "Epoch: 0006 cost= 0.260704279\n",
      "Epoch: 0007 cost= 0.257174045\n",
      "Epoch: 0008 cost= 0.244761392\n",
      "Epoch: 0009 cost= 0.239918828\n",
      "Epoch: 0010 cost= 0.223045096\n",
      "Epoch: 0011 cost= 0.222738445\n",
      "Epoch: 0012 cost= 0.216413900\n",
      "Epoch: 0013 cost= 0.215562791\n",
      "Epoch: 0014 cost= 0.218311787\n",
      "Epoch: 0015 cost= 0.210391760\n",
      "Learning finished.\n"
     ]
    }
   ],
   "source": [
    "model_2.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X,Y in train_loader:\n",
    "        X = X.view(-1, 28*28).to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model_2(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost/ train_total_batch\n",
    "    \n",
    "    print('Epoch:','%04d' % (epoch+1),'cost=', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9254999756813049\n",
      "Label:  3\n",
      "Prediction:  3\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model_2.eval()\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1,28*28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = model(X_test)\n",
    "    \n",
    "    correct_prediction = torch.argmax(prediction,1)==Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "    \n",
    "    r = random.randint(0,len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r+1].view(-1,28*28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r+1].to(device)\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model_2(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
