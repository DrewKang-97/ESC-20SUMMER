{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZm-ZNiVGlHD",
        "colab_type": "text"
      },
      "source": [
        "# **ESC 20-SUMMER HW2**\n",
        "### **19기 최우현**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **1. 주석을 기반으로 하여 코딩을 해주세요.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvyERySs2sbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pylab as plt\n",
        "import random"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVFFHNrkDuVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yIb5G4d3G11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Setting Parameters\n",
        "learning_rate = 0.1\n",
        "training_epochs = 15\n",
        "batch_size = 100"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjbhadsx3HAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train and test sets by loading MNIST datasets \n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yMRTHUq3HGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          drop_last=True)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6jtx6Lt3G9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기) \n",
        "# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n",
        "linear1 = torch.nn.Linear(784, 100, bias=True)\n",
        "linear2 = torch.nn.Linear(100, 100, bias=True)\n",
        "linear3 = torch.nn.Linear(100, 10, bias=True)\n",
        "\n",
        "relu = torch.nn.ReLU()\n",
        "bn1 = torch.nn.BatchNorm1d(100)\n",
        "bn2 = torch.nn.BatchNorm1d(100)\n",
        "\n",
        "drop_prob = 0.3\n",
        "dropout = torch.nn.Dropout(p=drop_prob)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4oMTZz04Nhk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "599405b3-ff99-4aed-ac03-581f9a6d6af9"
      },
      "source": [
        "# xavier initialization을 이용하여 각 layer의 weight 초기화\n",
        "torch.nn.init.xavier_uniform_(linear1.weight)\n",
        "torch.nn.init.xavier_uniform_(linear2.weight)\n",
        "torch.nn.init.xavier_uniform_(linear3.weight)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-6.2633e-02, -1.3666e-02,  6.7819e-03,  2.1235e-01, -2.2802e-01,\n",
              "          3.3828e-02,  3.7804e-02,  1.8646e-02,  1.5740e-01, -7.2078e-02,\n",
              "          1.0993e-01,  6.3241e-02, -1.9329e-01, -1.5207e-01, -1.5088e-01,\n",
              "         -7.3699e-02, -1.7913e-01, -1.8692e-02, -9.9205e-03,  2.7981e-02,\n",
              "          1.4345e-01, -4.4702e-02,  9.2977e-02,  2.2748e-01,  1.3338e-01,\n",
              "          3.2675e-02,  7.2531e-02, -5.5378e-02,  2.2212e-01, -1.2881e-01,\n",
              "         -1.0136e-01, -1.6306e-01,  1.9431e-02, -1.6841e-01,  2.1120e-01,\n",
              "          1.4543e-01,  1.0364e-01,  1.5119e-01,  1.2235e-01, -4.4031e-02,\n",
              "         -1.6112e-01, -8.0274e-02, -1.9023e-01, -1.9396e-01,  5.7393e-02,\n",
              "         -8.3063e-02,  1.7044e-01,  8.0732e-02,  1.0061e-01,  9.8878e-02,\n",
              "         -2.2450e-01, -1.6809e-01, -2.1813e-01,  1.9226e-01, -1.4771e-01,\n",
              "          3.0419e-02,  1.5642e-01, -1.4071e-01,  1.9636e-01, -8.8909e-02,\n",
              "          5.4119e-02,  6.8158e-03,  2.1694e-01,  1.4856e-01,  1.6720e-01,\n",
              "          9.2527e-02,  1.2920e-01, -1.9791e-02, -1.2459e-01, -8.7753e-02,\n",
              "         -5.5263e-02, -1.1062e-01, -7.5888e-02, -1.7271e-01,  9.2971e-02,\n",
              "          8.7732e-02, -1.5687e-01, -2.0456e-01, -1.7113e-01,  1.5842e-01,\n",
              "         -1.1709e-01, -6.5241e-02,  2.0149e-01, -2.1041e-01, -1.8136e-01,\n",
              "         -1.3557e-01, -1.4849e-01,  1.3949e-01, -6.3980e-03, -1.9696e-01,\n",
              "         -5.7679e-02,  2.1382e-01,  8.9886e-02,  1.6519e-01,  1.1409e-02,\n",
              "          4.8136e-02, -9.8521e-02,  1.0263e-01,  1.9365e-01,  1.7746e-01],\n",
              "        [-1.6553e-01, -1.6097e-01,  7.8771e-02,  1.5756e-01,  1.4941e-01,\n",
              "          4.4494e-02, -1.4182e-01, -3.2872e-02,  1.0732e-01,  5.4274e-02,\n",
              "         -7.5394e-03,  1.3340e-01, -2.0251e-01,  1.5239e-01,  2.0748e-01,\n",
              "         -5.0767e-02, -3.7348e-02, -6.3034e-02,  1.1678e-01, -1.9507e-01,\n",
              "          9.2053e-02,  1.2819e-01, -9.7011e-02,  2.1162e-01, -5.2146e-02,\n",
              "         -5.4049e-02, -1.4567e-01, -1.7658e-01,  9.5885e-02,  1.0685e-01,\n",
              "         -7.9948e-02, -7.6411e-02,  9.7543e-02, -2.1468e-01,  3.4717e-02,\n",
              "         -1.5921e-01, -1.5166e-01,  2.2815e-01, -5.7130e-04,  1.4945e-01,\n",
              "          5.7691e-02,  1.5206e-01, -1.4487e-01, -1.6132e-01,  3.3004e-02,\n",
              "         -1.2534e-01,  1.2023e-01, -1.5437e-01, -2.1741e-02,  1.5324e-02,\n",
              "          1.2692e-01,  2.1731e-01,  1.5710e-01, -9.7233e-02,  2.1830e-01,\n",
              "         -1.4653e-01,  3.0300e-02, -1.8823e-01,  1.1183e-01, -2.0991e-01,\n",
              "          1.6739e-01, -1.8507e-01, -1.6911e-01, -2.1383e-01,  1.9409e-01,\n",
              "          2.1867e-01, -1.6733e-01, -2.1343e-01, -1.8373e-02,  1.1375e-01,\n",
              "         -9.6133e-02,  1.1056e-01, -4.5142e-02, -1.1723e-01,  1.7566e-02,\n",
              "         -1.4001e-01,  1.9457e-01,  2.1781e-01,  8.2423e-02,  8.6930e-02,\n",
              "          2.8620e-02, -1.7733e-01, -2.1713e-01, -2.4769e-02, -6.2690e-02,\n",
              "          9.7229e-03,  6.1350e-02,  1.0703e-01, -2.2143e-01,  1.7126e-01,\n",
              "         -3.6691e-02,  1.5262e-01,  1.5921e-01, -1.1128e-01, -1.1999e-01,\n",
              "          1.6200e-01,  9.5315e-02, -9.3449e-02,  3.9714e-02,  4.3511e-03],\n",
              "        [ 2.1464e-01, -2.0239e-01, -1.1964e-01, -1.4997e-01, -1.2039e-01,\n",
              "          3.5554e-03, -8.8481e-02, -1.7627e-01,  1.5291e-01,  1.1499e-01,\n",
              "          2.3032e-01,  2.1454e-01, -2.2231e-01, -6.2731e-02,  3.3028e-02,\n",
              "          2.0633e-04, -1.6481e-01,  2.8578e-02,  1.2573e-01, -3.6757e-02,\n",
              "         -4.0859e-02, -7.7576e-02, -2.9878e-02,  9.8529e-02, -9.8076e-02,\n",
              "          6.6930e-02,  9.7675e-02, -6.2733e-03, -9.5014e-02, -1.4507e-01,\n",
              "         -2.2774e-01,  2.0291e-02,  2.0858e-01, -1.0300e-01,  2.1395e-01,\n",
              "         -1.7776e-01, -3.5471e-03,  9.2629e-02, -6.6288e-02, -1.5037e-01,\n",
              "          8.7622e-02, -2.1310e-01, -1.4956e-01,  1.5824e-01,  1.7188e-02,\n",
              "          1.8364e-01, -8.5246e-02, -6.7537e-02,  1.9472e-01,  4.6886e-02,\n",
              "         -8.8490e-02,  2.3353e-01, -2.6032e-02,  4.4893e-02, -1.1091e-01,\n",
              "         -1.1798e-01,  1.6825e-01,  1.0792e-01, -1.7748e-01, -7.2169e-02,\n",
              "          1.1616e-01, -1.4144e-01,  9.7600e-02, -4.7962e-02,  4.7220e-02,\n",
              "          1.2324e-01,  1.5831e-01,  1.6145e-01,  2.2549e-02,  1.9464e-01,\n",
              "          1.2182e-01, -3.4187e-02, -2.8735e-02,  1.4444e-01, -9.0827e-02,\n",
              "         -1.1111e-01,  2.2006e-01,  6.3187e-02, -2.1455e-01, -1.9390e-01,\n",
              "         -8.0467e-02, -1.7043e-01, -9.8820e-02, -5.7805e-02,  6.0047e-03,\n",
              "         -9.4499e-02, -4.8645e-02, -8.2994e-02,  2.5286e-02, -2.0199e-01,\n",
              "          1.9371e-01,  5.4357e-02,  1.4752e-01, -3.0163e-04, -1.9178e-01,\n",
              "         -4.4249e-02,  1.2998e-01,  1.3865e-01, -1.1462e-01,  2.0730e-01],\n",
              "        [ 1.0459e-01, -2.1829e-01, -3.7836e-03,  1.7252e-01,  6.6880e-02,\n",
              "         -1.9346e-03,  1.8661e-02, -1.3139e-01, -8.0895e-02, -1.2079e-02,\n",
              "         -9.5070e-02, -1.5868e-02,  1.8314e-01, -1.1919e-02,  1.1905e-01,\n",
              "         -2.3211e-01, -1.3984e-01,  1.3336e-02,  6.7215e-02,  2.2850e-01,\n",
              "         -2.9541e-02,  1.9969e-01, -1.7325e-01, -3.8536e-02, -1.9585e-02,\n",
              "          7.5343e-02,  1.1979e-01,  2.8052e-02,  9.0369e-02,  1.6057e-01,\n",
              "          4.4195e-02, -3.3928e-03, -2.2814e-01,  2.3803e-02, -1.8364e-01,\n",
              "          2.2165e-01, -8.1417e-02, -9.8670e-02, -1.3770e-01, -2.3262e-01,\n",
              "         -1.8019e-01,  1.8527e-01, -4.7495e-03, -1.6079e-01,  1.9206e-01,\n",
              "         -4.2139e-02, -6.9816e-02, -4.9458e-02, -1.9388e-01, -1.9165e-01,\n",
              "          2.9906e-02,  2.0952e-01, -2.2059e-01,  9.7296e-03,  8.5368e-02,\n",
              "          1.7724e-01,  1.9616e-01, -5.8431e-02,  1.7893e-01,  1.6850e-01,\n",
              "          9.5298e-02, -1.5070e-02, -8.3927e-02, -2.0418e-01,  1.0810e-01,\n",
              "         -9.0221e-02,  1.5142e-01,  1.0011e-01, -1.7617e-01, -2.0307e-01,\n",
              "          1.7879e-01,  1.3335e-01,  9.9995e-02,  9.1290e-02, -1.8721e-02,\n",
              "         -1.3792e-02,  9.4547e-03,  1.0992e-01, -1.6949e-01,  1.6762e-01,\n",
              "         -9.0190e-02,  1.5366e-01, -1.1550e-01,  8.8088e-02, -5.7210e-02,\n",
              "         -1.4605e-01,  7.7784e-02, -6.6547e-02, -9.0158e-02,  7.3124e-02,\n",
              "         -1.5130e-01, -2.5716e-02, -5.1228e-02, -1.5968e-01,  1.7312e-02,\n",
              "         -6.6202e-02, -1.8778e-01,  1.8464e-03, -1.4776e-01, -3.5063e-02],\n",
              "        [-1.6881e-01, -5.5045e-02,  1.5116e-01, -3.6609e-02, -7.1845e-03,\n",
              "          6.9734e-03,  1.8135e-01, -1.7214e-01, -1.5639e-01, -2.9232e-02,\n",
              "          1.5589e-01,  2.1158e-01, -1.4777e-01,  1.9007e-01, -1.8328e-01,\n",
              "         -3.4540e-02,  1.0584e-01,  1.9927e-01, -8.5569e-02,  7.5441e-03,\n",
              "         -9.0518e-02,  1.4284e-01, -2.8688e-04,  2.2789e-01,  9.4710e-02,\n",
              "         -2.0676e-01, -2.2288e-01, -1.9670e-01,  1.0038e-01,  7.4139e-02,\n",
              "          4.4309e-02, -6.9947e-02, -1.0671e-01,  1.0185e-01,  1.8027e-01,\n",
              "         -6.4719e-02, -1.1873e-01, -1.4939e-01, -4.9611e-02, -1.7787e-01,\n",
              "          1.0976e-01, -1.9074e-01, -7.6043e-02, -1.5769e-01,  2.2462e-01,\n",
              "          9.3626e-02, -5.0636e-02,  9.8715e-03, -1.4935e-01, -5.9639e-02,\n",
              "         -2.0915e-01, -1.2199e-01,  3.0676e-02, -5.4573e-02,  6.4736e-02,\n",
              "          1.3736e-01, -4.9734e-03, -1.9459e-02,  2.2457e-01,  8.9960e-02,\n",
              "          8.9430e-02, -2.1982e-01,  3.2232e-02,  2.1719e-01, -2.1896e-01,\n",
              "         -1.6855e-01,  1.6109e-01,  2.8590e-02,  3.9734e-03, -3.2399e-02,\n",
              "          2.7576e-02,  1.3869e-01,  2.2042e-01, -1.1296e-01, -2.2583e-01,\n",
              "         -1.1242e-01,  1.7579e-01,  1.6105e-01,  9.3760e-02,  4.4595e-02,\n",
              "          5.2644e-02, -2.0183e-01,  1.0062e-01, -8.2352e-02, -8.0055e-02,\n",
              "          2.0167e-01, -1.1945e-01,  2.1705e-01,  7.9841e-02,  1.2713e-01,\n",
              "         -4.0239e-02,  3.6739e-03, -4.3602e-02, -1.8346e-01, -3.3095e-02,\n",
              "          2.1754e-01, -2.1884e-01, -1.2057e-01,  1.4068e-01,  1.4316e-01],\n",
              "        [ 1.0281e-01,  7.8743e-02, -1.2461e-02,  1.7995e-01,  8.3507e-02,\n",
              "         -1.1454e-01,  7.8719e-02, -8.0586e-03,  1.2169e-01,  1.0480e-03,\n",
              "         -2.1812e-01, -5.6583e-02,  1.7711e-01, -1.9188e-01, -4.1776e-02,\n",
              "         -2.2157e-02,  1.3932e-01,  3.8298e-02, -1.4430e-02, -1.7268e-01,\n",
              "          5.9750e-02, -1.7536e-01,  1.2651e-01, -1.5891e-01,  1.5215e-01,\n",
              "         -2.1121e-01, -2.0919e-01,  1.4170e-01, -1.7129e-01,  1.9579e-01,\n",
              "          4.3731e-02, -1.9704e-01,  8.7298e-02,  1.8835e-01,  5.0340e-02,\n",
              "         -7.9621e-02,  1.5459e-01, -1.9381e-01,  6.7439e-02,  3.1728e-02,\n",
              "          8.0155e-02,  1.3456e-01, -3.3908e-02,  5.9525e-02, -1.2270e-01,\n",
              "         -1.5020e-01, -2.1198e-01, -7.7274e-02, -1.7443e-01, -1.5598e-01,\n",
              "         -2.8036e-02, -1.2205e-02,  1.1456e-01,  4.1522e-02, -1.5708e-01,\n",
              "          1.5413e-01,  1.0713e-01,  1.5103e-01, -1.6425e-01,  1.9879e-01,\n",
              "          2.1990e-01,  8.6102e-02, -3.8601e-02, -2.5615e-02, -1.7034e-01,\n",
              "          1.1825e-01,  9.0172e-02,  1.6535e-01,  1.9591e-01, -2.0681e-02,\n",
              "         -1.1453e-01,  1.5724e-01,  1.0173e-01, -1.2582e-01, -6.4528e-02,\n",
              "          1.6844e-01,  1.3947e-01,  1.1434e-01,  1.9622e-02, -1.2213e-01,\n",
              "          1.7144e-02,  1.4167e-01, -1.9861e-01, -2.0689e-01, -1.8155e-01,\n",
              "          2.0461e-01,  4.1735e-02, -1.6432e-01,  4.1845e-02, -1.6952e-01,\n",
              "          1.7314e-01, -1.7666e-02, -7.1139e-02, -8.1838e-02,  2.2300e-01,\n",
              "          8.4920e-02,  1.6288e-01, -3.5425e-03,  1.0881e-01, -1.3979e-01],\n",
              "        [-9.2420e-02,  1.0899e-02,  2.1221e-01,  6.7371e-02, -4.7174e-02,\n",
              "         -1.7618e-02,  2.0938e-01, -8.6483e-02, -1.4051e-01, -1.5700e-01,\n",
              "         -1.8345e-01, -1.0951e-01, -4.6292e-02, -1.6601e-01,  1.2531e-01,\n",
              "         -1.1423e-01,  6.1584e-02,  1.6670e-01, -9.6971e-02,  7.0378e-02,\n",
              "          7.9779e-02, -1.4328e-01,  7.7843e-02, -3.5868e-02, -1.9818e-01,\n",
              "         -1.7508e-01, -4.7041e-03,  1.7237e-01, -1.0079e-01, -6.7369e-02,\n",
              "          2.3027e-01, -5.7940e-02,  4.7263e-02, -1.0391e-01, -2.0074e-02,\n",
              "         -1.8192e-02, -1.1208e-01, -2.0243e-01,  1.7183e-02,  1.7119e-02,\n",
              "          2.7476e-02, -2.6630e-02, -9.6149e-02, -9.7580e-02,  2.2735e-03,\n",
              "          2.0069e-01,  6.4201e-02, -3.5308e-02, -1.1279e-01,  2.2845e-01,\n",
              "         -7.7841e-02, -1.6527e-01,  4.3399e-02, -1.9055e-01, -1.7167e-01,\n",
              "         -1.4315e-01,  5.7592e-02,  2.2730e-01, -1.5471e-01, -1.9923e-02,\n",
              "          1.1968e-01,  1.5683e-01, -1.2216e-01, -1.8557e-01, -1.5694e-01,\n",
              "          1.3316e-01,  2.3115e-01,  1.8494e-01, -2.2411e-01,  1.1585e-01,\n",
              "          1.2499e-01, -2.0189e-01, -1.2978e-01,  1.8744e-01,  2.1955e-01,\n",
              "         -1.5979e-03,  4.5461e-02, -9.6663e-02,  1.3549e-01, -1.4628e-02,\n",
              "         -1.5776e-01,  1.4334e-01,  6.6855e-02,  1.1977e-01, -2.3172e-01,\n",
              "          1.9177e-01,  1.8434e-01,  3.6264e-03,  1.1808e-01, -9.3169e-02,\n",
              "         -1.5580e-01,  1.3157e-01,  8.9038e-02,  9.6646e-02,  1.2945e-02,\n",
              "         -1.9839e-01, -1.4616e-01, -2.2998e-01,  8.4357e-03,  5.7487e-02],\n",
              "        [ 1.3903e-01, -7.8925e-03,  7.9674e-02,  1.6947e-01, -2.1502e-01,\n",
              "         -1.4271e-01,  1.3175e-01, -4.9751e-02, -5.8170e-02, -1.8079e-01,\n",
              "         -2.1134e-01, -1.5616e-01, -1.2191e-01,  1.3859e-02,  7.1189e-02,\n",
              "          1.3572e-01,  4.7572e-02, -7.4897e-03,  1.6760e-05, -1.9746e-01,\n",
              "         -2.0809e-01,  2.3310e-01,  1.9003e-03, -1.6850e-01, -1.8337e-01,\n",
              "         -2.0593e-02, -2.1865e-01,  4.2815e-02, -7.6562e-02,  8.8243e-02,\n",
              "         -1.7376e-01, -1.2319e-01, -8.5994e-02, -1.4179e-02, -1.1576e-01,\n",
              "          2.3324e-01, -2.1752e-01,  1.7904e-01, -1.1001e-02, -5.4533e-02,\n",
              "          1.4177e-01, -9.9076e-02, -8.0426e-02,  1.2283e-02, -1.4739e-01,\n",
              "         -1.6229e-01,  1.4873e-01,  1.3770e-01, -7.6782e-02,  1.1032e-02,\n",
              "         -1.9648e-02,  1.1111e-01, -1.7198e-01, -7.1586e-02, -4.2270e-02,\n",
              "         -1.6359e-01,  1.3859e-01, -1.9298e-01, -1.5068e-01, -1.2881e-01,\n",
              "          1.7817e-01,  1.1803e-01, -1.5126e-01,  1.6403e-01,  8.3816e-02,\n",
              "         -4.9017e-02,  7.9790e-02, -6.9053e-02, -4.9032e-02,  1.6134e-01,\n",
              "          1.3821e-01,  7.8817e-02, -2.1077e-01, -6.6041e-02, -1.5642e-02,\n",
              "         -2.0003e-01, -1.0038e-01,  8.6953e-02, -1.0923e-01,  1.1405e-01,\n",
              "         -1.9587e-01,  4.3181e-02, -7.0284e-02,  5.6322e-02,  2.3165e-02,\n",
              "          8.4721e-04, -9.7372e-02,  1.3565e-01, -1.9329e-01,  1.4632e-01,\n",
              "         -2.2763e-01,  2.2080e-01, -1.8349e-01, -3.1314e-02, -2.2324e-01,\n",
              "         -1.8635e-01, -2.0555e-01,  1.1474e-01, -4.5251e-02, -6.7512e-03],\n",
              "        [-1.7221e-01, -1.3370e-01, -1.3294e-01, -3.3949e-02, -7.7830e-02,\n",
              "         -1.4884e-01, -1.3308e-01, -2.1294e-01, -3.4468e-02,  5.8669e-02,\n",
              "         -3.3126e-02,  1.1916e-01,  1.4309e-01, -8.7354e-02,  1.0867e-01,\n",
              "          1.7565e-02,  1.0758e-01, -7.3888e-04, -1.4040e-01,  5.8201e-02,\n",
              "          3.6517e-02, -8.0709e-02, -2.4166e-02, -1.9228e-01,  3.7109e-02,\n",
              "          2.1557e-01,  1.1295e-01, -1.7455e-01, -1.3171e-01,  5.0081e-02,\n",
              "         -1.3690e-01, -1.1438e-01, -1.7543e-01,  1.5044e-01,  5.3819e-03,\n",
              "         -1.2387e-03,  2.1497e-03,  1.4936e-01, -1.1636e-01,  5.0785e-02,\n",
              "         -2.7948e-02, -5.6673e-03, -1.2505e-01, -4.1362e-02, -1.0693e-01,\n",
              "          9.6027e-03,  5.2717e-02,  8.4687e-02,  8.4720e-02, -1.7369e-01,\n",
              "          2.9564e-02,  1.2031e-01, -1.7816e-01,  1.3917e-01, -1.2592e-01,\n",
              "          1.0509e-01,  8.1492e-02,  1.4203e-01,  1.2300e-01, -6.0724e-02,\n",
              "          1.4235e-01, -1.1137e-01,  6.1202e-02,  2.0471e-01,  1.4995e-02,\n",
              "         -1.5323e-01, -2.0497e-01, -1.5027e-01,  8.2271e-02,  5.2448e-02,\n",
              "         -5.5661e-02,  1.7519e-01,  1.6493e-01, -9.5366e-02,  1.4717e-01,\n",
              "          4.0594e-02,  7.6670e-02,  1.6027e-01,  2.7177e-02,  1.4456e-01,\n",
              "          1.5109e-01,  4.6431e-03,  3.2374e-02, -1.4497e-01, -1.9423e-01,\n",
              "          2.5460e-02,  1.8682e-02,  6.9643e-02,  5.2133e-02, -7.5778e-02,\n",
              "          8.9399e-02, -4.0579e-02,  7.7461e-02,  2.2428e-01, -3.4985e-02,\n",
              "          4.1337e-02, -1.9964e-01,  2.2667e-01, -1.2604e-01,  8.0290e-02],\n",
              "        [-1.1331e-01, -1.0698e-01, -2.7535e-02, -1.5780e-01, -2.9224e-02,\n",
              "          4.4759e-02,  2.2276e-01, -1.6400e-01, -3.5739e-02,  9.0456e-02,\n",
              "          1.3820e-01,  2.0618e-01, -2.1815e-01,  3.7719e-02, -7.6614e-02,\n",
              "         -1.2159e-01,  2.2050e-01,  2.2923e-01, -1.2748e-01, -1.4911e-01,\n",
              "         -7.3161e-02, -1.8961e-01,  1.1442e-01,  1.6794e-01,  7.3284e-02,\n",
              "          1.4932e-01,  5.0868e-02, -4.2893e-02, -1.0594e-01,  1.5475e-01,\n",
              "          1.1912e-01,  4.5767e-02, -1.7514e-02, -1.6360e-01, -2.1168e-01,\n",
              "          6.2966e-02,  6.0753e-02, -1.7942e-02, -9.7584e-02,  1.8469e-01,\n",
              "         -1.6171e-01,  1.0670e-01, -1.8743e-01,  2.0049e-01, -1.9275e-01,\n",
              "         -1.2082e-01,  9.6518e-03, -1.3037e-01,  1.2193e-01, -2.1377e-01,\n",
              "         -1.4391e-01, -6.9440e-02,  1.1482e-02,  1.0639e-01,  2.3094e-01,\n",
              "         -3.2054e-02, -1.7751e-01, -2.0364e-01, -3.7722e-02,  8.9566e-02,\n",
              "         -1.5267e-01, -1.9744e-01, -9.3707e-02, -4.2136e-02, -1.0836e-01,\n",
              "         -2.2928e-01, -2.2186e-01,  1.9809e-01, -1.0416e-01,  2.5306e-02,\n",
              "          1.3356e-01, -8.2655e-02,  2.0693e-01,  1.2706e-01, -1.9150e-01,\n",
              "         -7.1240e-02, -9.5722e-02,  1.5506e-01,  1.9879e-01,  5.2414e-02,\n",
              "         -1.7647e-01,  1.6772e-01,  1.8260e-01,  1.9001e-01,  1.9978e-01,\n",
              "          1.6348e-03,  1.4668e-01,  3.4754e-03, -1.5914e-01,  1.1932e-01,\n",
              "          3.7185e-02, -2.2085e-01,  7.2953e-02, -1.5846e-01,  2.3193e-01,\n",
              "         -1.1110e-01, -1.8080e-01,  1.7069e-01, -1.8606e-01,  1.6909e-01]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oUW4QE34Ngp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch.nn.Sequential을 이용하여 model 정의하기(쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n",
        "model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n",
        "                            linear2, bn2, relu, dropout,\n",
        "                            linear3).to(device)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sIq_BPH4NdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CGG1ie64NZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhJWuySv4Uta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cost 계산을 위한 변수 설정 \n",
        "train_total_batch = len(train_loader)\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBh7mbjY4Uq9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "d726b743-0316-444e-fbc5-4e0d8b77193b"
      },
      "source": [
        "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
        "model.train()    # set the model to train mode (dropout=True)\n",
        "\n",
        "for epoch in range(training_epochs): \n",
        "  avg_cost = 0\n",
        "\n",
        "  #train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
        "  for X, Y in train_loader:\n",
        "    # reshape input image into [batch_size by 784]\n",
        "    # label is not one-hot encoded\n",
        "    X = X.view(-1, 28 * 28).to(device)\n",
        "    Y = Y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "    cost = criterion(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    avg_cost += cost / train_total_batch\n",
        "  \n",
        "  print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "print('Learning finished')\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 0.499044180\n",
            "Epoch: 0002 cost = 0.366370738\n",
            "Epoch: 0003 cost = 0.327072978\n",
            "Epoch: 0004 cost = 0.299702227\n",
            "Epoch: 0005 cost = 0.301899821\n",
            "Epoch: 0006 cost = 0.287576944\n",
            "Epoch: 0007 cost = 0.281244099\n",
            "Epoch: 0008 cost = 0.265668184\n",
            "Epoch: 0009 cost = 0.264421642\n",
            "Epoch: 0010 cost = 0.266991317\n",
            "Epoch: 0011 cost = 0.262892544\n",
            "Epoch: 0012 cost = 0.250128269\n",
            "Epoch: 0013 cost = 0.255528241\n",
            "Epoch: 0014 cost = 0.240182489\n",
            "Epoch: 0015 cost = 0.229410902\n",
            "Learning finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAEGql_Y4x20",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "609d8d24-ff4f-4784-d7f6-2aa1038cdb73"
      },
      "source": [
        "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것) #X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용 #accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
        "with torch.no_grad():\n",
        "  model.eval()      # set the model to evaluation mode (dropout=False) \n",
        "  \n",
        "  X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
        "  Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "  prediction = model(X_test)\n",
        "  correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "  accuracy = correct_prediction.float().mean()\n",
        "  print('Accuracy:', accuracy.item())\n",
        "\n",
        "  ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드\n",
        "  r = random.randint(0, len(mnist_test)-1)\n",
        "  X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
        "  Y_single_data = mnist_test.test_labels[r:r + 1]\n",
        "  \n",
        "  print('Label: ', Y_single_data.item())\n",
        "  single_prediction = model(X_single_data)\n",
        "  print('Prediction: ', torch.argmax(single_prediction, 1).item()) "
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9053000211715698\n",
            "Label:  6\n",
            "Prediction:  6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeEfVZpjHT4Q",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **2. 위에서 만든 모델에서 있던 Layer 들의 Hidden node 수를 증가 또는 감소(ex: 200, 300, 50...) 시켰을 때, train set에서의 cost와 test set에서 Accuracy가 기존 결과와 비교하였을 때 어떻게 달라졌는지 비교해주시면 됩니다.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zLuPXSHlWm",
        "colab_type": "text"
      },
      "source": [
        "###784 -> 200 -> 150"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63_lyHK9JKbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linear4 = torch.nn.Linear(784, 200, bias=True)\n",
        "linear5 = torch.nn.Linear(200, 150, bias=True)\n",
        "linear6 = torch.nn.Linear(150, 10, bias=True)\n",
        "\n",
        "relu = torch.nn.ReLU()\n",
        "bn3 = torch.nn.BatchNorm1d(200)\n",
        "bn4 = torch.nn.BatchNorm1d(150)\n",
        "dropout = torch.nn.Dropout(p=drop_prob)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8WmL8m6tIhH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "ca6e798f-017a-4c65-f814-c6d96a8c3cfd"
      },
      "source": [
        "torch.nn.init.xavier_uniform_(linear4.weight)\n",
        "torch.nn.init.xavier_uniform_(linear5.weight)\n",
        "torch.nn.init.xavier_uniform_(linear6.weight)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0286, -0.0360,  0.1372,  ...,  0.0726, -0.1670, -0.1051],\n",
              "        [-0.1543, -0.0325, -0.1490,  ...,  0.1577, -0.0049, -0.1190],\n",
              "        [-0.0793,  0.1111,  0.0599,  ...,  0.0313, -0.0136, -0.1136],\n",
              "        ...,\n",
              "        [-0.1021,  0.1235, -0.1027,  ..., -0.0251,  0.1467, -0.1192],\n",
              "        [ 0.0155,  0.1034,  0.1569,  ...,  0.1199,  0.1338,  0.1008],\n",
              "        [ 0.1037, -0.0836, -0.0296,  ...,  0.1119,  0.0034,  0.0423]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVXuGZzStMrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = torch.nn.Sequential(linear4, bn3, relu, dropout, \n",
        "                             linear5, bn4, relu, dropout,\n",
        "                             linear6).to(device)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv89Lg3B-HMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "# optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
        "optimizer = torch.optim.Adam(model1.parameters(), lr=learning_rate) #model 설정 중요\n",
        "#cost 계산을 위한 변수 설정 \n",
        "train_total_batch = len(train_loader)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtvaxZ83Kaf6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "8a1344f0-2218-4719-fcdd-868129a3380f"
      },
      "source": [
        "model1.train()    # set the model to train mode (dropout=True)\n",
        "\n",
        "for epoch in range(training_epochs): \n",
        "  avg_cost = 0\n",
        "\n",
        "  #train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
        "  for X, Y in train_loader:\n",
        "    # reshape input image into [batch_size by 784]\n",
        "    # label is not one-hot encoded\n",
        "    X = X.view(-1, 28 * 28).to(device)\n",
        "    Y = Y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model1(X)\n",
        "    cost = criterion(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    avg_cost += cost / train_total_batch\n",
        "  \n",
        "  print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "print('Learning finished')\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 0.454042643\n",
            "Epoch: 0002 cost = 0.327877909\n",
            "Epoch: 0003 cost = 0.289008766\n",
            "Epoch: 0004 cost = 0.270124257\n",
            "Epoch: 0005 cost = 0.246979505\n",
            "Epoch: 0006 cost = 0.233637109\n",
            "Epoch: 0007 cost = 0.233835101\n",
            "Epoch: 0008 cost = 0.229399025\n",
            "Epoch: 0009 cost = 0.215847611\n",
            "Epoch: 0010 cost = 0.218724027\n",
            "Epoch: 0011 cost = 0.205324590\n",
            "Epoch: 0012 cost = 0.199643523\n",
            "Epoch: 0013 cost = 0.197351933\n",
            "Epoch: 0014 cost = 0.192683548\n",
            "Epoch: 0015 cost = 0.182401121\n",
            "Learning finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0gs2UMPGokq",
        "colab_type": "text"
      },
      "source": [
        "###784 -> 80 -> 40"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgc1izZRKjbY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "fd37b9ff-09fa-446e-c83f-6405b7ce44a8"
      },
      "source": [
        "with torch.no_grad():\n",
        "  model1.eval()      # set the model to evaluation mode (dropout=False) \n",
        "  \n",
        "  X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
        "  Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "  prediction = model1(X_test)\n",
        "  correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "  accuracy = correct_prediction.float().mean()\n",
        "  print('Accuracy:', accuracy.item())\n",
        "\n",
        "  ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드\n",
        "  r = random.randint(0, len(mnist_test)-1)\n",
        "  X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
        "  Y_single_data = mnist_test.test_labels[r:r + 1]\n",
        "  \n",
        "  print('Label: ', Y_single_data.item())\n",
        "  single_prediction = model1(X_single_data)\n",
        "  print('Prediction: ', torch.argmax(single_prediction, 1).item()) "
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.902400016784668\n",
            "Label:  3\n",
            "Prediction:  3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRMlbzQKKpXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linear7 = torch.nn.Linear(784, 80, bias=True)\n",
        "linear8 = torch.nn.Linear(80, 40, bias=True)\n",
        "linear9 = torch.nn.Linear(40, 10, bias=True)\n",
        "\n",
        "relu = torch.nn.ReLU()\n",
        "bn5 = torch.nn.BatchNorm1d(80)\n",
        "bn6 = torch.nn.BatchNorm1d(40)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SI2LZx7yshc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "outputId": "3ccf2dfc-21e1-434f-ecae-7c236676ce42"
      },
      "source": [
        "torch.nn.init.xavier_uniform_(linear7.weight)\n",
        "torch.nn.init.xavier_uniform_(linear8.weight)\n",
        "torch.nn.init.xavier_uniform_(linear9.weight)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0764, -0.1672, -0.1104,  0.2413, -0.0343,  0.1766, -0.1721, -0.0776,\n",
              "          0.1171, -0.1241, -0.0130,  0.1513, -0.3365,  0.2533,  0.0763,  0.1154,\n",
              "         -0.2137, -0.2114,  0.1076,  0.2315, -0.1514, -0.0470, -0.2208, -0.1064,\n",
              "         -0.3272,  0.2942,  0.1103, -0.1241, -0.2523,  0.2489,  0.2149,  0.0100,\n",
              "          0.0939,  0.1920, -0.1599,  0.3283,  0.1579,  0.2843, -0.3044, -0.3180],\n",
              "        [ 0.0129,  0.1536,  0.1610, -0.3339, -0.2555,  0.2820,  0.0388, -0.1521,\n",
              "          0.1598,  0.2437, -0.0340, -0.1931, -0.0019,  0.0156, -0.2784, -0.0579,\n",
              "          0.2416, -0.2166, -0.1497, -0.1625,  0.1780, -0.3412,  0.1921,  0.2808,\n",
              "         -0.0559, -0.0684, -0.1461,  0.0419, -0.1616, -0.0360,  0.0019, -0.0437,\n",
              "          0.2252, -0.3449,  0.0753, -0.0916, -0.2953, -0.1927,  0.1384, -0.0113],\n",
              "        [-0.0143,  0.2981, -0.1526, -0.0247,  0.1032, -0.2590,  0.2343, -0.1993,\n",
              "         -0.1258, -0.3448,  0.0600,  0.0269, -0.1728,  0.0507,  0.2754,  0.2248,\n",
              "         -0.2423, -0.3355, -0.0179,  0.2281, -0.1763, -0.0811,  0.2715,  0.3112,\n",
              "         -0.1273,  0.0975,  0.0124,  0.2322, -0.0740, -0.2053, -0.1791, -0.3313,\n",
              "          0.1589, -0.1890,  0.1946, -0.2688,  0.0123,  0.1177, -0.0413,  0.1654],\n",
              "        [ 0.3112,  0.2314,  0.0527, -0.3360, -0.2723, -0.1957, -0.3103,  0.2165,\n",
              "          0.2836,  0.1484,  0.0844,  0.0998, -0.1842, -0.3015,  0.2747,  0.1028,\n",
              "          0.1641, -0.1162,  0.2478, -0.0254,  0.2327,  0.1852,  0.3141,  0.2887,\n",
              "          0.3354,  0.2730,  0.2333,  0.0700, -0.1426, -0.1606,  0.1386, -0.1428,\n",
              "         -0.2082, -0.0100,  0.2792,  0.2970, -0.2581,  0.2703, -0.2989, -0.2377],\n",
              "        [ 0.1527,  0.0356,  0.1178, -0.1142, -0.1072, -0.2666,  0.1745,  0.1028,\n",
              "         -0.1526, -0.0492, -0.1674, -0.1234,  0.0416, -0.0210, -0.1113, -0.0480,\n",
              "          0.2975,  0.3141,  0.2996, -0.0152,  0.1694,  0.0087, -0.2586, -0.0938,\n",
              "          0.0523,  0.1310, -0.3362, -0.3414,  0.3443, -0.1140, -0.1206,  0.1681,\n",
              "          0.2169,  0.1901,  0.1997,  0.0301, -0.2847, -0.0169, -0.0823,  0.1787],\n",
              "        [-0.2702, -0.2108, -0.1762,  0.1418,  0.2020, -0.2675,  0.3258,  0.1273,\n",
              "          0.0315,  0.2756,  0.0605,  0.2450, -0.3237,  0.2618, -0.1264, -0.0736,\n",
              "         -0.3427,  0.0916, -0.1057,  0.2163,  0.1710, -0.2774, -0.1764, -0.3066,\n",
              "          0.0916, -0.1380,  0.2988, -0.0364,  0.0361,  0.1108,  0.0590,  0.0380,\n",
              "          0.1880, -0.1273, -0.2217,  0.0080,  0.2033,  0.0194,  0.1383,  0.2615],\n",
              "        [ 0.2665,  0.0937, -0.2536,  0.0600,  0.1680, -0.2444, -0.1624,  0.0608,\n",
              "         -0.1105, -0.2053,  0.2961, -0.0617, -0.1848,  0.0987, -0.2803,  0.0536,\n",
              "         -0.2966,  0.0613, -0.0635,  0.2384,  0.2719, -0.2443, -0.3000,  0.3239,\n",
              "         -0.1442,  0.0346, -0.2788, -0.0885,  0.0213, -0.0482,  0.2692, -0.3157,\n",
              "         -0.2784,  0.1991, -0.3433,  0.1408,  0.2350, -0.2515, -0.3277,  0.3308],\n",
              "        [ 0.0962,  0.1804, -0.2351, -0.1503, -0.0041,  0.0235,  0.2643,  0.0853,\n",
              "          0.2635,  0.0604,  0.0504,  0.1205,  0.1729, -0.2075,  0.1625, -0.2304,\n",
              "         -0.1799,  0.0125,  0.0657,  0.3063,  0.0500,  0.1565, -0.0657,  0.1998,\n",
              "         -0.0151,  0.0659, -0.1077,  0.1478,  0.2464,  0.1999,  0.0405,  0.2793,\n",
              "          0.0503, -0.2080, -0.2211,  0.1031,  0.1946,  0.0975,  0.2013,  0.2078],\n",
              "        [ 0.1206, -0.1833, -0.0951,  0.0451, -0.3096, -0.2790, -0.2716,  0.1367,\n",
              "         -0.0090, -0.1230, -0.2426,  0.2514,  0.2674, -0.2267, -0.1779, -0.2337,\n",
              "          0.0995, -0.0471,  0.1599,  0.3417,  0.1855,  0.0034,  0.2753, -0.2680,\n",
              "         -0.3375, -0.0859,  0.2177,  0.3433, -0.0273, -0.0886, -0.0773,  0.1944,\n",
              "          0.0707, -0.2107, -0.3356,  0.1210, -0.0093, -0.1314,  0.3153, -0.2441],\n",
              "        [-0.3007, -0.2170, -0.2695,  0.1364,  0.1882, -0.0252,  0.1571,  0.2233,\n",
              "         -0.1470, -0.3163,  0.0179, -0.1592,  0.2344, -0.3450,  0.2880, -0.3123,\n",
              "         -0.2908, -0.1620,  0.1560,  0.3020, -0.1959,  0.1046, -0.0081,  0.0803,\n",
              "          0.3334,  0.0066,  0.2762, -0.0537,  0.0932, -0.1082, -0.3193, -0.3298,\n",
              "         -0.1521, -0.2581,  0.2246,  0.0042, -0.2112, -0.0328,  0.2640,  0.0873]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj531Jt4yvE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = torch.nn.Sequential(linear7, bn5, relu, dropout, \n",
        "                             linear8, bn6, relu, dropout,\n",
        "                             linear9).to(device)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq85BOZo-YFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "# optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)\n",
        "#cost 계산을 위한 변수 설정 \n",
        "train_total_batch = len(train_loader)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TkYggcIKpkT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "506958cd-bb9e-4f1d-f14e-de1c2d2d0342"
      },
      "source": [
        "model2.train()    # set the model to train mode (dropout=True)\n",
        "\n",
        "for epoch in range(training_epochs): \n",
        "  avg_cost = 0\n",
        "\n",
        "  #train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
        "  for X, Y in train_loader:\n",
        "    # reshape input image into [batch_size by 784]\n",
        "    # label is not one-hot encoded\n",
        "    X = X.view(-1, 28 * 28).to(device)\n",
        "    Y = Y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model2(X)\n",
        "    cost = criterion(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    avg_cost += cost / train_total_batch\n",
        "  \n",
        "  print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "print('Learning finished')\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 0.529607475\n",
            "Epoch: 0002 cost = 0.386183470\n",
            "Epoch: 0003 cost = 0.344602883\n",
            "Epoch: 0004 cost = 0.332688093\n",
            "Epoch: 0005 cost = 0.323567152\n",
            "Epoch: 0006 cost = 0.302856177\n",
            "Epoch: 0007 cost = 0.286634803\n",
            "Epoch: 0008 cost = 0.288036227\n",
            "Epoch: 0009 cost = 0.280324161\n",
            "Epoch: 0010 cost = 0.279137641\n",
            "Epoch: 0011 cost = 0.272234321\n",
            "Epoch: 0012 cost = 0.266672462\n",
            "Epoch: 0013 cost = 0.261246413\n",
            "Epoch: 0014 cost = 0.262234926\n",
            "Epoch: 0015 cost = 0.259336144\n",
            "Learning finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFI0zPpnKpw2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "b660000f-911c-46b5-a4e2-7c7c3e8e7000"
      },
      "source": [
        "with torch.no_grad():\n",
        "  model2.eval()      # set the model to evaluation mode (dropout=False) \n",
        "  \n",
        "  X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
        "  Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "  prediction = model2(X_test)\n",
        "  correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "  accuracy = correct_prediction.float().mean()\n",
        "  print('Accuracy:', accuracy.item())\n",
        "\n",
        "  ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드\n",
        "  r = random.randint(0, len(mnist_test)-1)\n",
        "  X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
        "  Y_single_data = mnist_test.test_labels[r:r + 1]\n",
        "  \n",
        "  print('Label: ', Y_single_data.item())\n",
        "  single_prediction = model2(X_single_data)\n",
        "  print('Prediction: ', torch.argmax(single_prediction, 1).item()) "
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9509999752044678\n",
            "Label:  5\n",
            "Prediction:  5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}