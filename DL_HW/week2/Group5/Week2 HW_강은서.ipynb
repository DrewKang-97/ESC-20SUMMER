{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1-1) 아래에 주어진 주석을 기반으로 하여 코딩을 해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정 (learning rate, training epochs, batch_size)\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train과 test set으로 나누어 MNIST data 불러오기\n",
    "\n",
    "\n",
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "##train=True/False로 train과 test set 나눠주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          drop_last=True)\n",
    "#shuffle=True: 무작위 순서로 batch를 불러올지.\n",
    "#drop_last: batch_size로 자를 때 맨 마지막에 남는 데이터를 사용할 것인가 버릴 것인가, True면 버리는 것!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n",
    "# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n",
    "\n",
    "##각 layer 앞 뒤 node 숫자 일치시키기##\n",
    "linear1 = torch.nn.Linear(784, 100, bias=True)\n",
    "linear2 = torch.nn.Linear(100, 100, bias=True)\n",
    "linear3 = torch.nn.Linear(100, 10, bias=True)\n",
    "\n",
    "p=0.3 #p=사용하지 않을 비율 설정\n",
    "relu=torch.nn.ReLU()\n",
    "\n",
    "##batch normalization layer의 node숫자와 일치시키기##\n",
    "bn1 = torch.nn.BatchNorm1d(100)\n",
    "bn2 = torch.nn.BatchNorm1d(100)\n",
    "\n",
    "dropout= torch.nn.Dropout(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1205,  0.0780,  0.2083, -0.0511, -0.1907, -0.0021,  0.2305,  0.0628,\n",
       "         -0.2080, -0.1073, -0.0640,  0.0439,  0.0447, -0.0028,  0.2298, -0.1694,\n",
       "         -0.0139,  0.0272, -0.1436, -0.0879, -0.1024, -0.0190, -0.0448,  0.0658,\n",
       "          0.0409,  0.2276, -0.0474,  0.0322, -0.1632, -0.2285,  0.1079,  0.1244,\n",
       "         -0.0589,  0.1471,  0.1774,  0.1100, -0.1320,  0.0690,  0.1815, -0.0626,\n",
       "          0.1877, -0.1733, -0.0384, -0.0366,  0.0745,  0.0683,  0.0731, -0.2073,\n",
       "         -0.1977, -0.2081,  0.2183,  0.0862,  0.0402, -0.0257,  0.0546, -0.0773,\n",
       "         -0.1108,  0.1662, -0.1015, -0.0791, -0.2149, -0.1971, -0.0928, -0.0671,\n",
       "         -0.0284,  0.0535, -0.1230, -0.0234, -0.0683, -0.1999,  0.1092,  0.2007,\n",
       "         -0.0566, -0.0696, -0.1398, -0.0425,  0.1945,  0.0866, -0.0272, -0.0198,\n",
       "         -0.1480, -0.0042, -0.0540,  0.0091, -0.2271,  0.1585, -0.1174,  0.0069,\n",
       "         -0.1820,  0.1152,  0.2179, -0.1312,  0.0164, -0.1986,  0.2067,  0.1842,\n",
       "          0.2164,  0.0500, -0.2005,  0.1760],\n",
       "        [-0.0856, -0.1457, -0.1022, -0.1921,  0.0685,  0.1373, -0.0605, -0.2229,\n",
       "          0.0980,  0.1417, -0.1436,  0.1114,  0.0820,  0.1707, -0.0573, -0.0613,\n",
       "         -0.1493, -0.2103, -0.0906, -0.1580,  0.2061,  0.1599, -0.2123, -0.0189,\n",
       "          0.1813,  0.1305,  0.0011, -0.0339, -0.1984, -0.1800, -0.2322, -0.0926,\n",
       "         -0.0576,  0.0131, -0.1651, -0.0899,  0.1198,  0.1736, -0.0437,  0.2075,\n",
       "          0.1112,  0.1362,  0.0065,  0.0866,  0.1531,  0.1655, -0.0127,  0.0770,\n",
       "         -0.1670,  0.1129, -0.1657, -0.0390, -0.0809,  0.0587, -0.1150,  0.1010,\n",
       "          0.0521,  0.1130, -0.1080, -0.1013, -0.0930, -0.0551, -0.1570, -0.2300,\n",
       "         -0.0495, -0.1798,  0.0309, -0.0915, -0.1983,  0.2154, -0.1727,  0.1845,\n",
       "         -0.0083, -0.0033, -0.1568,  0.1961,  0.0724,  0.1249, -0.1687,  0.2048,\n",
       "          0.1067, -0.1275,  0.1857,  0.0461, -0.1471, -0.1393, -0.0249,  0.0323,\n",
       "         -0.0302,  0.2009, -0.1572, -0.1833,  0.0697,  0.2240, -0.0481,  0.1773,\n",
       "          0.0708,  0.0936, -0.1982,  0.0903],\n",
       "        [-0.0114, -0.0837, -0.1373, -0.1297,  0.1653, -0.1055, -0.1267, -0.1815,\n",
       "         -0.2093, -0.1092, -0.0800,  0.2246,  0.0189, -0.1081,  0.1295,  0.1915,\n",
       "         -0.0339, -0.1331,  0.2292, -0.1244,  0.1222,  0.0355, -0.0962,  0.0342,\n",
       "          0.1613, -0.1294,  0.1582,  0.0266,  0.0986,  0.1661,  0.0357,  0.0592,\n",
       "          0.1419, -0.0421, -0.1303, -0.0555,  0.0579,  0.0102,  0.2158, -0.0517,\n",
       "         -0.1879, -0.0295, -0.1387,  0.0999,  0.1566, -0.0595, -0.1846, -0.0816,\n",
       "         -0.0701,  0.1034, -0.1258, -0.1809,  0.1315, -0.0827, -0.2183,  0.1722,\n",
       "          0.1767,  0.0840,  0.0273,  0.1045,  0.1771, -0.0933,  0.1200, -0.1699,\n",
       "          0.1075,  0.1344, -0.1756, -0.1140,  0.0373,  0.1804, -0.0341, -0.1286,\n",
       "         -0.1992, -0.1263, -0.1096, -0.0216,  0.0156, -0.0103,  0.1247, -0.1879,\n",
       "          0.0878, -0.1566, -0.0615,  0.2072,  0.1661, -0.1159, -0.1491, -0.1167,\n",
       "         -0.1191,  0.0184,  0.1158,  0.1382, -0.0600,  0.0719, -0.1783,  0.1914,\n",
       "         -0.1130,  0.2289, -0.0546,  0.0636],\n",
       "        [-0.0097,  0.0340, -0.0200,  0.1448,  0.0087, -0.1241,  0.1894,  0.0079,\n",
       "          0.0388,  0.0041,  0.0127,  0.0994, -0.2315,  0.1079, -0.0350, -0.2083,\n",
       "         -0.0387, -0.1784,  0.1359, -0.1550, -0.0214,  0.0916, -0.0867,  0.0854,\n",
       "          0.0381, -0.1096,  0.1753, -0.1843, -0.1597, -0.1784,  0.0879,  0.0675,\n",
       "         -0.2146, -0.2054,  0.1811,  0.2326, -0.0785, -0.2193, -0.0395,  0.1070,\n",
       "          0.1468, -0.2195, -0.1006, -0.1459, -0.1959,  0.2234,  0.0182, -0.0715,\n",
       "         -0.0720, -0.0564, -0.1015, -0.1622,  0.0500,  0.0959, -0.0985, -0.1310,\n",
       "          0.0896,  0.0014, -0.0417, -0.2095, -0.2023, -0.1413, -0.0775,  0.2096,\n",
       "          0.1700,  0.0467,  0.1541, -0.0041,  0.0004,  0.1121,  0.0999, -0.1887,\n",
       "         -0.1480,  0.0407, -0.2156,  0.1060, -0.1500, -0.1426,  0.2224, -0.1052,\n",
       "         -0.0714,  0.2299, -0.0134, -0.0108, -0.2185, -0.2298,  0.1496, -0.2154,\n",
       "         -0.2102,  0.0814, -0.2307, -0.0027,  0.1117,  0.0653,  0.1235, -0.2153,\n",
       "          0.1703, -0.1770, -0.2109, -0.2247],\n",
       "        [ 0.2116,  0.0841,  0.1826,  0.2155, -0.0719,  0.1778,  0.0232,  0.1091,\n",
       "         -0.1564,  0.1004, -0.0473,  0.0806, -0.2077, -0.0793,  0.0016, -0.1993,\n",
       "          0.0563,  0.1324, -0.2087, -0.0607, -0.2211, -0.1007,  0.2232, -0.1448,\n",
       "          0.1171,  0.0559,  0.0669,  0.2257, -0.1545,  0.0948, -0.0526,  0.0034,\n",
       "          0.0180,  0.0956, -0.0675,  0.0057, -0.1219, -0.0025, -0.0450,  0.2132,\n",
       "         -0.1143, -0.1093,  0.1849,  0.1911,  0.0197,  0.1430, -0.0040,  0.1445,\n",
       "         -0.1264, -0.0288, -0.0094,  0.1439,  0.0368,  0.1838, -0.0285, -0.0084,\n",
       "          0.0981,  0.0915, -0.1467,  0.2174,  0.0181,  0.0906,  0.1770, -0.1392,\n",
       "          0.0045,  0.0291,  0.1497, -0.1300,  0.2076, -0.0164,  0.2317, -0.0176,\n",
       "          0.2258,  0.1791, -0.1663, -0.1475,  0.1660, -0.0887, -0.0451, -0.1778,\n",
       "         -0.0319,  0.1011,  0.1515, -0.0242, -0.1460,  0.1551, -0.2322, -0.1401,\n",
       "          0.0477,  0.1777, -0.0477,  0.2181, -0.0651,  0.1938, -0.0976,  0.0231,\n",
       "         -0.0602,  0.1176,  0.0557, -0.1590],\n",
       "        [-0.0689,  0.0344,  0.1319, -0.0867,  0.0026, -0.1583, -0.1837, -0.1085,\n",
       "         -0.0515, -0.0410, -0.1530,  0.0437, -0.0261,  0.1200, -0.1758,  0.1765,\n",
       "          0.0603, -0.1283,  0.0471,  0.2199,  0.0649, -0.1280, -0.0855,  0.2057,\n",
       "         -0.1751, -0.0673, -0.1332,  0.0974, -0.0362, -0.1378,  0.0323, -0.2015,\n",
       "         -0.1410, -0.0957,  0.1365, -0.0237, -0.1664,  0.1909, -0.1235,  0.2030,\n",
       "          0.1673, -0.1749, -0.1155,  0.1552,  0.0926,  0.2204,  0.2051,  0.2285,\n",
       "          0.0313,  0.0285,  0.1470, -0.1016, -0.0036, -0.1989, -0.0695,  0.1749,\n",
       "          0.1611,  0.1736, -0.1063, -0.2327, -0.1155,  0.1079,  0.1842,  0.2325,\n",
       "         -0.1884, -0.0251, -0.2270, -0.0051, -0.1166, -0.1974, -0.0056, -0.2214,\n",
       "         -0.1180,  0.1186,  0.1561, -0.1382,  0.1147, -0.0836, -0.1531,  0.0458,\n",
       "          0.0018,  0.0628, -0.0708,  0.0468,  0.2086,  0.0940, -0.2259,  0.1088,\n",
       "         -0.2263,  0.2094, -0.0375,  0.1850, -0.1868, -0.2263,  0.1220, -0.1143,\n",
       "         -0.1214, -0.0570, -0.0315, -0.0232],\n",
       "        [-0.1394, -0.2017,  0.0232, -0.2219,  0.1296,  0.2059,  0.0607, -0.0631,\n",
       "          0.1314,  0.1383, -0.0738,  0.1813,  0.2293, -0.2159, -0.0471, -0.1930,\n",
       "         -0.0639, -0.2069,  0.0683,  0.1458,  0.2048,  0.0111, -0.2209, -0.1761,\n",
       "         -0.0573,  0.0607, -0.1475,  0.2150, -0.0215,  0.1521,  0.0146, -0.0150,\n",
       "         -0.0482,  0.1819,  0.1286, -0.1963,  0.0546, -0.0674,  0.1989,  0.0588,\n",
       "         -0.2005,  0.0779, -0.2128, -0.0213,  0.1377, -0.0769, -0.2174,  0.1082,\n",
       "          0.1687, -0.2128,  0.1880, -0.0059, -0.0564,  0.1296,  0.1539,  0.1690,\n",
       "         -0.0105, -0.0766, -0.0203, -0.1980,  0.1114, -0.1142,  0.0959,  0.1671,\n",
       "          0.0583,  0.1917, -0.0883,  0.1545, -0.0041,  0.1956, -0.2081,  0.1677,\n",
       "          0.1691,  0.0497,  0.2324,  0.0286,  0.1359,  0.1846, -0.0915,  0.1689,\n",
       "          0.1529,  0.1480,  0.0290, -0.2135, -0.1845, -0.1156,  0.0227, -0.1208,\n",
       "          0.0088,  0.1551,  0.2158,  0.1490,  0.0345, -0.0654,  0.2182,  0.1258,\n",
       "         -0.1438,  0.1535,  0.1833,  0.1400],\n",
       "        [ 0.0832,  0.1110, -0.0608, -0.0930, -0.0886,  0.1737, -0.0568,  0.2260,\n",
       "          0.0415,  0.1355,  0.0582,  0.0461,  0.1544,  0.0709,  0.1585,  0.1270,\n",
       "          0.1093, -0.0144,  0.1787,  0.2074,  0.1478, -0.0073,  0.1394, -0.1529,\n",
       "         -0.1760, -0.2204,  0.1038,  0.0318, -0.0296, -0.1925, -0.1884, -0.1340,\n",
       "          0.1120,  0.1653, -0.2282,  0.1540,  0.1394,  0.0422, -0.1299,  0.0383,\n",
       "          0.1648,  0.0469, -0.0158, -0.0676,  0.0254, -0.0094, -0.1519, -0.1324,\n",
       "          0.1725,  0.0134,  0.0840, -0.1232,  0.2309,  0.1991,  0.0241, -0.0543,\n",
       "          0.0883, -0.0493, -0.0712,  0.0866,  0.1602,  0.0382,  0.0268, -0.0179,\n",
       "          0.1648,  0.1385, -0.0805, -0.1196, -0.2127, -0.1749,  0.0240,  0.0542,\n",
       "         -0.0014,  0.2150,  0.2260, -0.0410, -0.0480, -0.2140, -0.1373, -0.2134,\n",
       "          0.1779,  0.0161,  0.0257,  0.0025,  0.1620,  0.1216,  0.0663,  0.0110,\n",
       "          0.0775, -0.1956,  0.2079, -0.1422,  0.2009,  0.0764, -0.1458, -0.1929,\n",
       "          0.1969, -0.0094,  0.1084, -0.0021],\n",
       "        [ 0.1066,  0.0554, -0.1155, -0.0994, -0.0711,  0.2321, -0.0244, -0.1458,\n",
       "         -0.0354,  0.0781,  0.1020, -0.1081,  0.1477,  0.1602, -0.2214,  0.0889,\n",
       "          0.1793, -0.0146,  0.0011, -0.0099,  0.0866,  0.0659,  0.2117, -0.0911,\n",
       "          0.0540, -0.2175, -0.0879,  0.0871, -0.0152,  0.0072, -0.1469,  0.0150,\n",
       "          0.1066, -0.0032,  0.1735, -0.1845, -0.1975,  0.0986, -0.2185,  0.2046,\n",
       "          0.0727,  0.0038, -0.0610,  0.0120,  0.0801, -0.0725, -0.0358, -0.2178,\n",
       "         -0.0905, -0.0131,  0.1909, -0.0801, -0.2313, -0.0345,  0.1310,  0.1031,\n",
       "          0.1003, -0.1824, -0.0620, -0.0963, -0.1983, -0.1765, -0.1404,  0.2282,\n",
       "         -0.0542,  0.1102,  0.0014, -0.1985,  0.1962, -0.2117, -0.0691, -0.1307,\n",
       "         -0.1567, -0.1736,  0.1976, -0.0537,  0.1084,  0.1384, -0.0073,  0.0760,\n",
       "         -0.1492,  0.1768, -0.1742, -0.0990,  0.1023, -0.0757,  0.1243,  0.0345,\n",
       "          0.0196, -0.0493, -0.1529, -0.0894,  0.0762, -0.0452, -0.0820,  0.0486,\n",
       "          0.1729, -0.1118, -0.1159,  0.0810],\n",
       "        [ 0.0032, -0.0919,  0.1164,  0.0527,  0.1432, -0.1486, -0.1175,  0.1286,\n",
       "          0.0613, -0.0245, -0.1130,  0.1419,  0.1681,  0.1341,  0.2249,  0.0545,\n",
       "          0.0249,  0.0610, -0.0381,  0.0876,  0.0609,  0.1867,  0.0712,  0.0136,\n",
       "          0.0741,  0.1311, -0.0151, -0.0892, -0.0268,  0.1502,  0.1232,  0.0542,\n",
       "          0.1872, -0.0382, -0.0530,  0.1616, -0.2207, -0.0912,  0.2035,  0.1018,\n",
       "         -0.1320,  0.0571, -0.0603,  0.0063, -0.0577, -0.0614,  0.1088,  0.0952,\n",
       "         -0.1768, -0.0254, -0.1039,  0.1225, -0.0181, -0.0100, -0.1551,  0.1370,\n",
       "          0.0608,  0.0086,  0.0137, -0.1532, -0.0118,  0.0358, -0.2217,  0.0539,\n",
       "         -0.0885,  0.1666,  0.1272, -0.0855, -0.0226, -0.2188,  0.0431,  0.1900,\n",
       "         -0.1356,  0.0391,  0.0876,  0.1484, -0.0003, -0.0928, -0.0387,  0.0531,\n",
       "         -0.1565,  0.0248, -0.1308,  0.1408, -0.0108,  0.0812,  0.0337, -0.1627,\n",
       "          0.0315,  0.0877,  0.0284,  0.1692,  0.1825,  0.1683,  0.0407, -0.1075,\n",
       "         -0.2062,  0.0097,  0.0191,  0.1040]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xavier initialization을 이용하여 각 layer의 weight 초기화\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Sequential을 이용하여 model 정의하기(쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n",
    "\n",
    "model= torch.nn.Sequential(linear1, bn1, relu, dropout,\n",
    "                          linear2, bn2, relu, dropout,\n",
    "                          linear3)\n",
    "#.to(device)? : CUDA로 GPU를 사용하지 않는 경우 끝에 .to(device)쓰지 않으면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost 계산을 위한 변수 설정\n",
    "train_total_batch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.505973935\n",
      "Epoch: 0002 cost = 0.374820977\n",
      "Epoch: 0003 cost = 0.330094486\n",
      "Epoch: 0004 cost = 0.305057883\n",
      "Epoch: 0005 cost = 0.294240087\n",
      "Epoch: 0006 cost = 0.284462243\n",
      "Epoch: 0007 cost = 0.271324962\n",
      "Epoch: 0008 cost = 0.275348186\n",
      "Epoch: 0009 cost = 0.255264670\n",
      "Epoch: 0010 cost = 0.255746990\n",
      "Epoch: 0011 cost = 0.254456669\n",
      "Epoch: 0012 cost = 0.255053639\n",
      "Epoch: 0013 cost = 0.253777713\n",
      "Epoch: 0014 cost = 0.242705539\n",
      "Epoch: 0015 cost = 0.243390620\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "    \n",
    "#train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
    "    \n",
    "    #reshape imput image into [batch_size by 784]\n",
    "    for X, Y in train_loader:\n",
    "        X=X.view(-1,28*28) #view를 사용하여 차원 변환\n",
    "        Y=Y\n",
    "        #train data set의 X,Y값 불러옴\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        #backpropagtion과 optimizer로 loss를 최적화함\n",
    "        \n",
    "        avg_cost += cost/train_total_batch\n",
    "    \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9121000170707703\n",
      "Label:  7\n",
      "Prediction:  7\n"
     ]
    }
   ],
   "source": [
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "#accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "\n",
    "model.eval() #set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "\n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    bn_acc = correct_prediction.float().mean()\n",
    "    print(\"Accuracy: \", bn_acc.item())\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1-2) 지금까지는 Layer의 수를 바꾸거나, Batch Normalization Layer를 추가하는 등 Layer에만 변화를 주며 모델의 성능을 향상 시켰습니다.\n",
    "\n",
    "이번 문제에서는 위에서 만든 모델에서 있던 Layer 들의 Hidden node 수를 증가 또는 감소 (ex: 200, 300, 50...) 시켰을 때, train set에서의 cost와 test set에서 Accuracy가 기존 결과와 비교하였을 때 어떻게 달라졌는지 비교해주시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 784 -> 100 -> 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = torch.nn.Linear(784, 100, bias=True)\n",
    "linear2 = torch.nn.Linear(100, 100, bias=True)\n",
    "linear3 = torch.nn.Linear(100, 10, bias=True)\n",
    "\n",
    "p=0.3 #p=사용하지 않을 비율 설정\n",
    "relu=torch.nn.ReLU()\n",
    "\n",
    "bn1 = torch.nn.BatchNorm1d(100)\n",
    "bn2 = torch.nn.BatchNorm1d(100)\n",
    "\n",
    "dropout= torch.nn.Dropout(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0141,  0.0575,  0.1267,  0.1323, -0.0520, -0.0983, -0.0328,  0.0146,\n",
       "         -0.0935, -0.1003, -0.1462, -0.1260,  0.1620, -0.1910, -0.1845, -0.0143,\n",
       "          0.0726, -0.0219,  0.0643, -0.0700,  0.0056,  0.1205,  0.1401, -0.1086,\n",
       "          0.2312,  0.0255,  0.0363,  0.2021, -0.0030,  0.0865, -0.0153,  0.2048,\n",
       "         -0.1891, -0.1599, -0.2078, -0.0810, -0.1818, -0.0546,  0.2137,  0.1072,\n",
       "         -0.2031, -0.0940, -0.2078, -0.2330,  0.0370,  0.2050, -0.2225, -0.2101,\n",
       "         -0.1164,  0.0313, -0.1084, -0.1578, -0.0453,  0.1554, -0.0503,  0.1085,\n",
       "          0.1284, -0.0671,  0.1871, -0.0860,  0.1700, -0.1653, -0.2013,  0.1971,\n",
       "          0.0529,  0.2136,  0.0149, -0.1469,  0.1622, -0.2162, -0.2207,  0.0086,\n",
       "         -0.1243, -0.1667, -0.1443, -0.0995,  0.0138, -0.0087,  0.0174,  0.2297,\n",
       "          0.1413,  0.0397,  0.1469,  0.1085, -0.1170, -0.2129, -0.0195, -0.0308,\n",
       "         -0.0232, -0.0171, -0.0926, -0.1880, -0.0160, -0.0438,  0.1291, -0.0794,\n",
       "         -0.0654,  0.0247, -0.2049, -0.1906],\n",
       "        [-0.0335,  0.1746,  0.0185,  0.1787,  0.0723,  0.2260,  0.2048,  0.1433,\n",
       "         -0.1234,  0.0900, -0.0405,  0.1863,  0.0293, -0.0629, -0.0174, -0.2205,\n",
       "          0.1371, -0.2279,  0.1467, -0.2209, -0.2210, -0.0779, -0.1779,  0.0865,\n",
       "         -0.0926, -0.1042, -0.1247, -0.1833, -0.0460,  0.1831, -0.2233, -0.1784,\n",
       "          0.1889, -0.1531, -0.2175,  0.0188,  0.0086, -0.1672,  0.2224,  0.1045,\n",
       "          0.0824, -0.1553,  0.0490,  0.1461,  0.0926,  0.2265, -0.0334,  0.0283,\n",
       "          0.0375,  0.1104,  0.1129,  0.1303,  0.1449, -0.0216, -0.0138,  0.0570,\n",
       "         -0.0879, -0.0249,  0.0802, -0.1542, -0.0395, -0.0636, -0.1449, -0.1872,\n",
       "         -0.0825,  0.0228,  0.1869, -0.1978, -0.0535,  0.0995, -0.0668, -0.1378,\n",
       "          0.0638,  0.1587, -0.1076,  0.1460, -0.2056,  0.1232, -0.0701,  0.0606,\n",
       "         -0.1549,  0.1519, -0.1560,  0.1883,  0.0372,  0.2037,  0.0101,  0.1087,\n",
       "          0.1044,  0.0015, -0.0632,  0.0362, -0.0942,  0.1549, -0.1488,  0.0209,\n",
       "          0.0344, -0.0095, -0.0543, -0.1404],\n",
       "        [ 0.0171, -0.1470, -0.0728, -0.0857, -0.0537,  0.0956,  0.2029, -0.1552,\n",
       "         -0.0244,  0.0088,  0.2004,  0.2117,  0.1917,  0.0276, -0.0382,  0.1113,\n",
       "          0.0487, -0.0175,  0.0320, -0.1560, -0.1031, -0.0645, -0.0419,  0.0833,\n",
       "         -0.1927, -0.1833, -0.2062, -0.1641, -0.1500, -0.0648,  0.1994,  0.1760,\n",
       "         -0.0987,  0.0732, -0.1611,  0.1357, -0.1646,  0.1506, -0.1319, -0.2133,\n",
       "         -0.2293,  0.1754, -0.2087, -0.0822, -0.1970,  0.2284, -0.0772, -0.0209,\n",
       "          0.0149, -0.0663, -0.1182,  0.1596,  0.0559,  0.0145, -0.0984,  0.0719,\n",
       "          0.0978, -0.0983,  0.0197,  0.0988, -0.2285, -0.1932, -0.2121,  0.0322,\n",
       "         -0.0556,  0.0976, -0.2124,  0.1329,  0.0678,  0.0277, -0.2154,  0.2115,\n",
       "         -0.0841,  0.2091, -0.1567, -0.0479,  0.1282,  0.0649, -0.0549,  0.0775,\n",
       "          0.2152, -0.1364, -0.0174,  0.0053, -0.2153,  0.1183,  0.1091, -0.0729,\n",
       "          0.1333, -0.0064,  0.0240,  0.1707, -0.0653,  0.0276,  0.0701,  0.1686,\n",
       "         -0.1043, -0.0146,  0.0862,  0.0675],\n",
       "        [ 0.0294,  0.0450,  0.0685,  0.0076, -0.1141, -0.1458, -0.1347, -0.0864,\n",
       "          0.0006, -0.1761, -0.1210,  0.1364,  0.0839,  0.2135, -0.2162,  0.1243,\n",
       "         -0.0952,  0.0166, -0.0679,  0.0386,  0.1386,  0.1717,  0.1007,  0.2333,\n",
       "         -0.1492,  0.2209, -0.0638,  0.0022, -0.2109,  0.0367, -0.1001,  0.0340,\n",
       "          0.2208, -0.1887,  0.0974,  0.1545,  0.1394,  0.1659, -0.2171, -0.0517,\n",
       "         -0.0454,  0.2242, -0.1545,  0.1985, -0.1147, -0.2082,  0.0028, -0.1600,\n",
       "          0.0112,  0.0674, -0.1717, -0.0936, -0.0364, -0.0845, -0.1372,  0.1004,\n",
       "          0.1331,  0.1971,  0.1597, -0.1320,  0.1874, -0.1132, -0.1042, -0.1284,\n",
       "          0.0144,  0.0547, -0.1462, -0.2246,  0.0250,  0.0009,  0.0458,  0.1518,\n",
       "          0.0532,  0.1928, -0.2274,  0.0499,  0.1102, -0.0802, -0.1172, -0.1530,\n",
       "          0.1470,  0.0859,  0.1070, -0.0821, -0.0523,  0.0541,  0.1888, -0.2248,\n",
       "         -0.0645, -0.0962, -0.1421,  0.1238,  0.1810, -0.0907,  0.1418,  0.1882,\n",
       "         -0.0161,  0.1309, -0.2305, -0.1062],\n",
       "        [ 0.1464, -0.2243, -0.1836,  0.1765, -0.1371,  0.2306, -0.1239, -0.0297,\n",
       "         -0.0957, -0.1808,  0.0452,  0.0927, -0.0562, -0.0576,  0.0738, -0.1501,\n",
       "          0.2100,  0.1893,  0.0519,  0.0207, -0.1399, -0.1112, -0.0233, -0.1392,\n",
       "          0.1052,  0.0198,  0.2000, -0.0862,  0.1617,  0.0275, -0.1439, -0.2212,\n",
       "          0.0240,  0.1872, -0.0908, -0.1750,  0.0755,  0.1397,  0.0247,  0.2320,\n",
       "          0.0879, -0.0535,  0.0916, -0.1493,  0.0250,  0.0027,  0.1122,  0.1212,\n",
       "          0.1501, -0.2170, -0.2112,  0.0250, -0.1924,  0.1541,  0.1305, -0.0018,\n",
       "         -0.0870, -0.0193,  0.0394, -0.1642, -0.0310,  0.1887,  0.1912, -0.0893,\n",
       "          0.1292,  0.1029,  0.1740,  0.2085,  0.1291,  0.0310,  0.0713,  0.0878,\n",
       "          0.1543,  0.1190, -0.1299,  0.1043, -0.2200,  0.0477,  0.1926, -0.1289,\n",
       "          0.1261, -0.0894,  0.1241, -0.1454, -0.1537, -0.1503, -0.0137,  0.0775,\n",
       "         -0.0372, -0.0883,  0.0562,  0.1396,  0.1722, -0.1432, -0.0419,  0.0926,\n",
       "          0.1087, -0.1479,  0.2278, -0.1178],\n",
       "        [ 0.0670, -0.0388, -0.2314, -0.2250, -0.1604,  0.0994, -0.1013, -0.0024,\n",
       "         -0.2246,  0.2094,  0.0798,  0.1773,  0.0916, -0.2004,  0.1350,  0.2171,\n",
       "         -0.1121,  0.2131,  0.0245,  0.1902,  0.2326, -0.0976, -0.0945, -0.1665,\n",
       "          0.2269,  0.0131,  0.0913, -0.0952,  0.2144, -0.0660, -0.0929,  0.1871,\n",
       "          0.1248,  0.0229, -0.2061,  0.1622, -0.0490,  0.0860,  0.0118, -0.0360,\n",
       "         -0.1615, -0.0478, -0.1251, -0.1715,  0.1495, -0.2004,  0.2190, -0.0295,\n",
       "         -0.0507, -0.1502,  0.0114, -0.0546, -0.1405,  0.1193,  0.2180,  0.1446,\n",
       "          0.1850, -0.0662, -0.0508,  0.1298,  0.0942, -0.0436,  0.0169,  0.1255,\n",
       "         -0.2330, -0.1424,  0.1559,  0.0290, -0.1318,  0.2324,  0.1141, -0.0847,\n",
       "          0.1834,  0.1910,  0.2089, -0.2086,  0.1053, -0.0291,  0.1797, -0.1020,\n",
       "         -0.0292, -0.0328,  0.1016,  0.2188,  0.1760,  0.1157,  0.2160, -0.1411,\n",
       "         -0.0648,  0.1898,  0.0425,  0.0514,  0.0396, -0.0346,  0.0530, -0.0646,\n",
       "         -0.0224, -0.1373,  0.1182,  0.0646],\n",
       "        [-0.0093,  0.2113,  0.2020, -0.0158, -0.2047,  0.0485, -0.1372,  0.2286,\n",
       "          0.0825, -0.0285, -0.2308, -0.0519,  0.1864,  0.0446,  0.1147, -0.0754,\n",
       "         -0.1906,  0.0521,  0.0007,  0.0045, -0.1459, -0.2080, -0.1344, -0.1609,\n",
       "          0.1082,  0.2288, -0.0747, -0.1167, -0.1315,  0.1450,  0.0502, -0.1579,\n",
       "          0.0347, -0.1220,  0.1568, -0.1532, -0.1234,  0.0276, -0.0888, -0.0693,\n",
       "         -0.1754,  0.2030,  0.0858,  0.0435, -0.0917, -0.0510,  0.0856, -0.1503,\n",
       "          0.0707, -0.0927, -0.1463, -0.0357,  0.1634, -0.0381, -0.0244, -0.0400,\n",
       "          0.0766, -0.0947, -0.0793, -0.1041, -0.0626,  0.0343,  0.1726, -0.2170,\n",
       "          0.1163,  0.1005,  0.0719,  0.2098, -0.1123, -0.0240, -0.1816, -0.1730,\n",
       "          0.1895,  0.1268, -0.0972,  0.0540,  0.1281,  0.1642, -0.1375, -0.0039,\n",
       "          0.1584,  0.0843,  0.0484, -0.1881,  0.1787,  0.2168,  0.0809,  0.0081,\n",
       "         -0.1215, -0.0908, -0.1366,  0.1563, -0.1390,  0.0932, -0.0939, -0.0221,\n",
       "         -0.0832,  0.0575,  0.1082,  0.0856],\n",
       "        [ 0.1210, -0.1239, -0.1117,  0.0236, -0.1309, -0.2005,  0.0072,  0.1192,\n",
       "         -0.0299,  0.0717, -0.1647, -0.2051,  0.1821,  0.0638, -0.0774, -0.0560,\n",
       "         -0.1975,  0.1116, -0.0034, -0.0862,  0.1635,  0.1803,  0.1770, -0.0054,\n",
       "         -0.0216, -0.1302, -0.1027, -0.0840, -0.1854,  0.0919, -0.0336,  0.1148,\n",
       "         -0.1721,  0.2157,  0.1617,  0.2267,  0.0212, -0.2002,  0.1649, -0.1961,\n",
       "          0.0675,  0.1102,  0.1279,  0.1875, -0.1786,  0.0209,  0.1694, -0.1255,\n",
       "          0.0657, -0.1507,  0.2081, -0.1689, -0.0912, -0.2186, -0.0425, -0.1471,\n",
       "         -0.1455,  0.0179, -0.0479, -0.0661, -0.2093,  0.0697, -0.0778,  0.2255,\n",
       "          0.0342,  0.1125,  0.0087, -0.2316,  0.1032,  0.0169, -0.0778, -0.1503,\n",
       "          0.1487,  0.1206,  0.0303,  0.1845, -0.0430, -0.0434,  0.1199,  0.1586,\n",
       "          0.0102,  0.1599,  0.0074,  0.0039,  0.2055, -0.1380, -0.2134, -0.1965,\n",
       "          0.0344, -0.0743, -0.0739,  0.1643, -0.1198,  0.2038,  0.1762, -0.0912,\n",
       "          0.1932,  0.0992,  0.1358,  0.0334],\n",
       "        [-0.0291,  0.1214, -0.0499,  0.0609, -0.1549, -0.1850, -0.2137,  0.2152,\n",
       "          0.1744, -0.0649, -0.1594,  0.1052, -0.2253,  0.1487,  0.0696, -0.0941,\n",
       "          0.1354,  0.0534, -0.2302,  0.1177,  0.0443, -0.1015, -0.0848,  0.0108,\n",
       "         -0.0464,  0.0467,  0.2125,  0.1298, -0.1534,  0.1247, -0.2123, -0.1537,\n",
       "         -0.0137, -0.0603, -0.1768, -0.1570,  0.1149, -0.0134, -0.0874, -0.0173,\n",
       "          0.1075, -0.2244,  0.1027, -0.0432,  0.2206,  0.0174,  0.1735,  0.0180,\n",
       "          0.0893, -0.1842, -0.2065,  0.1476, -0.0796, -0.1755, -0.0354, -0.1792,\n",
       "         -0.1600, -0.2199, -0.0414, -0.0111,  0.2252,  0.1788,  0.0256,  0.0005,\n",
       "          0.1461,  0.2031,  0.1394,  0.1566, -0.0447, -0.2121,  0.1962,  0.2139,\n",
       "          0.2099,  0.1161,  0.0359,  0.0877, -0.1877, -0.0165,  0.1646, -0.1688,\n",
       "          0.0092, -0.0515,  0.0158,  0.1515,  0.0514,  0.0681,  0.2330,  0.1779,\n",
       "          0.2301,  0.0325, -0.1626, -0.2140, -0.0055,  0.0374, -0.1055, -0.0940,\n",
       "         -0.0705,  0.0394,  0.0920,  0.0916],\n",
       "        [-0.1654, -0.1958, -0.0971, -0.0034,  0.1798, -0.2050, -0.1294,  0.0647,\n",
       "          0.1396,  0.0654,  0.0386, -0.2297, -0.0059, -0.1812, -0.2274,  0.1625,\n",
       "         -0.1821, -0.1930, -0.2276, -0.0880,  0.1630, -0.2011, -0.0969, -0.1610,\n",
       "         -0.2158, -0.0584, -0.1521,  0.0141, -0.1271, -0.0562, -0.0366, -0.0209,\n",
       "          0.2031, -0.0870, -0.0085, -0.1703,  0.1540,  0.2161,  0.2104, -0.0543,\n",
       "         -0.0830,  0.0829,  0.0212, -0.0866, -0.1473,  0.1165,  0.1196,  0.2150,\n",
       "          0.0179, -0.0460, -0.1251, -0.0395,  0.1538,  0.0904, -0.0721,  0.1870,\n",
       "         -0.1910,  0.0834, -0.2088, -0.1727, -0.1505, -0.2022, -0.1043,  0.0659,\n",
       "          0.1880,  0.1145, -0.0786,  0.0644,  0.1289, -0.2109, -0.0182,  0.1200,\n",
       "          0.1645, -0.0382,  0.0381,  0.1939, -0.1224, -0.1771, -0.2124,  0.1612,\n",
       "          0.0712, -0.1098,  0.0631, -0.0391, -0.1015,  0.1456,  0.1911, -0.0331,\n",
       "         -0.1048, -0.2219, -0.1365, -0.1799, -0.0573, -0.0201,  0.1919,  0.1215,\n",
       "         -0.1171,  0.0263,  0.0318, -0.1968]], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1= torch.nn.Sequential(linear1, bn1, relu, dropout,\n",
    "                          linear2, bn2, relu, dropout,\n",
    "                          linear3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.Adam(model1.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_batch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.509752035\n",
      "Epoch: 0002 cost = 0.376984417\n",
      "Epoch: 0003 cost = 0.325008869\n",
      "Epoch: 0004 cost = 0.304919869\n",
      "Epoch: 0005 cost = 0.303272873\n",
      "Epoch: 0006 cost = 0.295696229\n",
      "Epoch: 0007 cost = 0.277839959\n",
      "Epoch: 0008 cost = 0.277612209\n",
      "Epoch: 0009 cost = 0.260496169\n",
      "Epoch: 0010 cost = 0.253079027\n",
      "Epoch: 0011 cost = 0.257494658\n",
      "Epoch: 0012 cost = 0.246476203\n",
      "Epoch: 0013 cost = 0.244516090\n",
      "Epoch: 0014 cost = 0.232028931\n",
      "Epoch: 0015 cost = 0.234589934\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "    \n",
    "#train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
    "    \n",
    "    #reshape imput image into [batch_size by 784]\n",
    "    for X, Y in train_loader:\n",
    "        X=X.view(-1,28*28) #view를 사용하여 차원 변환\n",
    "        Y=Y\n",
    "        #train data set의 X,Y값 불러옴\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model1(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        #backpropagtion과 optimizer로 loss를 최적화함\n",
    "        \n",
    "        avg_cost += cost/train_total_batch\n",
    "    \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9451000094413757\n",
      "Label:  0\n",
      "Prediction:  0\n"
     ]
    }
   ],
   "source": [
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "#accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "\n",
    "model1.eval() #set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "\n",
    "    prediction = model1(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    bn_acc = correct_prediction.float().mean()\n",
    "    print(\"Accuracy: \", bn_acc.item())\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model1(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 784 -> 1000 -> 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear4 = torch.nn.Linear(784, 1000, bias=True)\n",
    "linear5 = torch.nn.Linear(1000, 1500, bias=True)\n",
    "linear6 = torch.nn.Linear(1500, 10, bias=True)\n",
    "\n",
    "p=0.3 #p=사용하지 않을 비율 설정\n",
    "relu=torch.nn.ReLU()\n",
    "\n",
    "bn3 = torch.nn.BatchNorm1d(1000)\n",
    "bn4 = torch.nn.BatchNorm1d(1500)\n",
    "\n",
    "dropout= torch.nn.Dropout(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0407, -0.0047, -0.0232,  ..., -0.0086, -0.0559,  0.0302],\n",
       "        [-0.0364, -0.0028, -0.0491,  ...,  0.0306, -0.0187, -0.0070],\n",
       "        [-0.0484, -0.0125,  0.0539,  ..., -0.0450,  0.0080, -0.0296],\n",
       "        ...,\n",
       "        [-0.0627, -0.0428,  0.0387,  ..., -0.0351, -0.0130, -0.0013],\n",
       "        [-0.0512,  0.0069,  0.0154,  ...,  0.0064,  0.0580,  0.0128],\n",
       "        [ 0.0284,  0.0277,  0.0373,  ...,  0.0465,  0.0297,  0.0496]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(linear4.weight)\n",
    "torch.nn.init.xavier_uniform_(linear5.weight)\n",
    "torch.nn.init.xavier_uniform_(linear6.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2= torch.nn.Sequential(linear4, bn3, relu, dropout,\n",
    "                          linear5, bn4, relu, dropout,\n",
    "                          linear6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_batch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.595859170\n",
      "Epoch: 0002 cost = 0.299860537\n",
      "Epoch: 0003 cost = 0.246342838\n",
      "Epoch: 0004 cost = 0.237264767\n",
      "Epoch: 0005 cost = 0.214813247\n",
      "Epoch: 0006 cost = 0.202371538\n",
      "Epoch: 0007 cost = 0.195428580\n",
      "Epoch: 0008 cost = 0.198250443\n",
      "Epoch: 0009 cost = 0.179849774\n",
      "Epoch: 0010 cost = 0.186480060\n",
      "Epoch: 0011 cost = 0.177620798\n",
      "Epoch: 0012 cost = 0.171020985\n",
      "Epoch: 0013 cost = 0.158515245\n",
      "Epoch: 0014 cost = 0.155551821\n",
      "Epoch: 0015 cost = 0.156162798\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "    \n",
    "#train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
    "    \n",
    "    #reshape imput image into [batch_size by 784]\n",
    "    for X, Y in train_loader:\n",
    "        X=X.view(-1,28*28) #view를 사용하여 차원 변환\n",
    "        Y=Y\n",
    "        #train data set의 X,Y값 불러옴\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model2(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        #backpropagtion과 optimizer로 loss를 최적화함\n",
    "        \n",
    "        avg_cost += cost/train_total_batch\n",
    "    \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6467999815940857\n",
      "Label:  7\n",
      "Prediction:  2\n"
     ]
    }
   ],
   "source": [
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "#accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "\n",
    "model2.eval() #set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "\n",
    "    prediction = model2(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    bn_acc = correct_prediction.float().mean()\n",
    "    print(\"Accuracy: \", bn_acc.item())\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model2(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 784 ->100 -> 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear7 = torch.nn.Linear(784, 100, bias=True)\n",
    "linear8 = torch.nn.Linear(100, 50, bias=True)\n",
    "linear9 = torch.nn.Linear(50, 10, bias=True)\n",
    "\n",
    "p=0.3 #p=사용하지 않을 비율 설정\n",
    "relu=torch.nn.ReLU()\n",
    "\n",
    "bn5 = torch.nn.BatchNorm1d(100)\n",
    "bn6 = torch.nn.BatchNorm1d(50)\n",
    "\n",
    "dropout= torch.nn.Dropout(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2558,  0.0322, -0.0335,  0.2357, -0.0295,  0.1702, -0.2362, -0.2294,\n",
       "          0.2278, -0.1203,  0.0405, -0.0365,  0.0576,  0.1175, -0.1440,  0.1539,\n",
       "         -0.2912,  0.1186, -0.2204, -0.1538,  0.2017, -0.0124, -0.2390,  0.1142,\n",
       "          0.2699, -0.1331, -0.1918, -0.2755, -0.2551, -0.1655, -0.0820,  0.1560,\n",
       "          0.0983,  0.2108, -0.1410, -0.1500, -0.1539, -0.3015, -0.1923,  0.0974,\n",
       "          0.0144, -0.1713, -0.2626, -0.3053,  0.0906, -0.0865,  0.0568,  0.1816,\n",
       "         -0.3102, -0.0425],\n",
       "        [-0.0609, -0.2609, -0.1620,  0.2394, -0.0776, -0.1102, -0.2819, -0.2259,\n",
       "         -0.1561,  0.2034,  0.3002, -0.2456,  0.2383,  0.0705,  0.3127, -0.0086,\n",
       "         -0.0297,  0.2253, -0.0819, -0.1649,  0.2037, -0.0192,  0.2881,  0.0704,\n",
       "         -0.1196, -0.2933,  0.2009, -0.1122, -0.3105, -0.2292, -0.1595, -0.1218,\n",
       "         -0.1431, -0.0089, -0.2752, -0.1150,  0.1539, -0.1333,  0.2920, -0.0164,\n",
       "          0.0296,  0.2287,  0.0672, -0.2799, -0.2850, -0.2932,  0.2086,  0.1172,\n",
       "          0.2013,  0.2941],\n",
       "        [ 0.2785,  0.3009,  0.2870,  0.1932, -0.0675, -0.1448, -0.0351, -0.2744,\n",
       "         -0.1972,  0.1087,  0.0215,  0.1796, -0.3127,  0.1626,  0.1393,  0.2492,\n",
       "          0.1009,  0.2663,  0.1297,  0.1638, -0.1296, -0.0492, -0.2713,  0.2100,\n",
       "          0.0812, -0.1342,  0.0446,  0.1202, -0.2219,  0.0112, -0.1898, -0.3046,\n",
       "          0.3135, -0.0535,  0.2568, -0.2929, -0.1588, -0.1466, -0.1463, -0.2144,\n",
       "          0.1228, -0.0607, -0.0400,  0.0445,  0.2247, -0.2796,  0.0556,  0.0190,\n",
       "         -0.2663,  0.2128],\n",
       "        [ 0.0044, -0.0473, -0.3066, -0.2113,  0.2974, -0.1165,  0.1554,  0.0560,\n",
       "          0.0660, -0.1063, -0.3039, -0.2788,  0.1273,  0.2080,  0.2731,  0.2261,\n",
       "         -0.1208,  0.1921,  0.1834,  0.2529, -0.2953, -0.1358,  0.0801,  0.2523,\n",
       "         -0.1648,  0.0022, -0.2011, -0.0406,  0.2138, -0.2522, -0.0548,  0.2950,\n",
       "          0.2755,  0.2386, -0.1121, -0.2562,  0.1494, -0.2405, -0.0768,  0.1056,\n",
       "          0.0670,  0.1454, -0.0043,  0.0090, -0.2813, -0.2641,  0.2946,  0.2753,\n",
       "         -0.1270, -0.1368],\n",
       "        [ 0.2511, -0.0862,  0.0269,  0.0284,  0.0994,  0.0861,  0.0208,  0.3106,\n",
       "         -0.3060,  0.1596, -0.2548, -0.1873,  0.3155,  0.1908, -0.1076, -0.1102,\n",
       "         -0.2760, -0.1453,  0.1719, -0.3036, -0.2474,  0.1577,  0.0942,  0.0286,\n",
       "         -0.2960,  0.0683,  0.1825,  0.0404, -0.3010, -0.2070, -0.0332, -0.0211,\n",
       "          0.3111, -0.2556,  0.0347,  0.1284,  0.1028, -0.1552,  0.1148,  0.1746,\n",
       "          0.2549,  0.0575,  0.2254,  0.0029, -0.1243, -0.1295, -0.1687, -0.2592,\n",
       "         -0.1760, -0.2379],\n",
       "        [ 0.1950, -0.2253, -0.1582,  0.1333,  0.0384,  0.0696,  0.3125,  0.2095,\n",
       "          0.0952,  0.1633, -0.1246, -0.0820,  0.0128,  0.0874,  0.0986, -0.0709,\n",
       "          0.1803, -0.0005, -0.2192,  0.0952,  0.2922, -0.2556, -0.1473, -0.1256,\n",
       "          0.1304, -0.0006,  0.0886, -0.0858,  0.2937,  0.1723, -0.2450, -0.1764,\n",
       "          0.0817,  0.0331,  0.0016,  0.0534, -0.1563,  0.2606,  0.0260,  0.1466,\n",
       "         -0.0051,  0.1108,  0.2364, -0.1582,  0.2465,  0.0166,  0.0830,  0.2014,\n",
       "          0.0536,  0.1247],\n",
       "        [ 0.1570,  0.0366,  0.2960, -0.1249,  0.2863,  0.0708,  0.0986, -0.0405,\n",
       "          0.2560, -0.0471, -0.0284, -0.1812, -0.2112, -0.3079,  0.0829, -0.0516,\n",
       "          0.2704,  0.0984,  0.2724,  0.1784,  0.1768, -0.0229,  0.0184,  0.2605,\n",
       "         -0.2933,  0.2514,  0.0270,  0.1543, -0.0577, -0.0912,  0.1409,  0.2744,\n",
       "          0.0866,  0.1328,  0.2257, -0.1267, -0.1602,  0.1546,  0.1632,  0.0062,\n",
       "         -0.2173, -0.0906,  0.2409,  0.0503,  0.1877, -0.2199, -0.0995,  0.0810,\n",
       "         -0.2423,  0.1511],\n",
       "        [-0.2762, -0.0720, -0.1658,  0.2223, -0.2856,  0.1685, -0.2045, -0.2982,\n",
       "         -0.1620, -0.0094,  0.1722,  0.3134,  0.2077,  0.0267,  0.0004,  0.0616,\n",
       "         -0.2775,  0.1289,  0.1334, -0.2800, -0.0984,  0.2184, -0.1098, -0.1277,\n",
       "         -0.1929,  0.1927, -0.0886,  0.0383,  0.2093,  0.2114, -0.2603,  0.3072,\n",
       "          0.0915, -0.0079,  0.0623, -0.2984, -0.2959,  0.0868,  0.2386,  0.2182,\n",
       "         -0.0037,  0.2696, -0.1044, -0.2231, -0.0291, -0.0905,  0.2859,  0.1696,\n",
       "         -0.0758,  0.2763],\n",
       "        [-0.2541, -0.2506, -0.3033,  0.0545,  0.0927,  0.1449, -0.1138, -0.1288,\n",
       "         -0.2025, -0.1052, -0.0807, -0.0093,  0.1476, -0.1536,  0.2671, -0.0969,\n",
       "          0.0177, -0.2716, -0.1226,  0.0220, -0.2098,  0.2180,  0.0426,  0.1659,\n",
       "          0.1378,  0.0451,  0.1156, -0.2883, -0.2844, -0.2565, -0.2104, -0.2799,\n",
       "         -0.2143, -0.0162,  0.2316,  0.1101,  0.2936,  0.1695, -0.1357, -0.0370,\n",
       "          0.0037, -0.1506,  0.0342,  0.1355,  0.1569, -0.1236,  0.0066,  0.1708,\n",
       "          0.2823,  0.2377],\n",
       "        [-0.0981,  0.2295, -0.2671,  0.2937, -0.2386, -0.0228, -0.0340, -0.0040,\n",
       "         -0.0667,  0.0977,  0.1673,  0.0357, -0.2327, -0.1716,  0.1934, -0.0280,\n",
       "          0.2760,  0.2171,  0.2519,  0.0013,  0.2843, -0.1932,  0.0794, -0.0050,\n",
       "          0.2123, -0.0722,  0.3105, -0.2453,  0.1756, -0.0263, -0.3048, -0.1087,\n",
       "          0.2110, -0.0441, -0.1068, -0.0958, -0.1940, -0.0831, -0.0464, -0.0267,\n",
       "          0.2531,  0.0952, -0.1150, -0.0305, -0.3053, -0.1491,  0.1402, -0.0230,\n",
       "         -0.2704,  0.3118]], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(linear7.weight)\n",
    "torch.nn.init.xavier_uniform_(linear8.weight)\n",
    "torch.nn.init.xavier_uniform_(linear9.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3= torch.nn.Sequential(linear7, bn5, relu, dropout,\n",
    "                          linear8, bn6, relu, dropout,\n",
    "                          linear9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.Adam(model3.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_batch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.501817763\n",
      "Epoch: 0002 cost = 0.364305645\n",
      "Epoch: 0003 cost = 0.326035678\n",
      "Epoch: 0004 cost = 0.309241623\n",
      "Epoch: 0005 cost = 0.295210183\n",
      "Epoch: 0006 cost = 0.286034703\n",
      "Epoch: 0007 cost = 0.276048630\n",
      "Epoch: 0008 cost = 0.273326099\n",
      "Epoch: 0009 cost = 0.264486879\n",
      "Epoch: 0010 cost = 0.260078013\n",
      "Epoch: 0011 cost = 0.251592517\n",
      "Epoch: 0012 cost = 0.250828534\n",
      "Epoch: 0013 cost = 0.243016526\n",
      "Epoch: 0014 cost = 0.240743205\n",
      "Epoch: 0015 cost = 0.242477134\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "    \n",
    "#train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
    "    \n",
    "    #reshape imput image into [batch_size by 784]\n",
    "    for X, Y in train_loader:\n",
    "        X=X.view(-1,28*28) #view를 사용하여 차원 변환\n",
    "        Y=Y\n",
    "        #train data set의 X,Y값 불러옴\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model3(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        #backpropagtion과 optimizer로 loss를 최적화함\n",
    "        \n",
    "        avg_cost += cost/train_total_batch\n",
    "    \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.944599986076355\n",
      "Label:  4\n",
      "Prediction:  4\n"
     ]
    }
   ],
   "source": [
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "#accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "\n",
    "model3.eval() #set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "\n",
    "    prediction = model3(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    bn_acc = correct_prediction.float().mean()\n",
    "    print(\"Accuracy: \", bn_acc.item())\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model3(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
