{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 1-1) 아래에 주어진 주석을 기반으로 코딩을 해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train과 test set으로 나누어 MNIST data 불러오기\n",
    "train = dsets.MNIST(root='MNIST_data/',\n",
    "                    train = True,\n",
    "                    transform = transforms.ToTensor(),\n",
    "                    download = True)\n",
    "test = dsets.MNIST(root='MNIST_data/',\n",
    "                   train = False,\n",
    "                   transform = transforms.ToTensor(),\n",
    "                   download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = True,\n",
    "                                          drop_last = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = True,\n",
    "                                          drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n",
    "# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n",
    "linear1 = torch.nn.Linear(784, 100, bias = True)\n",
    "linear2 = torch.nn.Linear(100, 100, bias = True)\n",
    "linear3 = torch.nn.Linear(100, 10, bias = True)\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p = 0.3)\n",
    "bn1 = torch.nn.BatchNorm1d(100)\n",
    "bn2 = torch.nn.BatchNorm1d(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1061,  0.1171,  0.1897, -0.2068, -0.1429, -0.1203, -0.0339, -0.1187,\n",
       "          0.0867, -0.1745, -0.0858, -0.1713,  0.1788,  0.1784,  0.0838, -0.0200,\n",
       "         -0.1273,  0.2078,  0.2287, -0.0328, -0.0244, -0.1916, -0.1104,  0.1045,\n",
       "          0.2265,  0.0765, -0.1907, -0.1358,  0.0962,  0.1731, -0.1784,  0.1765,\n",
       "          0.1268, -0.2318, -0.0584, -0.1518,  0.0475,  0.0087,  0.0236, -0.1465,\n",
       "          0.2123, -0.0823,  0.0968,  0.1805,  0.0704,  0.2256,  0.0521,  0.0855,\n",
       "         -0.0572,  0.1395, -0.1459, -0.0405, -0.0033, -0.0890,  0.0056, -0.0305,\n",
       "         -0.0803,  0.0035, -0.0334, -0.0532,  0.0193,  0.0151, -0.2333, -0.0639,\n",
       "          0.2015, -0.0596, -0.1510,  0.1850,  0.1448,  0.2179, -0.0722,  0.1960,\n",
       "          0.2247,  0.0017, -0.0077,  0.0363,  0.1166,  0.1629, -0.0850,  0.0184,\n",
       "          0.1257,  0.2102,  0.1442,  0.0036, -0.0635, -0.0218, -0.2082,  0.0890,\n",
       "         -0.0393, -0.0478, -0.1239,  0.2262,  0.1905, -0.2294,  0.2024,  0.1591,\n",
       "         -0.1293,  0.0673, -0.1542, -0.1014],\n",
       "        [-0.1628,  0.2265, -0.0055, -0.2295, -0.0253,  0.1265,  0.1697,  0.1290,\n",
       "          0.0922,  0.0421, -0.1129,  0.1364,  0.1073,  0.2152, -0.1022, -0.0440,\n",
       "         -0.2061,  0.1452, -0.0885, -0.0687, -0.0885, -0.0174, -0.2136,  0.1506,\n",
       "         -0.0993, -0.0172, -0.2023, -0.2288, -0.0336, -0.1122,  0.1420, -0.0473,\n",
       "         -0.0465, -0.0727,  0.0880,  0.1928, -0.1230, -0.1072,  0.1851,  0.1345,\n",
       "          0.1411,  0.1542, -0.0752,  0.0432, -0.0274,  0.0200, -0.0731, -0.0101,\n",
       "         -0.0738,  0.1637, -0.1186, -0.0601, -0.0707, -0.0071,  0.1993, -0.0833,\n",
       "         -0.0368,  0.0556, -0.1449, -0.0408,  0.1565,  0.1608, -0.1961, -0.2311,\n",
       "          0.1579,  0.0210, -0.1019, -0.0914,  0.1565,  0.0402,  0.0544, -0.0491,\n",
       "          0.1475,  0.1198,  0.1293,  0.0452,  0.2194,  0.0046,  0.0857, -0.1296,\n",
       "         -0.1506, -0.1587,  0.1524, -0.0340, -0.0389, -0.1304,  0.0319, -0.1684,\n",
       "          0.2108,  0.2133,  0.0241, -0.1903,  0.1735,  0.0107,  0.2064, -0.1823,\n",
       "          0.0671, -0.0980,  0.0972, -0.0525],\n",
       "        [-0.1570, -0.2305,  0.1049,  0.0816, -0.1138, -0.1271,  0.1822, -0.0870,\n",
       "          0.0270,  0.2278, -0.0159,  0.1163,  0.2137, -0.2077,  0.0314, -0.2018,\n",
       "         -0.1253, -0.0376,  0.1149,  0.0056, -0.1684, -0.1712, -0.1360,  0.1382,\n",
       "          0.2063,  0.1067,  0.2230,  0.2290,  0.0546,  0.1708,  0.0380, -0.0046,\n",
       "         -0.0191,  0.1158,  0.0154, -0.1479,  0.1384, -0.1290,  0.0500, -0.1039,\n",
       "         -0.0360, -0.0272,  0.1257, -0.1824,  0.1973,  0.0667,  0.2296, -0.0084,\n",
       "          0.1399, -0.0814,  0.2269,  0.0840,  0.1313,  0.1847, -0.2223, -0.0220,\n",
       "          0.0753,  0.1509, -0.1185, -0.0702,  0.0691,  0.0727, -0.1573, -0.0901,\n",
       "         -0.0430,  0.0734, -0.1855, -0.1319,  0.1747, -0.0705,  0.1183,  0.2163,\n",
       "          0.2234,  0.0095, -0.1031,  0.1374,  0.1547,  0.2067, -0.0866,  0.0015,\n",
       "         -0.2144, -0.0337,  0.1966, -0.2192,  0.0469,  0.1107, -0.1164,  0.2218,\n",
       "         -0.1971, -0.0566, -0.2247,  0.1878,  0.2260,  0.0707, -0.1455, -0.2154,\n",
       "          0.0963,  0.0413, -0.2206, -0.0978],\n",
       "        [ 0.1201, -0.2074, -0.0379, -0.0538,  0.0194,  0.1007,  0.1169,  0.0258,\n",
       "         -0.0079,  0.1014,  0.1628, -0.0427, -0.1372,  0.2156,  0.1685, -0.2089,\n",
       "          0.0412,  0.1744, -0.0880, -0.2221, -0.1704, -0.0094,  0.0303, -0.2300,\n",
       "          0.0690,  0.1039,  0.0021, -0.1548,  0.0511, -0.1220,  0.0912, -0.0641,\n",
       "          0.0297, -0.1913, -0.1312,  0.1648, -0.0871,  0.2249,  0.0885, -0.0996,\n",
       "          0.2087,  0.0816, -0.0548, -0.0516, -0.1801, -0.1391,  0.0447, -0.1075,\n",
       "         -0.2335, -0.0524, -0.0695, -0.1756, -0.0061,  0.1316,  0.0546,  0.1310,\n",
       "         -0.0811, -0.2230,  0.0898,  0.0468,  0.0431,  0.0594, -0.0009, -0.0603,\n",
       "         -0.1042,  0.1484,  0.1958,  0.0588, -0.1016, -0.1628, -0.0730, -0.2275,\n",
       "         -0.1820,  0.1798, -0.2226, -0.0957, -0.1080, -0.2155,  0.1663, -0.1696,\n",
       "          0.0025,  0.0172,  0.0500,  0.1019, -0.1136,  0.1774, -0.0956,  0.0835,\n",
       "         -0.1833, -0.0095, -0.0329,  0.0374, -0.0945,  0.1562, -0.0768,  0.0668,\n",
       "         -0.0395,  0.1882,  0.2028, -0.0730],\n",
       "        [ 0.0639,  0.0646, -0.0829, -0.0082, -0.0599, -0.1570,  0.0158, -0.0820,\n",
       "          0.0387, -0.1243,  0.0074, -0.0359, -0.0095, -0.2299, -0.2070,  0.0085,\n",
       "          0.0672,  0.1338,  0.1164,  0.1790, -0.1328, -0.0388, -0.1130,  0.2019,\n",
       "          0.2157, -0.1262, -0.0715, -0.1363,  0.1814, -0.1264, -0.0394, -0.1131,\n",
       "          0.0008,  0.1287, -0.0222,  0.0289,  0.0438, -0.0413, -0.0166,  0.2050,\n",
       "          0.1821, -0.1702,  0.1228,  0.0317, -0.0491, -0.0984,  0.0323,  0.1925,\n",
       "          0.0005,  0.0922, -0.1161,  0.1751, -0.1405,  0.1616, -0.1533,  0.2278,\n",
       "          0.0776, -0.0061, -0.0563, -0.0616,  0.1132, -0.1104, -0.2173, -0.0638,\n",
       "         -0.0647, -0.1715,  0.1168,  0.1703, -0.2164,  0.0133,  0.2119,  0.1494,\n",
       "          0.1177, -0.2226,  0.1888, -0.1427, -0.0120,  0.0965, -0.2034, -0.1704,\n",
       "          0.1367, -0.1860, -0.1283, -0.1328,  0.1695,  0.0463,  0.2253, -0.0282,\n",
       "         -0.0089,  0.0386,  0.1070, -0.1114,  0.1835, -0.0568, -0.2243, -0.1722,\n",
       "         -0.1551, -0.1525, -0.2002,  0.1141],\n",
       "        [-0.1117,  0.0576, -0.0088, -0.1790, -0.1259, -0.0988,  0.2198,  0.0902,\n",
       "         -0.0389, -0.1212,  0.0820,  0.0701, -0.1413,  0.0848,  0.0928, -0.1288,\n",
       "          0.0120, -0.0393, -0.0604,  0.0951,  0.1169,  0.2166,  0.2038, -0.1164,\n",
       "          0.2159,  0.0880, -0.1464,  0.1000,  0.0719,  0.1022,  0.0984,  0.0475,\n",
       "         -0.0408,  0.0261, -0.0137,  0.0605, -0.1422, -0.0568, -0.1050, -0.1781,\n",
       "          0.0121, -0.1971, -0.1758,  0.1925, -0.0431,  0.1035, -0.0570, -0.2295,\n",
       "          0.0188, -0.2123,  0.1460, -0.0068, -0.0133,  0.1292, -0.1158, -0.1238,\n",
       "         -0.1696, -0.0598, -0.2323, -0.1897,  0.0578,  0.2319, -0.1073,  0.0629,\n",
       "          0.1618, -0.1301,  0.0211, -0.2093, -0.2248, -0.1884, -0.0468, -0.2306,\n",
       "         -0.1082, -0.1728,  0.2218, -0.1741, -0.0793,  0.2116, -0.0499,  0.1168,\n",
       "          0.0586, -0.2262, -0.1171, -0.1883, -0.0456,  0.2087,  0.1724, -0.2044,\n",
       "          0.1193, -0.0989,  0.1240, -0.0786,  0.0227,  0.1381, -0.2044,  0.1328,\n",
       "          0.0490,  0.1703, -0.1913,  0.1467],\n",
       "        [ 0.0510,  0.0232, -0.1127, -0.1644,  0.1220, -0.1530, -0.0979, -0.1394,\n",
       "          0.1830,  0.2301, -0.1536,  0.0089, -0.1635, -0.0608,  0.1899,  0.0628,\n",
       "         -0.0788, -0.0867, -0.0275,  0.2308, -0.0402,  0.1499,  0.2215,  0.0652,\n",
       "         -0.1859,  0.1944, -0.1927, -0.2197, -0.2302,  0.1550,  0.0685, -0.0092,\n",
       "         -0.1902,  0.0164,  0.0512,  0.2287,  0.1812, -0.0771,  0.2147, -0.1286,\n",
       "          0.2262,  0.1840, -0.1703, -0.1435, -0.2259,  0.1998, -0.1843, -0.1215,\n",
       "         -0.1458,  0.2132, -0.1666,  0.0337, -0.0940,  0.0081, -0.1150,  0.1799,\n",
       "          0.0607,  0.0859, -0.1961, -0.0298, -0.1907, -0.1385,  0.1486, -0.1968,\n",
       "          0.1561,  0.0990, -0.1834,  0.0875,  0.0594,  0.1594, -0.0643, -0.0846,\n",
       "          0.0703, -0.1156, -0.0066,  0.1103,  0.1919,  0.0488, -0.1796,  0.1591,\n",
       "         -0.0624, -0.0368, -0.0994, -0.1328,  0.1344,  0.0938, -0.1217, -0.1376,\n",
       "          0.1529, -0.0741,  0.1158,  0.1565, -0.2328,  0.0558,  0.1763, -0.0543,\n",
       "         -0.1812,  0.1809, -0.0896,  0.2030],\n",
       "        [ 0.0585, -0.2089, -0.1449,  0.0308,  0.1115, -0.2127,  0.0986,  0.1177,\n",
       "         -0.0307, -0.1378, -0.1355,  0.0848, -0.2002, -0.0109, -0.1103, -0.2224,\n",
       "          0.1268, -0.0985,  0.1205, -0.1779,  0.2039,  0.1774,  0.1069,  0.0327,\n",
       "          0.0044,  0.2076, -0.0026, -0.1429, -0.1897, -0.1176, -0.0056, -0.1281,\n",
       "         -0.0403,  0.2318, -0.0974, -0.0897, -0.1901, -0.0166,  0.1169, -0.1804,\n",
       "          0.0752, -0.1990, -0.1248,  0.1187,  0.0697,  0.1550,  0.0335, -0.0232,\n",
       "          0.0876, -0.0218,  0.0216, -0.1733,  0.1891, -0.0700, -0.2270,  0.0224,\n",
       "         -0.2031, -0.1189,  0.0693, -0.1577,  0.1504,  0.0122,  0.2085,  0.1135,\n",
       "         -0.1859, -0.1981, -0.1427, -0.0331,  0.1109, -0.1244,  0.2284,  0.2098,\n",
       "         -0.1730,  0.1038,  0.0567,  0.1593,  0.0411, -0.2280, -0.0965, -0.1272,\n",
       "          0.0216, -0.0861,  0.0193,  0.1607, -0.2180, -0.1613,  0.0114,  0.0685,\n",
       "         -0.1370, -0.1388,  0.0688,  0.1647,  0.0383, -0.1894,  0.1489, -0.0325,\n",
       "         -0.1397, -0.0203,  0.0427,  0.1211],\n",
       "        [ 0.1739,  0.0205, -0.0735,  0.0469, -0.1410, -0.0568, -0.1655, -0.0064,\n",
       "         -0.1620, -0.0822,  0.1825,  0.0039,  0.1992,  0.1289,  0.0755,  0.1356,\n",
       "         -0.0061,  0.0687,  0.0459,  0.2209, -0.0516,  0.0376,  0.1392, -0.0264,\n",
       "          0.1434,  0.1389, -0.2021,  0.0276,  0.0514, -0.1295,  0.1989, -0.2109,\n",
       "          0.1639,  0.1385,  0.1733, -0.1863,  0.1656, -0.1295,  0.1026, -0.0237,\n",
       "          0.1630, -0.0303, -0.2068,  0.2298,  0.2266,  0.1362, -0.0609, -0.0786,\n",
       "         -0.0624, -0.2298,  0.0693, -0.0870, -0.0123, -0.2331,  0.2126, -0.0708,\n",
       "          0.0868,  0.0910, -0.1209,  0.2105, -0.1751, -0.1221, -0.0416, -0.0710,\n",
       "          0.1580, -0.1127,  0.1274,  0.0735, -0.0867,  0.0207,  0.1704, -0.0789,\n",
       "          0.1484,  0.1169, -0.2273,  0.0612,  0.0950, -0.1116, -0.0621,  0.0164,\n",
       "         -0.1777, -0.0317, -0.0812,  0.0472,  0.0759,  0.2209, -0.1794,  0.0895,\n",
       "         -0.1886,  0.0656,  0.1310, -0.0430, -0.1770,  0.0239,  0.0688, -0.2255,\n",
       "         -0.2312,  0.1615, -0.1408, -0.0348],\n",
       "        [ 0.0181,  0.0243,  0.0658, -0.0061,  0.0131,  0.1086,  0.1426, -0.1593,\n",
       "          0.0658, -0.0835, -0.0469, -0.2327,  0.0843, -0.1473,  0.1391, -0.1667,\n",
       "         -0.0623, -0.0881,  0.1515,  0.1163,  0.0550,  0.1332,  0.1936,  0.0836,\n",
       "         -0.1499,  0.0186,  0.1199,  0.0569,  0.0705, -0.1032,  0.0815,  0.1385,\n",
       "          0.1413, -0.0229,  0.2117,  0.1441,  0.2085,  0.1793,  0.0213, -0.1127,\n",
       "         -0.0709,  0.0601, -0.0779,  0.2000,  0.1741,  0.0279, -0.0273, -0.0181,\n",
       "         -0.1631, -0.2001, -0.1146,  0.1326, -0.1823, -0.0087, -0.0427,  0.2162,\n",
       "          0.0948,  0.0743, -0.2312,  0.0206,  0.0070, -0.1738,  0.1909,  0.0710,\n",
       "          0.1532, -0.0817,  0.1546, -0.1478, -0.0956,  0.0994,  0.1103, -0.1066,\n",
       "          0.2230,  0.2187,  0.1116,  0.2109,  0.0540,  0.0283, -0.2032,  0.2073,\n",
       "          0.0442, -0.0131, -0.1973, -0.1598,  0.1952, -0.1153, -0.1538, -0.0072,\n",
       "         -0.0149,  0.0077, -0.0779,  0.1893, -0.0496, -0.1885, -0.1721, -0.2285,\n",
       "         -0.1176,  0.0574,  0.2143,  0.1281]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xavier initialization을 이용하여 각 layer의 weight 초기화\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Sequential을 이용하여 model 정의하기(쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n",
    "model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n",
    "                            linear2, bn2, relu, dropout,\n",
    "                            linear3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "criterion = torch.nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.496009558\n",
      "Epoch: 0002 cost = 0.365117162\n",
      "Epoch: 0003 cost = 0.321423441\n",
      "Epoch: 0004 cost = 0.304662853\n",
      "Epoch: 0005 cost = 0.292454153\n",
      "Epoch: 0006 cost = 0.289761156\n",
      "Epoch: 0007 cost = 0.275276959\n",
      "Epoch: 0008 cost = 0.268245369\n",
      "Epoch: 0009 cost = 0.274177849\n",
      "Epoch: 0010 cost = 0.257648170\n",
      "Epoch: 0011 cost = 0.241259992\n",
      "Epoch: 0012 cost = 0.241195634\n",
      "Epoch: 0013 cost = 0.239378348\n",
      "Epoch: 0014 cost = 0.236419350\n",
      "Epoch: 0015 cost = 0.227829695\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "#cost 계산을 위한 변수 설정\n",
    "train_total_batch = len(train_loader)\n",
    "\n",
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
    "for epoch in range(training_epochs):\n",
    "    model.train()\n",
    "    avg_cost = 0\n",
    "    \n",
    "    #train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 28 * 28)\n",
    "        Y_hat = model(X)\n",
    "        cost = criterion(Y_hat, Y)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost / train_total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9700000286102295\n",
      "Label:  3\n",
      "Prediction:  3\n"
     ]
    }
   ],
   "source": [
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "#accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    accuracy = 0\n",
    "    \n",
    "    for i, (X, Y) in enumerate(test_loader):\n",
    "        X_test = X.view(-1, 28 * 28)\n",
    "        Y_test = Y\n",
    "        prediction = model(X_test)\n",
    "        correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "        accuracy = correct_prediction.float().mean()\n",
    "       \n",
    "    print(\"Accuracy: \", accuracy.item())\n",
    "    \n",
    "    ## Test set에서 random으로 data를 뽑아 Label과 Preditcion을 비교하는 코드\n",
    "    r = random.randint(0, len(test_loader)-1)\n",
    "    X_single_data = X[r:r+1].view(-1, 28*28)\n",
    "    Y_single_data = Y[r:r+1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 1-2) Layer들의 Hidden node 수를 증가 또는 감소시켜보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증가\n",
    "linear1 = torch.nn.Linear(784, 200, bias = True)\n",
    "linear2 = torch.nn.Linear(200, 150, bias = True)\n",
    "linear3 = torch.nn.Linear(150, 10, bias = True)\n",
    "\n",
    "bn1 = torch.nn.BatchNorm1d(200)\n",
    "bn2 = torch.nn.BatchNorm1d(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1409, -0.1490, -0.0132,  ..., -0.1501, -0.0596, -0.1913],\n",
       "        [-0.1624,  0.0009,  0.1408,  ..., -0.1339, -0.0253,  0.1415],\n",
       "        [ 0.1044, -0.1092,  0.1175,  ...,  0.0904,  0.0186, -0.0249],\n",
       "        ...,\n",
       "        [-0.1252, -0.0718, -0.0555,  ...,  0.1434,  0.1532,  0.0019],\n",
       "        [-0.0872, -0.1050, -0.0568,  ..., -0.1874,  0.0539, -0.0934],\n",
       "        [ 0.1372, -0.0544,  0.0367,  ..., -0.0490, -0.0068,  0.1394]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.nn.Sequential(linear1, bn1, relu, dropout,\n",
    "                            linear2, bn2, relu, dropout,\n",
    "                            linear3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "\n",
    "#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.459155381\n",
      "Epoch: 0002 cost = 0.327410132\n",
      "Epoch: 0003 cost = 0.289819002\n",
      "Epoch: 0004 cost = 0.273510128\n",
      "Epoch: 0005 cost = 0.250397474\n",
      "Epoch: 0006 cost = 0.256609350\n",
      "Epoch: 0007 cost = 0.238115966\n",
      "Epoch: 0008 cost = 0.233371884\n",
      "Epoch: 0009 cost = 0.225198969\n",
      "Epoch: 0010 cost = 0.215849593\n",
      "Epoch: 0011 cost = 0.220183015\n",
      "Epoch: 0012 cost = 0.207451299\n",
      "Epoch: 0013 cost = 0.206049830\n",
      "Epoch: 0014 cost = 0.204814732\n",
      "Epoch: 0015 cost = 0.190526366\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "#cost 계산을 위한 변수 설정\n",
    "train_total_batch = len(train_loader)\n",
    "\n",
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
    "for epoch in range(training_epochs):\n",
    "    model2.train()\n",
    "    avg_cost = 0\n",
    "    \n",
    "    #train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 28 * 28)\n",
    "        Y_hat = model2(X)\n",
    "        cost = criterion(Y_hat, Y)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost / train_total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "Label:  6\n",
      "Prediction:  6\n"
     ]
    }
   ],
   "source": [
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "#accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "with torch.no_grad():\n",
    "    model2.eval()\n",
    "    accuracy = 0\n",
    "    \n",
    "    for i, (X, Y) in enumerate(test_loader):\n",
    "        X_test = X.view(-1, 28 * 28)\n",
    "        Y_test = Y\n",
    "        prediction = model2(X_test)\n",
    "        correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "        accuracy = correct_prediction.float().mean()\n",
    "       \n",
    "    print(\"Accuracy: \", accuracy.item())\n",
    "    \n",
    "    ## Test set에서 random으로 data를 뽑아 Label과 Preditcion을 비교하는 코드\n",
    "    r = random.randint(0, len(test_loader)-1)\n",
    "    X_single_data = X[r:r+1].view(-1, 28*28)\n",
    "    Y_single_data = Y[r:r+1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model2(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss도 많이 줄어들고, Accuracy가 1이 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감소\n",
    "linear1 = torch.nn.Linear(784, 70, bias = True)\n",
    "linear2 = torch.nn.Linear(70, 50, bias = True)\n",
    "linear3 = torch.nn.Linear(50, 10, bias = True)\n",
    "bn1 = torch.nn.BatchNorm1d(70)\n",
    "bn2 = torch.nn.BatchNorm1d(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0466, -0.0164, -0.1147,  0.3002,  0.0831, -0.2412, -0.0331,  0.0613,\n",
       "         -0.2212,  0.2542,  0.0849, -0.2804,  0.1685, -0.0052, -0.0893,  0.0243,\n",
       "         -0.1935, -0.0522, -0.1118,  0.1373,  0.2536, -0.0136, -0.2451,  0.1960,\n",
       "         -0.0241,  0.2051,  0.0365, -0.1055,  0.2960,  0.1667,  0.1083,  0.2020,\n",
       "          0.1318, -0.2663,  0.3076, -0.2342,  0.2477,  0.0196, -0.2697, -0.1849,\n",
       "         -0.0641, -0.1532,  0.3008, -0.0925,  0.2623, -0.3114,  0.1231, -0.2500,\n",
       "         -0.1580,  0.1566],\n",
       "        [-0.0502, -0.3016, -0.1494,  0.0086, -0.2670,  0.0922,  0.0430, -0.0768,\n",
       "         -0.0763, -0.1499, -0.3005,  0.2044, -0.3063, -0.1917,  0.2693,  0.0877,\n",
       "          0.1267,  0.1547,  0.1827, -0.1984, -0.2897,  0.1113, -0.1584, -0.2167,\n",
       "         -0.2574,  0.2825, -0.2097, -0.2305, -0.0680,  0.2803, -0.0802, -0.0875,\n",
       "          0.2149, -0.3101, -0.2694,  0.1776, -0.0675,  0.2912,  0.1428,  0.2651,\n",
       "          0.1582,  0.1508, -0.0995, -0.2166, -0.2885,  0.0357,  0.3044, -0.2390,\n",
       "          0.2590, -0.2839],\n",
       "        [ 0.0978, -0.0457,  0.1482,  0.0040,  0.2415,  0.1931,  0.1087,  0.1640,\n",
       "         -0.3055, -0.1512, -0.0880,  0.2591,  0.1406,  0.0805, -0.1027,  0.1253,\n",
       "          0.1314, -0.1213,  0.2683, -0.2586, -0.0942, -0.1150,  0.0881, -0.0489,\n",
       "          0.2745,  0.1806, -0.2146, -0.1982, -0.2253, -0.1365, -0.1806,  0.1224,\n",
       "         -0.2217,  0.0087,  0.0237, -0.2330, -0.3050,  0.0584,  0.1958, -0.2756,\n",
       "          0.1071,  0.1402,  0.0681,  0.0993,  0.1071,  0.1686,  0.0770, -0.1428,\n",
       "          0.1897, -0.0877],\n",
       "        [-0.0231, -0.0850, -0.2837,  0.2999, -0.1652,  0.0081, -0.3053,  0.2791,\n",
       "         -0.0495,  0.0717, -0.0035, -0.0030,  0.0888,  0.3023,  0.1973,  0.0974,\n",
       "         -0.2256,  0.0757,  0.3059,  0.0491, -0.2358,  0.1505, -0.1496,  0.0818,\n",
       "          0.1379,  0.2988, -0.0683,  0.1913,  0.1484, -0.1563, -0.0758,  0.1440,\n",
       "          0.1849,  0.0837, -0.2920,  0.0797, -0.0540, -0.1658, -0.0144,  0.1168,\n",
       "         -0.1779, -0.0349,  0.2011,  0.0484, -0.0202, -0.1772,  0.2583, -0.2532,\n",
       "         -0.0346,  0.0452],\n",
       "        [-0.1104,  0.0281, -0.2048, -0.2855,  0.1841, -0.2105, -0.1773, -0.1516,\n",
       "         -0.2213,  0.0130,  0.1803, -0.2214,  0.1512,  0.0080, -0.1060,  0.2551,\n",
       "          0.2962, -0.2920, -0.3136,  0.3013, -0.2505,  0.0415, -0.1251,  0.1019,\n",
       "         -0.2408, -0.0367, -0.2948, -0.2755,  0.2445,  0.0587,  0.0999,  0.2746,\n",
       "          0.0344, -0.0760,  0.1591, -0.2977, -0.3158, -0.0860,  0.2034, -0.1687,\n",
       "         -0.0877, -0.2672,  0.2885,  0.0261,  0.0973, -0.2679,  0.0019,  0.1569,\n",
       "          0.2300,  0.0408],\n",
       "        [-0.1490,  0.0446,  0.1093, -0.1055,  0.0392,  0.1011,  0.1929,  0.2965,\n",
       "         -0.2828, -0.0788,  0.2708,  0.3105,  0.1600, -0.2239,  0.0983,  0.0106,\n",
       "          0.2981, -0.1452, -0.1941, -0.0987, -0.0782,  0.2207, -0.0853, -0.0732,\n",
       "         -0.1879,  0.3016, -0.0117,  0.0627, -0.0938,  0.1190, -0.3028, -0.2938,\n",
       "          0.0188,  0.2269, -0.2469,  0.2274, -0.0744, -0.0819,  0.1361,  0.1423,\n",
       "          0.1615,  0.0689,  0.0278,  0.1521,  0.0771,  0.1165, -0.2281, -0.3016,\n",
       "         -0.0546, -0.2428],\n",
       "        [-0.1214,  0.1630,  0.1300, -0.0437,  0.2929,  0.2760, -0.2124, -0.1050,\n",
       "          0.1994,  0.0881, -0.2362, -0.2146,  0.2664, -0.0030,  0.0932, -0.2404,\n",
       "         -0.0169,  0.3079, -0.2849,  0.0097,  0.1636, -0.2901,  0.2544,  0.1842,\n",
       "         -0.0300, -0.0670, -0.1775, -0.0697,  0.1057,  0.2884, -0.0227,  0.2013,\n",
       "         -0.3144,  0.2799, -0.2797,  0.0495, -0.0037, -0.1112, -0.0749,  0.0039,\n",
       "         -0.1632,  0.2453,  0.1823, -0.1890,  0.2179,  0.0210,  0.1439, -0.1792,\n",
       "         -0.0947,  0.0034],\n",
       "        [ 0.1023,  0.0822,  0.2531, -0.0411, -0.1304, -0.3027, -0.1405, -0.1196,\n",
       "         -0.3121, -0.3004,  0.2655,  0.0590,  0.1018,  0.0975,  0.1268, -0.1656,\n",
       "         -0.1671, -0.0372,  0.1598,  0.0752,  0.1613,  0.1722, -0.3043, -0.0793,\n",
       "          0.1922,  0.0959,  0.1695,  0.0785,  0.2293,  0.2640, -0.0282,  0.0623,\n",
       "         -0.1078, -0.2225,  0.1143, -0.1225,  0.1683,  0.0133,  0.3107, -0.1415,\n",
       "          0.1857, -0.0140, -0.3029,  0.2099, -0.0514,  0.1677, -0.1298,  0.0307,\n",
       "         -0.1888, -0.2376],\n",
       "        [ 0.0302, -0.1036,  0.0787,  0.0166,  0.1497,  0.1896, -0.1802,  0.0004,\n",
       "          0.2398, -0.0563,  0.2797,  0.2316, -0.2181, -0.1434,  0.1567, -0.1776,\n",
       "         -0.2112, -0.1796, -0.0347, -0.0917, -0.2284,  0.0542, -0.3073, -0.0682,\n",
       "         -0.2462,  0.1315,  0.1404,  0.0431, -0.2291,  0.1145, -0.0095, -0.0286,\n",
       "          0.2692, -0.2971,  0.1493, -0.2894,  0.1793,  0.0211, -0.1097,  0.2820,\n",
       "          0.0922,  0.1372,  0.2224, -0.2167, -0.1685, -0.2670,  0.1841, -0.1898,\n",
       "         -0.1993,  0.1180],\n",
       "        [-0.2249, -0.1462, -0.0652,  0.1689,  0.2361,  0.2284,  0.1672,  0.2893,\n",
       "         -0.1799, -0.2688,  0.1247, -0.2149,  0.2329,  0.1369,  0.1743, -0.2306,\n",
       "         -0.0231,  0.0961, -0.0761, -0.2324,  0.0165, -0.2747,  0.1159,  0.1627,\n",
       "          0.2934, -0.1674, -0.1204, -0.0639, -0.0941, -0.0245,  0.0138,  0.0364,\n",
       "          0.0683, -0.2401, -0.0041,  0.0275,  0.0317, -0.1220, -0.1263, -0.0395,\n",
       "         -0.2887, -0.0854, -0.0298, -0.0827,  0.2871,  0.2951,  0.0948,  0.1646,\n",
       "          0.2806,  0.2849]], requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = torch.nn.Sequential(linear1, bn1, relu, dropout,\n",
    "                            linear2, bn2, relu, dropout,\n",
    "                            linear3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "\n",
    "#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.544624627\n",
      "Epoch: 0002 cost = 0.409764826\n",
      "Epoch: 0003 cost = 0.375281453\n",
      "Epoch: 0004 cost = 0.341665894\n",
      "Epoch: 0005 cost = 0.325729996\n",
      "Epoch: 0006 cost = 0.319383621\n",
      "Epoch: 0007 cost = 0.320487916\n",
      "Epoch: 0008 cost = 0.304233700\n",
      "Epoch: 0009 cost = 0.304083586\n",
      "Epoch: 0010 cost = 0.298640341\n",
      "Epoch: 0011 cost = 0.285165370\n",
      "Epoch: 0012 cost = 0.292602718\n",
      "Epoch: 0013 cost = 0.280387670\n",
      "Epoch: 0014 cost = 0.281007975\n",
      "Epoch: 0015 cost = 0.286925703\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "#cost 계산을 위한 변수 설정\n",
    "train_total_batch = len(train_loader)\n",
    "\n",
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
    "for epoch in range(training_epochs):\n",
    "    model3.train()\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for X,Y in train_loader:\n",
    "        X = X.view(-1, 28*28)\n",
    "        Y = Y\n",
    "        Y_hat = model3(X)\n",
    "        cost = criterion(Y_hat, Y)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost / train_total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9700000286102295\n",
      "Label:  0\n",
      "Prediction:  0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model3.eval()\n",
    "    accuracy = 0\n",
    "    \n",
    "    for i, (X,Y) in enumerate(test_loader):\n",
    "        X_test = X.view(-1, 28 * 28)\n",
    "        Y_test = Y\n",
    "        prediction = model3(X_test)\n",
    "        correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "        accuracy = correct_prediction.float().mean()\n",
    "       \n",
    "    print(\"Accuracy: \", accuracy.item())\n",
    "    \n",
    "    ## Test set에서 random으로 data를 뽑아 Label과 Preditcion을 비교하는 코드\n",
    "    r = random.randint(0, len(test_loader)-1)\n",
    "    X_single_data = X[r:r+1].view(-1, 28*28)\n",
    "    Y_single_data = Y[r:r+1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model3(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것 역시 loss와 accuracy가 좋았음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
