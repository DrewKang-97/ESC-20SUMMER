{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN08rQvPQkSXZkxetWkYwLA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dxUx53QciTq",
        "colab_type": "text"
      },
      "source": [
        "Q 1-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOFS47kLab9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqOtCE0mapGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.1\n",
        "training_epochs = 15\n",
        "batch_size = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSGnqvl3asUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDSMppGdawoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugFgZCUgazRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linear1 = torch.nn.Linear(784, 100, bias=True)\n",
        "linear2 = torch.nn.Linear(100, 100, bias=True)\n",
        "linear3 = torch.nn.Linear(100, 10, bias=True)\n",
        "relu = torch.nn.ReLU()\n",
        "dropout = torch.nn.Dropout(p=0.3)\n",
        "bn1 = torch.nn.BatchNorm1d(100)\n",
        "bn2 = torch.nn.BatchNorm1d(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3br7QDAa05g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.nn.init.xavier_uniform_(linear1.weight)\n",
        "torch.nn.init.xavier_uniform_(linear2.weight)\n",
        "torch.nn.init.xavier_uniform_(linear3.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGx5lqZ2a3Qz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n",
        "                            linear2, bn2, relu, dropout,\n",
        "                            linear3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FMYPeBPbBHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()  \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16idzsoJbCRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_total_batch = len(train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsgRWSo5bEJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(training_epochs):\n",
        "    model.train()\n",
        "    avg_cost=0\n",
        "    \n",
        "    for X, Y in train_loader:\n",
        "        X = X.view(-1, 28 * 28)\n",
        "        Y = Y\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        prediction = model(X)\n",
        "        loss = criterion(prediction, Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        avg_cost += loss / train_total_batch\n",
        "\n",
        "    \n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "    avg_cost_o=avg_cost\n",
        "print('Learning finished')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJfuVvfobFfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    \n",
        "    for X_test, Y_test in test_loader:\n",
        "        X_test = X.view(-1, 28 * 28)\n",
        "        Y_test = Y\n",
        "\n",
        "        prediction = model(X_test)\n",
        "        correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "        accuracy = correct_prediction.float().mean()\n",
        "        \n",
        "    print('Accuracy:', accuracy.item())\n",
        "    accuracy_o= accuracy\n",
        "    \n",
        "    r = random.randint(0, len(mnist_test)-1)\n",
        "    X_single_data = mnist_test.data[r:r + 1].view(-1, 28 *28).float()\n",
        "    Y_single_data = mnist_test.targets[r:r + 1]\n",
        "    \n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = model(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6UU4Ti9cbEq",
        "colab_type": "text"
      },
      "source": [
        "Q.1-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-HKvpjcbgkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a1 = torch.nn.Linear(784, 200, bias=True)\n",
        "a2 = torch.nn.Linear(200, 50, bias=True)\n",
        "a3 = torch.nn.Linear(50, 10, bias=True)\n",
        "relu = torch.nn.ReLU()\n",
        "dropout = torch.nn.Dropout(p=0.3)\n",
        "a_bn1 = torch.nn.BatchNorm1d(200)\n",
        "a_bn2 = torch.nn.BatchNorm1d(50)\n",
        "\n",
        "torch.nn.init.xavier_uniform_(a1.weight)\n",
        "torch.nn.init.xavier_uniform_(a2.weight)\n",
        "torch.nn.init.xavier_uniform_(a3.weight)\n",
        "\n",
        "model_a = torch.nn.Sequential(a1, a_bn1, relu, dropout,\n",
        "                            a2, a_bn2, relu, dropout,\n",
        "                            a3)\n",
        "criterion = torch.nn.CrossEntropyLoss()  \n",
        "optimizer_a= torch.optim.Adam(model_a.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8hsTXwvcZES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b1 = torch.nn.Linear(784, 50, bias=True)\n",
        "b2 = torch.nn.Linear(50, 200, bias=True)\n",
        "b3 = torch.nn.Linear(200, 10, bias=True)\n",
        "relu = torch.nn.ReLU()\n",
        "dropout = torch.nn.Dropout(p=0.3)\n",
        "b_bn1 = torch.nn.BatchNorm1d(50)\n",
        "b_bn2 = torch.nn.BatchNorm1d(200)\n",
        "\n",
        "torch.nn.init.xavier_uniform_(b1.weight)\n",
        "torch.nn.init.xavier_uniform_(b2.weight)\n",
        "torch.nn.init.xavier_uniform_(b3.weight)\n",
        "\n",
        "model_b=torch.nn.Sequential(b1, b_bn1, relu, dropout,\n",
        "                            b2, b_bn2, relu, dropout,\n",
        "                            b3)\n",
        "criterion = torch.nn.CrossEntropyLoss()  \n",
        "optimizer_b= torch.optim.Adam(model_b.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcEIXazcbluT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(training_epochs):\n",
        "    model_a.train()\n",
        "    model_b.train()\n",
        "    avg_cost_a=0\n",
        "    avg_cost_b=0\n",
        "    loss_a=0\n",
        "    loss_b=0\n",
        "    \n",
        "    for X, Y in train_loader:\n",
        "        X = X.view(-1, 28 * 28)\n",
        "        Y = Y\n",
        "\n",
        "        optimizer_a.zero_grad()\n",
        "        prediction_a = model_a(X)\n",
        "        loss_a = criterion(prediction_a, Y)\n",
        "        loss_a.backward()\n",
        "        optimizer_a.step()\n",
        "        \n",
        "        avg_cost_a += loss_a / train_total_batch\n",
        "                \n",
        "        optimizer_b.zero_grad()\n",
        "        prediction_b = model_b(X)\n",
        "        loss_b = criterion(prediction_b, Y)\n",
        "        loss_b.backward()\n",
        "        optimizer_b.step()\n",
        "        \n",
        "        avg_cost_b += loss_b / train_total_batch\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost a =', '{:.9f}'.format(avg_cost_a),'cost b =', '{:.9f}'.format(avg_cost_b))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKI3TK_XbnnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    model_a.eval()\n",
        "    model_b.eval()\n",
        "    \n",
        "    for X_test, Y_test in test_loader:\n",
        "        X_test = X.view(-1, 28 * 28)\n",
        "        Y_test = Y\n",
        "\n",
        "        prediction_a = model_a(X_test)\n",
        "        correct_prediction_a = torch.argmax(prediction_a, 1) == Y_test\n",
        "        accuracy_a = correct_prediction_a.float().mean()\n",
        "\n",
        "        prediction_b = model_b(X_test)\n",
        "        correct_prediction_b = torch.argmax(prediction_b, 1) == Y_test\n",
        "        accuracy_b = correct_prediction_b.float().mean()\n",
        "        \n",
        "    print('Accuracy_a:', accuracy_a.item(), 'Accuracy_b', accuracy_b.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6yrJl9xcH3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Accuracy_o:', accuracy_o.item(), 'Accuracy_a:', accuracy_a.item(), 'Accuracy_b', accuracy_b.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SB_5_3-ezdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}