{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guideline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 마크다운 문법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[이미지 삽입]** <br>\n",
    "같은 폴더내에 img폴더를 만든다. 그리고 해당 이미지를 폴더안에 넣습니다.\n",
    "이 때 width, height, title, alt는 없어도 그림이 들어갑니다.\n",
    "\n",
    "<img src=\"img/esc_icon.png\" width=\"40%\" height=\"30%\" title=\"지정가능\" alt=\"name1\"></img>\n",
    "\n",
    "\n",
    "**[글의 양식]** <br>\n",
    "#1개부터 #6개까지 조절가능.\n",
    "\n",
    "수평선\n",
    "<hr/>\n",
    "만든뒤에\n",
    "\n",
    "**글자강조하고** <br> 줄바꾸고\n",
    "> 인용문을\n",
    ">> 중첩하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 표 | 만 | 들 | 기 |\n",
    "|---|---|---|---|\n",
    "|이|렇|게|넣|\n",
    "|으|면|됩|니|   \n",
    "|다|호|호|호|   \n",
    "\n",
    "[링크는여기](https://www.naver.com/)\n",
    "\n",
    "<center> 가운데정렬 </center>\n",
    "<div style=\"text-align: right\"> 오른쪽정렬 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[수식삽입]**<br>\n",
    "문장 중간에 $ f(x) = x_1 + x_2 $ 넣는 코드 <br>\n",
    "한 줄 띄우고 가운데정렬 $$ f(x) = x^1 + x^2 $$\n",
    "\n",
    "$$ f(x) = x_{a+b} + y^{a-b} $$\n",
    "$$ P_{\\theta}(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2} (\\frac{x-\\mu}{\\sigma})^2} $$\n",
    "$$ A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 단축키정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "셀 안에서는 ESC를 누른 후에 셀 밖으로 나가야 M,Y,A,B.D+D가 먹힌다.<br><br> 셀 밖에서는 ENTER를 입력하면 해당 셀로 다시 들어간다.\n",
    "\n",
    "<center>\n",
    "M : 셀을 Markdown 모드로 변경. <br>\n",
    "Y : 셀을 Code 모드로 변경. <br>\n",
    "A : 현재 셀 위에 새로운 셀 추가 <br>\n",
    "B : 현재 셀 밑에 새로운 셀 추가 <br>\n",
    "D+D : 현재 셀 삭제 <br>\n",
    "CTRL + ENTER : 셀 실행 <br>\n",
    "SHIFT + ENTER : 셀 실행 후 다음 셀로 넘어가기 <br>\n",
    "ALT + ENTER : 셀 실행 후 새로운 셀 추가하기\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실자료제작예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Gradient descent method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning이란 무엇인가?**<br>\n",
    "Loss를 최소화 하는 모델의 파라미터 $\\mathbf{w}$를 찾아나가는 것이 결국 learning이다.\n",
    "\n",
    "<img src=\"img/gd.png\" width=\"70%\" height=\"30%\"></img>\n",
    "\n",
    "> PyTorchZeroToAll Lecture 03: Gradient Descent [source](https://www.youtube.com/watch?v=b4Vyma9wPHo&list=PLlMkM4tgfjnJ3I-dbhO9JTw7gNty6o_2m&index=3)\n",
    "\n",
    "**Gradient에 대한 이야기**\n",
    "$$ \\nabla f = (\\dfrac{\\partial f}{\\partial w_1},\\dots,\\dfrac{\\partial f}{\\partial w_n}) $$ <br>\n",
    "만약 산이나 언덕의 특정 지점 (x,y)에서의 높이는 H(x,y)라고 하면, gradient는 경사가 가파른 방향(위를 바라보는)과 그 경사의 크기를 나타낸다. \n",
    "\n",
    "$$ w=w-\\alpha\\dfrac{\\partial loss}{\\partial w} $$ \n",
    "\n",
    "Gradient descent는 결국 최적의 $ \\mathbf{w} $를 찾아나가는 방식이다.\n",
    "\n",
    "이 때 $\\alpha$는 learning rate를 의미하며 hyper-parameter로서 사용자가 조정을 해서 최적의 값을 튜닝해야한다. $\\alpha$가 너무 큰 경우에는 폭이 너무 커서 global minimum이 아니라 local minimum으로 잘못 수령할 수 있으며, 너무 작은 경우에는 학습 속도가 오래 걸린다는 단점이 있다.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "**구체적인 프로세스** <br>\n",
    "등산을 하고 산 중턱에 도착했는데 밤이 되고 앞이 하나도 보이지 않는다고 하자. 이 때 한 걸음 한 걸음씩 내딛으면서 하산하는 길을 찾는 것이 바로 Gradient descent 방식이다. 값을 조금씩 바꿔가면서 loss를 최소화하는 $ \\mathbf{w} $를 찾는 것이다.\n",
    "\n",
    "이런 과정을 우리는 함수의 최적화라고 한다.\n",
    "최적화란 목적함수(여기서는 Loss)의 함수값을 최적화(최대화 혹은 최소화)시키는 파라미터 조합을 찾는 문제이다.\n",
    "목적함수의 선형여부, 제약조건의 존재여부에 따라 최적화문제가 달라진다.\n",
    "\n",
    "보통 스코어의 경우 최대화문제이며 비용이나 손실 및 에러 등은 최소화 문제로 보통 간주한다.\n",
    "하지만 f의 최대화와 -f의 최소화는 동일하므로 방법론적으로는 이 두개는 같은 선상에 놓여있다.\n",
    "\n",
    "이러한 최적화문제는 결국 하나의 위치를 잡고(Initial point) 함수값이 감소(최소화문제)하는 방향으로 조금씩 파라미터의 값을 이동해나가는 것이다.\n",
    "이 때의 이슈는 **'어느 방향으로?', '한 번에 얼만큼?'** 이다.\n",
    "그리고 수학적으로 방향을 결정하는 것은 기울기(First derivative)이며, 이동량을 결정하는 것은 곡률(Second derivative)이다.\n",
    "\n",
    "Newton's method, Gauss-Newton method, LM(Levenberg-Marquardt) 모두 Second derivative를 이용한 최적화 기법이다.\n",
    "\n",
    "비선형 함수의 최적화는 주로 Levenberg-Marquardt를 이용하는데, Gauss-Newton method와 GD의 결합으로 볼 수 있다.\n",
    "\n",
    "reference : https://darkpgmr.tistory.com/149"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 코드구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.712, b: -0.221 Cost: 0.872870\n",
      "Epoch  100/1000 W: 1.027, b: -0.062 Cost: 0.000562\n",
      "Epoch  200/1000 W: 1.022, b: -0.049 Cost: 0.000347\n",
      "Epoch  300/1000 W: 1.017, b: -0.039 Cost: 0.000215\n",
      "Epoch  400/1000 W: 1.013, b: -0.030 Cost: 0.000133\n",
      "Epoch  500/1000 W: 1.010, b: -0.024 Cost: 0.000082\n",
      "Epoch  600/1000 W: 1.008, b: -0.019 Cost: 0.000051\n",
      "Epoch  700/1000 W: 1.006, b: -0.015 Cost: 0.000031\n",
      "Epoch  800/1000 W: 1.005, b: -0.012 Cost: 0.000019\n",
      "Epoch  900/1000 W: 1.004, b: -0.009 Cost: 0.000012\n",
      "Epoch 1000/1000 W: 1.003, b: -0.007 Cost: 0.000007\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 외에도 글자색수정, gif삽입 등 여러가지 기능들이 있습니다.\n",
    "유튜브나 자료들을 정리하면서 공부하고 흔적을 남기며 이를 또 공유하는 것.\n",
    "이것이 바로 기쁨이 아닐까요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
