{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESC Session1 HW - 김정현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Threshold가 0.3으로 acceptance rate가 높은 경우, 체납할 고객에게도 회원가입하기 쉬워진다. 이 예시에서 True는 체납하지 않을 고객, False는 체납할 고객이기 때문에 threshold를 0.3으로 낮게 설정한다는 것은 FP을 올리고 (체납할 고객도 가입) FN을 낮추는 것을 의미한다 (체납하지 않을 고객에게 가입 불허). Type1 error는 귀무가설이 true일때 귀무가설을 reject하는 것이고, Type2 error는 대립가설이 true일때 귀무가설을 reject하지 않는 것이다. 여기서 귀무가설을 \"고객은 체납할 것이다\"로 설정하자. Type1 error는 체납할 고객을 가입시키는 경우이고 (FP에 해당), Type2 error는 체납하지 않을 고객을 가입 불허하는 경우이다 (FN에 해당). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2-1\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Accuracy=$\\frac{TP+TN}{TP+FP+FN+TN}$\n",
    "<br>\n",
    "<br>\n",
    "Precision=$\\frac{TP}{TP+FP}$\n",
    "<br>\n",
    "<br>\n",
    "Recall=$\\frac{TP}{TP+FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2-2\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Job-market signaling: employer's decision (기업의 고용 선택): \n",
    "\n",
    "<br>\n",
    "- True: 생산성 높은 직원\n",
    "<br>\n",
    "- False: 생산성 낮은 직원\n",
    "<br>\n",
    "- TP: 생산성 높은 직원을 고용\n",
    "<br>\n",
    "- FP: 생산성 낮은 직원을 고용\n",
    "<br>\n",
    "- TN: 생산성 높은 직원을 고용하지 않음\n",
    "<br>\n",
    "- FN: 생산성 낮은 직원을 고용하지 않음\n",
    "<br>\n",
    "<br>\n",
    "Threshold를 높인다는 것은 FP를 낮추고, FN을 높인다는 것이다. 즉, 고용 기준이 높아져서 (가령 학교, 성적, 인턴경험과 같은 '스펙') 생산성 낮은 직원 고용을 줄이는 대신, 생산성 높은 고객도 고용하지 않는 경우가 생기게 된다. \n",
    "<br>\n",
    "<br>\n",
    "기업이 threshold를 늘리는 것은 합리적 선택으로 볼 수 있다. 기업은 직원을 고용하기 전까지 지원자의 생산성을 파악하기 어렵다. 실제로 생산성 낮은 지원자를 생산성이 높다고 오판하여 고용을 하면 기업에게 손해일 것이다. 따라서 기업은 비용 최소화를 위해 theshold를 높이려고 할 것이다. 특히 한국같은 경우 고용시장이 유연하지 않기 때문에 직원을 고용할때 직원에게 장기적으로 지불해야할 누적 연봉을 고려해야하며 생산성 낮은 직원을 쉽게 해고하기 어렵다. 고용할 직원을 신중히 뽑아야하므로 threshold를 높이는 것은 합리적이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn #code required for model\n",
    "import torchvision.datasets as dsets #MNIST dataset\n",
    "import torchvision.transforms as transforms #torchvision.transforms module contains methods to transform objects into others\n",
    "#will use to to transform images to PyTorch tensors\n",
    "from torch.autograd import Variable #torch.autograd module contains Variable class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download and load dataset to memory\n",
    "\n",
    "#MNIST Dataset (Images and Labels)\n",
    "train_dataset=dsets.MNIST(root='./data',train=True,transform=transforms.ToTensor(), download=True)\n",
    "test_dataset=dsets.MNIST(root='./data',train=False,transform=transforms.ToTensor())\n",
    "\n",
    "#Hyper parameters\n",
    "input_size=784 #image size is 28*28\n",
    "num_classes=10 #10digits - 10 diff outputs\n",
    "num_epochs=5 #train 5 times\n",
    "batch_size=100 #train small batches of 100 images to prent memory overflow\n",
    "learning_rate=0.001\n",
    "\n",
    "#Dataset Loader (Input Pipline)\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#Define model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes): #initialize model as subclass of torch.nn.Module\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear=nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x): #define forward pass\n",
    "        out = self.linear(x) #softmax is internally calculated during each forward pass\n",
    "        return out\n",
    "\n",
    "#instantiate object for the same\n",
    "model=LogisticRegression(input_size, num_classes)\n",
    "\n",
    "#set loss function and optimizer\n",
    "#softmax is internally computed\n",
    "#set parameters to be updated\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step:[100/600], Loss: 2.1925\n",
      "Epoch: [1/5], Step:[200/600], Loss: 2.0657\n",
      "Epoch: [1/5], Step:[300/600], Loss: 2.0361\n",
      "Epoch: [1/5], Step:[400/600], Loss: 1.9148\n",
      "Epoch: [1/5], Step:[500/600], Loss: 1.8864\n",
      "Epoch: [1/5], Step:[600/600], Loss: 1.8158\n",
      "Epoch: [2/5], Step:[100/600], Loss: 1.7613\n",
      "Epoch: [2/5], Step:[200/600], Loss: 1.6840\n",
      "Epoch: [2/5], Step:[300/600], Loss: 1.6216\n",
      "Epoch: [2/5], Step:[400/600], Loss: 1.5796\n",
      "Epoch: [2/5], Step:[500/600], Loss: 1.5472\n",
      "Epoch: [2/5], Step:[600/600], Loss: 1.4181\n",
      "Epoch: [3/5], Step:[100/600], Loss: 1.5209\n",
      "Epoch: [3/5], Step:[200/600], Loss: 1.3617\n",
      "Epoch: [3/5], Step:[300/600], Loss: 1.3720\n",
      "Epoch: [3/5], Step:[400/600], Loss: 1.3159\n",
      "Epoch: [3/5], Step:[500/600], Loss: 1.3167\n",
      "Epoch: [3/5], Step:[600/600], Loss: 1.2969\n",
      "Epoch: [4/5], Step:[100/600], Loss: 1.2071\n",
      "Epoch: [4/5], Step:[200/600], Loss: 1.2295\n",
      "Epoch: [4/5], Step:[300/600], Loss: 1.1853\n",
      "Epoch: [4/5], Step:[400/600], Loss: 1.1690\n",
      "Epoch: [4/5], Step:[500/600], Loss: 1.1598\n",
      "Epoch: [4/5], Step:[600/600], Loss: 1.1268\n",
      "Epoch: [5/5], Step:[100/600], Loss: 1.1143\n",
      "Epoch: [5/5], Step:[200/600], Loss: 1.1380\n",
      "Epoch: [5/5], Step:[300/600], Loss: 1.0737\n",
      "Epoch: [5/5], Step:[400/600], Loss: 1.0551\n",
      "Epoch: [5/5], Step:[500/600], Loss: 1.0015\n",
      "Epoch: [5/5], Step:[600/600], Loss: 0.9980\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "#1. reset all gradients to 0\n",
    "#2. Make a forward pass\n",
    "#3. Calculate the loss\n",
    "#4. Perform backpropagation\n",
    "#5. Update all weights\n",
    "\n",
    "#Training the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images=Variable(images.view(-1,28*28))\n",
    "        labels=Variable(labels)\n",
    "        \n",
    "        #forward+backward+optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(images)\n",
    "        loss=criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1)%100==0:\n",
    "            print('Epoch: [%d/%d], Step:[%d/%d], Loss: %.4f' %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(images); #각 i번째 iteration에는 data가 100개 (batchsize=100)\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 8, 8, 2, 6, 2, 2, 6, 1, 3, 7, 5, 7, 5, 6, 8, 0, 2, 9, 9, 9, 6, 5,\n",
      "        3, 7, 5, 9, 1, 1, 4, 9, 6, 5, 5, 8, 1, 9, 1, 8, 2, 7, 8, 3, 3, 2, 6, 0,\n",
      "        6, 4, 4, 0, 5, 3, 9, 4, 9, 6, 7, 4, 1, 9, 4, 3, 4, 3, 1, 6, 4, 7, 7, 1,\n",
      "        0, 9, 8, 4, 8, 2, 0, 3, 1, 1, 1, 1, 1, 2, 2, 1, 3, 1, 6, 6, 7, 9, 6, 9,\n",
      "        5, 4, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(labels);len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 82 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "#Test the Model\n",
    "correct=0\n",
    "total=0\n",
    "for images, labels in test_loader:\n",
    "    images=Variable(images.view(-1,28*28))\n",
    "    outputs=model(images)\n",
    "    _, predicted = torch.max(outputs.data,1) #vector에서 가장 큰 원소를 고르는 것? \n",
    "    total+=labels.size(0)\n",
    "    correct+=(predicted==labels).sum()\n",
    "print('Accuracy of the model on the 10000 test images: %d %%' %(100*correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4-1: optim.SGD 사용하지 않고 구현하는 방법\n",
    "<br>\n",
    "1. forward함수 정의 (독립변수들의 linear combination)\n",
    "2. softmax함수 정의\n",
    "3. cost function 정의 using cross entropy loss\n",
    "4. cost function의 weight에 대한 gradient 정의: cost function을 weight에 대해 직접 미분해서 input으로 softmax값과 labels를 받음(?)\n",
    "5. weight=weight - learning_rate*gradient로 GD Algorithm를 manually 계산&update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-845356d34ee2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#back propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#initialize gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "#대략적으로 이렇게 코딩하면 되지 않을까 생각합니다...\n",
    "\n",
    "def grad(output, labels): #cost function을 w에 대해 미분한 함수 사용 (계산이 복잡하여 식을 정확히 도출할 수 없었습니다)\n",
    "    grad=labels*images+sum(labels*exp(w@x)*x/sum(exp(w@x)))\n",
    "    return grad\n",
    "\n",
    "for epoch in range(num_epochs): #train 횟수\n",
    "    for i, (images, labels) in enumerate(train_loader): #batch size는 100개, 총 600번의 iterations\n",
    "        images=Variable(images.view(-1,28*28))\n",
    "        labels=Variable(labels)\n",
    "        w=Variable(torch.Tensor([j for (j,image) in enumerate(images)])) #weight벡터 초기화\n",
    "        \n",
    "        #forward+backward+optimize\n",
    "        outputs=model(images) #forward pass\n",
    "        loss=criterion(outputs, labels)\n",
    "        loss.backward() #back propagation\n",
    "        w.data=w.data-0.001*w.grad.data \n",
    "        w.grad.data.zero_() #initialize gradient\n",
    "        \n",
    "        if (i+1)%100==0:\n",
    "            print('Epoch: [%d/%d], Step:[%d/%d], Loss: %.4f' %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4-2: 다른 optimizer 이용하여 결과값 비교\n",
    "<br>\n",
    "https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x240a5c4dd10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.196, b: 0.519 Cost: 4.589475\n",
      "Epoch  100/1000 W: 0.452, b: 1.107 Cost: 0.201632\n",
      "Epoch  200/1000 W: 0.547, b: 1.002 Cost: 0.146484\n",
      "Epoch  300/1000 W: 0.620, b: 0.841 Cost: 0.103372\n",
      "Epoch  400/1000 W: 0.692, b: 0.681 Cost: 0.067674\n",
      "Epoch  500/1000 W: 0.760, b: 0.532 Cost: 0.041302\n",
      "Epoch  600/1000 W: 0.819, b: 0.401 Cost: 0.023549\n",
      "Epoch  700/1000 W: 0.868, b: 0.293 Cost: 0.012553\n",
      "Epoch  800/1000 W: 0.907, b: 0.207 Cost: 0.006254\n",
      "Epoch  900/1000 W: 0.936, b: 0.141 Cost: 0.002909\n",
      "Epoch 1000/1000 W: 0.958, b: 0.093 Cost: 0.001262\n"
     ]
    }
   ],
   "source": [
    "##adam\n",
    "\n",
    "#데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.239, b: -0.022 Cost: 3.896044\n",
      "Epoch  100/1000 W: 0.826, b: 0.386 Cost: 0.021840\n",
      "Epoch  200/1000 W: 0.896, b: 0.231 Cost: 0.007883\n",
      "Epoch  300/1000 W: 0.955, b: 0.100 Cost: 0.001478\n",
      "Epoch  400/1000 W: 0.989, b: 0.025 Cost: 0.000093\n",
      "Epoch  500/1000 W: 0.999, b: 0.002 Cost: 0.000001\n",
      "Epoch  600/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  700/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  800/1000 W: 1.016, b: 0.016 Cost: 0.002175\n",
      "Epoch  900/1000 W: 1.001, b: 0.001 Cost: 0.000005\n",
      "Epoch 1000/1000 W: 1.002, b: 0.002 Cost: 0.000030\n"
     ]
    }
   ],
   "source": [
    "##rmsprop\n",
    "\n",
    "#데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD에 momentum추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.440, b: -0.357 Cost: 3.021716\n",
      "Epoch  100/1000 W: 1.005, b: -0.003 Cost: 0.000059\n",
      "Epoch  200/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  300/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  400/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  500/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  600/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  700/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  800/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  900/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.000, b: -0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "##sgd에서 momentum 추가\n",
    "\n",
    "#데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) #momentum=0.9로 설정\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
