{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1) 이 경우에서는 Type 1 error가 높을까요? Type 2 error가 높을까요? FP,FN과 관련지어 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "은행 예시에서 조건부 성공 확률의 threshold를 0.3으로 낮춰 설정했다. False Positive는 높고 False Negative는 낮은 값을 가질 것이다.\n",
    "False Positive는 Type 1 Error, False Negative는 Type 2 Error에 각각 상응하기 때문에\n",
    "Type 1 Error가 Type 2 Error보다 높다고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2-1) Accuracy, Precision and Recall이 무엇인지 TP,FP,FN,TN의 식으로 나타내 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy : 모델이 제대로 분류하는 비율 = (TP + TN) / (TP + FP + FN + TN)\n",
    "Precision : True로 분류된 것 중 실제 True의 비율 = TP / (TP + FP)\n",
    "Recall : 실제 True인 것 중 True로 분류한 비율 = TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2-2) Precision and Recall에 관한 제가 든 예시가 아닌 실생활 예시 하나를 들어주세요. 생각한 예시에서 성공확률의 threshold를 높인다는 것은 무슨 의미인가요? threshold를 늘리는 것이 비교적 합리적일까요? 아니면 비합리적일까요? 혹은 알 수 없을까요? 각자의 의견을 근거를 들어 적어주시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision과 Recall은 분모에 FP, FN을 가지는 것이 차이점이다\n",
    "서류 전형 합격/불합격을 판단하는 상황에서 FN은 서류 탈락이지만 실력이 있는 지원자이고 FP는 그 반대이다.\n",
    "지원자의 자기소개서가 주어졌을 때 합격하는 주건부 확률의 threshold를 낮추면 FN을 방지하는 것이고\n",
    "반대로 threshold를 높게 설정하는 것은 FP를 줄이는 결과는 가져온다. 어느 쪽이 합리적인지 알 수 없다.\n",
    "FN FP 둘 중 어느 것을 더 중요하게 여기는지 혹은 향후 선발 일정에 따라 달라질 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3) 코드 따라해보고 주석 달기\n",
    "코드 출처 : https://www.geeksforgeeks.org/identifying-handwritten-digits-using-logistic-regression-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST Dataset(Images and Labels)\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                            train=True,\n",
    "                            transform = transforms.ToTensor(),\n",
    "                            download=True)\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                           train = False,\n",
    "                           transform = transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper Parameters\n",
    "input_size = 784 # 28*28\n",
    "num_classes = 10 #number of digits as outputs\n",
    "num_epochs = 5 # train for five times on the entire dataset\n",
    "batch_size = 100 # to prevent memory overflow\n",
    "learning_rate = 0.001\n",
    "\n",
    "#Dataset Loader (Input Pipline)\n",
    "train_Loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "test_Loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self,input_size, num_classes):\n",
    "        super(LogisticRegression,self).__init__()\n",
    "        self.linear = nn.Linear(input_size,num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "model = LogisticRegression(input_size,num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 2.2269\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 2.1029\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 2.0177\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.8901\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.8488\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.7662\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.6826\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.6371\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.5951\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.5411\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.5012\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.4868\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.4114\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.4156\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.4198\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.3188\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.2815\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.2740\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.2048\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.1832\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.1982\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.1978\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.1245\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 1.0689\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 1.0193\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 1.1348\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 0.9600\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 1.0835\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 0.9482\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 1.0546\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_Loader):\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad() # 1. Reset all gradients to 0\n",
    "        outputs = model(images) # 2. Make a forward pass\n",
    "        loss = criterion(outputs, labels) # 3. Calculate the loss\n",
    "        loss.backward() # 4. Perform backpropagation\n",
    "        optimizer.step() # 5. Update all weights\n",
    "        \n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch+1, num_epochs, i+1, len(train_dataset) // batch_size, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images:  83 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "#Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_Loader:\n",
    "    images = Variable(images.view(-1,28*28))\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data,1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % (\n",
    "       100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4-1) optim.SGD를 사용하지 않고 코드를 짠다면 어떤 방식으로 짜야할지 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD를 사용하지 않는다면 모델 훈련하는 for 구문에\n",
    "w 값을 업데이트하는 w.data = w.data-learning_rate * w.grad.data[0]과\n",
    "w 값을 초기화하는 w.grad.data.zero_() 를 직접 넣어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4-2) Gradient Descent 다른 optimizer 3가지 이용하여 결과 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = LinearRegressionModel()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.396, b: -0.852 Cost: 4.616215\n",
      "Epoch  100/1000 W: 1.035, b: -0.206 Cost: 0.020583\n",
      "Epoch  200/1000 W: 1.067, b: -0.148 Cost: 0.003183\n",
      "Epoch  300/1000 W: 1.057, b: -0.126 Cost: 0.002317\n",
      "Epoch  400/1000 W: 1.047, b: -0.104 Cost: 0.001574\n",
      "Epoch  500/1000 W: 1.037, b: -0.083 Cost: 0.001001\n",
      "Epoch  600/1000 W: 1.029, b: -0.064 Cost: 0.000598\n",
      "Epoch  700/1000 W: 1.022, b: -0.048 Cost: 0.000336\n",
      "Epoch  800/1000 W: 1.016, b: -0.035 Cost: 0.000177\n",
      "Epoch  900/1000 W: 1.011, b: -0.025 Cost: 0.000088\n",
      "Epoch 1000/1000 W: 1.008, b: -0.017 Cost: 0.000041\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    cost = F.mse_loss(prediction,y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch,nb_epochs,W,b,cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = LinearRegressionModel()\n",
    "\n",
    "optimizer = optim.ASGD(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.994, b: -0.533 Cost: 0.367124\n",
      "Epoch  100/1000 W: 1.154, b: -0.349 Cost: 0.017568\n",
      "Epoch  200/1000 W: 1.121, b: -0.274 Cost: 0.010851\n",
      "Epoch  300/1000 W: 1.095, b: -0.216 Cost: 0.006703\n",
      "Epoch  400/1000 W: 1.075, b: -0.169 Cost: 0.004140\n",
      "Epoch  500/1000 W: 1.059, b: -0.133 Cost: 0.002557\n",
      "Epoch  600/1000 W: 1.046, b: -0.105 Cost: 0.001579\n",
      "Epoch  700/1000 W: 1.036, b: -0.082 Cost: 0.000975\n",
      "Epoch  800/1000 W: 1.028, b: -0.065 Cost: 0.000602\n",
      "Epoch  900/1000 W: 1.022, b: -0.051 Cost: 0.000371\n",
      "Epoch 1000/1000 W: 1.018, b: -0.040 Cost: 0.000229\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    cost = F.mse_loss(prediction,y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch,nb_epochs,W,b,cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = LinearRegressionModel()\n",
    "\n",
    "optimizer = optim.Adamax(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.798, b: -0.536 Cost: 19.499521\n",
      "Epoch  100/1000 W: -0.024, b: 0.229 Cost: 4.075563\n",
      "Epoch  200/1000 W: 0.359, b: 0.589 Cost: 0.766248\n",
      "Epoch  300/1000 W: 0.533, b: 0.730 Cost: 0.188644\n",
      "Epoch  400/1000 W: 0.609, b: 0.768 Cost: 0.102410\n",
      "Epoch  500/1000 W: 0.644, b: 0.762 Cost: 0.086978\n",
      "Epoch  600/1000 W: 0.665, b: 0.737 Cost: 0.079398\n",
      "Epoch  700/1000 W: 0.682, b: 0.706 Cost: 0.072408\n",
      "Epoch  800/1000 W: 0.698, b: 0.671 Cost: 0.065431\n",
      "Epoch  900/1000 W: 0.714, b: 0.635 Cost: 0.058497\n",
      "Epoch 1000/1000 W: 0.731, b: 0.596 Cost: 0.051682\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    cost = F.mse_loss(prediction,y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch,nb_epochs,W,b,cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam, ASGD, Adamax 세 가지 optimizer 중에서 같은 epoch에서 ASGD가 가장 높은 cost 값을 가졌다.\n",
    "Adam과 Adamax의 경우 이전에는 Adamax가 더 낮은 cost를 반환했지만 출력된 600번째 epoch에서부터 Adam이 더 낮은 cost를 가졌다.\n",
    "W=1, b=0의 결론은 Adam이 가장 먼저(적은 epoch) 도출했다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
