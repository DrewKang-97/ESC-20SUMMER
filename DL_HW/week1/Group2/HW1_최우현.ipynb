{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1_최우현.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ODC5Ol69Z6u",
        "colab_type": "text"
      },
      "source": [
        "#**2020 Summer ESC :: Week 1 세션 과제**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m_xPbBc9acr",
        "colab_type": "text"
      },
      "source": [
        "19기 최우현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63tX4s6o87YR",
        "colab_type": "text"
      },
      "source": [
        "### **1. 이 경우에서는 Type 1 error가 높을까요? Type 2 Error가 높을까요? FP, FN과 관련지어 설명해주세요.**\n",
        "Type 1 error는 $P(Reject the Null | H_{0} is true)$이므로 FN에, Type 2 error는 $P(Accept the Null | H_{0} is not true)$ 이므로 FP에 해당한다. Threshold가 낮기 때문에 웬만하면 accept하게 됩니다. 즉, FP가 높고 FN이 낮습니다. 따라서 Type 2 error가 높습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_5VOkrj87zk",
        "colab_type": "text"
      },
      "source": [
        "### **2-1. Accuracy, Precision and Recall이 무엇인지 TP,FP,FN,TN의 식으로 나타내 주세요.**\n",
        "$Accuracy = (TP+TN)/(TP+FN+FP+TN)$\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "$Precision = TP/(TP+FP)$\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "$Recall = TP/(TP+FN)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJhP15ID_QWo",
        "colab_type": "text"
      },
      "source": [
        "### **2-2.  Precision and Recall에 관한 제가 든 예시가 아닌 실생활 예시 하나를 들어주세요. 생각한 예시에서 성공확률의 threshold를 높인다는 것은 무슨 의미인가요? 생각한 예시에서는 그 threshold를 늘리는 것이 비교적 합리적일까요? 아니면 비합리적일까요? 혹은 알 수 없을까요? 각자의 의견을 근거를 들어 적어주시기 바랍니다.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDCTAmIp_8a4",
        "colab_type": "text"
      },
      "source": [
        "### **3. 코드 따라해보고 주석 달기!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RssSsYKN_yyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn \n",
        "import torchvision.datasets as dsets \n",
        "import torchvision.transforms as transforms \n",
        "from torch.autograd import Variable "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhAnWj5QAzUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MNIST Dataset (Images and Labels) \n",
        "train_dataset = dsets.MNIST(root ='./data',  \n",
        "                            train = True,  \n",
        "                            transform = transforms.ToTensor(), \n",
        "                            download = True) \n",
        "  \n",
        "test_dataset = dsets.MNIST(root ='./data',  \n",
        "                           train = False,  \n",
        "                           transform = transforms.ToTensor()) \n",
        " # Hyper Parameters  \n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        " \n",
        "# Dataset Loader (Input Pipline) \n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,  \n",
        "                                           batch_size = batch_size,  \n",
        "                                           shuffle = True) \n",
        "  \n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,  \n",
        "                                          batch_size = batch_size,  \n",
        "                                          shuffle = False) "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aud9wsQVAzTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining model\n",
        "class LogisticRegression(nn.Module): \n",
        "    def __init__(self, input_size, num_classes): \n",
        "        super(LogisticRegression, self).__init__() \n",
        "        self.linear = nn.Linear(input_size, num_classes) \n",
        "  \n",
        "    def forward(self, x): \n",
        "        out = self.linear(x) \n",
        "        return out \n",
        "model = LogisticRegression(input_size, num_classes) \n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXoHLIAFAzRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting loss function and the optimiser\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)  #Stochastic gradient descent"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moaCnTWhAzMx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "ccd65374-3d31-4a54-bfae-0dc3e001ad97"
      },
      "source": [
        "# Training the Model \n",
        "for epoch in range(num_epochs): \n",
        "    for i, (images, labels) in enumerate(train_loader): \n",
        "        images = Variable(images.view(-1, 28 * 28)) \n",
        "        labels = Variable(labels) \n",
        "  \n",
        "        # Forward + Backward + Optimize \n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(images) \n",
        "        loss = criterion(outputs, labels) \n",
        "        loss.backward() \n",
        "        optimizer.step() \n",
        "  \n",
        "        if (i + 1) % 100 == 0: \n",
        "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
        "                  % (epoch + 1, num_epochs, i + 1, \n",
        "                     len(train_dataset) // batch_size, loss.data)) \n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 2.2059\n",
            "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 2.1574\n",
            "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 2.0532\n",
            "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.9637\n",
            "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.8840\n",
            "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.7684\n",
            "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.7399\n",
            "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.6468\n",
            "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.6576\n",
            "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.5985\n",
            "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.5039\n",
            "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.4423\n",
            "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.4183\n",
            "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.4151\n",
            "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.4261\n",
            "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.3080\n",
            "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.2756\n",
            "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.2950\n",
            "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.3014\n",
            "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.2065\n",
            "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.1738\n",
            "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.0301\n",
            "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.2600\n",
            "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 1.0970\n",
            "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 1.0799\n",
            "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 1.0940\n",
            "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 1.1045\n",
            "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 1.0908\n",
            "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 1.0558\n",
            "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 1.1072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2MNzhrxCsva",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8fa23413-5e6b-47b9-cc9f-7244a20fcf96"
      },
      "source": [
        "# Test the Model \n",
        "correct = 0\n",
        "total = 0\n",
        "for images, labels in test_loader: \n",
        "    images = Variable(images.view(-1, 28 * 28)) \n",
        "    outputs = model(images) \n",
        "    _, predicted = torch.max(outputs.data, 1) \n",
        "    total += labels.size(0) \n",
        "    correct += (predicted == labels).sum() \n",
        "  \n",
        "print('Accuracy of the model on the 10000 test images: % d %%' % ( \n",
        "            100 * correct // total)) "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the 10000 test images:  82 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVu1E-bvDoIK",
        "colab_type": "text"
      },
      "source": [
        "### **4-1. 코드에서 optim.SGD를 사용하지않고 코드를 짠다면 어떤 방식으로 짜야할까요? 설명해주세요.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25HdpGT5ENMd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "95031508-8fe0-4a62-cefa-377f7e7c9a71"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[1], [2], [3]])\n",
        "\n",
        "# optimizer 설정\n",
        "W = torch.zeros(1)\n",
        "b = torch.zeros(1)\n",
        "lr = 0.01\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "    \n",
        "    # H(x) 계산\n",
        "    prediction = (x_train * W) + b\n",
        "    \n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train)\n",
        "    \n",
        "    # cost로 H(x) 개선\n",
        "    # SGD 대신 일일히 미분한 식을 코드로 쳐야 함\n",
        "    gradient = torch.sum((W * x_train + b - y_train) * x_train)\n",
        "    b_new = torch.sum((W * x_train + b - y_train ))\n",
        "    \n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
        "        ))\n",
        "    W -= lr * gradient\n",
        "    b -= lr * b_new"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 W: 0.000, b: 0.000 Cost: 4.666667\n",
            "Epoch  100/1000 W: 0.887, b: 0.257 Cost: 0.009462\n",
            "Epoch  200/1000 W: 0.921, b: 0.179 Cost: 0.004594\n",
            "Epoch  300/1000 W: 0.945, b: 0.125 Cost: 0.002231\n",
            "Epoch  400/1000 W: 0.962, b: 0.087 Cost: 0.001083\n",
            "Epoch  500/1000 W: 0.973, b: 0.061 Cost: 0.000526\n",
            "Epoch  600/1000 W: 0.981, b: 0.042 Cost: 0.000255\n",
            "Epoch  700/1000 W: 0.987, b: 0.029 Cost: 0.000124\n",
            "Epoch  800/1000 W: 0.991, b: 0.020 Cost: 0.000060\n",
            "Epoch  900/1000 W: 0.994, b: 0.014 Cost: 0.000029\n",
            "Epoch 1000/1000 W: 0.996, b: 0.010 Cost: 0.000014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuNsiD32Dn_e",
        "colab_type": "text"
      },
      "source": [
        "### **4-2. 다른 optimizer(3개 ex)adam, rmsprop, sgd에 momentum 추가 등등)를 이용하여 결과값을 비교해주세요..**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5SqIVQJC5oa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "c833fd19-3394-4c27-b8dd-0bf35b940062"
      },
      "source": [
        "# 1. SGD with momentum\n",
        "# 데이터\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[1], [2], [3]])\n",
        "\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "    \n",
        "# 모델 초기화\n",
        "model = LinearRegressionModel()\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)# 보통 0.9를 사용한다고 함\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "      \n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "    \n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train)\n",
        "    \n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad() \n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        params = list(model.parameters())\n",
        "        W = params[0].item()\n",
        "        b = params[1].item()\n",
        "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, W, b, cost.item()\n",
        "        ))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 W: 0.949, b: -0.572 Cost: 0.570647\n",
            "Epoch  100/1000 W: 1.011, b: -0.022 Cost: 0.000090\n",
            "Epoch  200/1000 W: 1.000, b: -0.001 Cost: 0.000000\n",
            "Epoch  300/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
            "Epoch  400/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
            "Epoch  500/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
            "Epoch  600/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
            "Epoch  700/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
            "Epoch  800/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
            "Epoch  900/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
            "Epoch 1000/1000 W: 1.000, b: -0.000 Cost: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY36JP3dH33e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "51b9b4f4-6d27-4f85-b8be-c72af5906566"
      },
      "source": [
        "# 2. NAG(Nesterov Accelerated Gradient)\n",
        "# momentum 먼저, 그 다음 gradient\n",
        "\n",
        "# 데이터\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[1], [2], [3]])\n",
        "\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "    \n",
        "# 모델 초기화\n",
        "model = LinearRegressionModel()\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "      \n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "    \n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train)\n",
        "    \n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad() \n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        params = list(model.parameters())\n",
        "        W = params[0].item()\n",
        "        b = params[1].item()\n",
        "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, W, b, cost.item()\n",
        "        ))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 W: 0.133, b: -0.510 Cost: 8.890903\n",
            "Epoch  100/1000 W: 1.003, b: -0.006 Cost: 0.000005\n",
            "Epoch  200/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
            "Epoch  300/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
            "Epoch  400/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
            "Epoch  500/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
            "Epoch  600/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
            "Epoch  700/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
            "Epoch  800/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
            "Epoch  900/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
            "Epoch 1000/1000 W: 1.000, b: -0.000 Cost: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}