{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H0 : False H1 : True<br>\n",
    "실제 false / 예측 : True -> FP<br>\n",
    "실제 True / 예측 : false -> FN<br>\n",
    "\n",
    "Type 1 / Type 2 는 trade off 관계. 즉 FP.FN 도 Trade off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy(정확도: 전체 예측 중 옳게 말한 결과의 비율) = (TP+TN) / (TP+TN+FP+FN)\n",
    "\n",
    "Precision(정밀도: True라고 예측한 것 중 실제로 True의 비율) = TP / (TP+FP)\n",
    "\n",
    "Recall(재현율: 실제 True 중 True라고 예측한 것의 비율) = TP / (TP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "도둑탐지모델 -> 도둑(T), 시민(F) 에 대해서 실제 도둑인사람 중에서 모델이 도둑이라고 예측한 비율인 재현율이 매우 중요할것이다. (도둑은 다 잡아야하니까)\n",
    "\n",
    "threshold 를 낮추면, 도둑(T) 에 대한 조건이 낮아지므로 많은 도둑을 잡을 수 있을것이다. (재현율은 높음)\n",
    "\n",
    "몰론 어느정도의 시민들이 억울하게 도둑으로 분류되긴 하겠지만, 어쩃든 도둑은 모두 잡게될거니까~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 주석달기(sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T13:07:54.201267Z",
     "start_time": "2020-07-29T13:07:54.195256Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision.datasets as dsets \n",
    "import torchvision.transforms as transforms \n",
    "from torch.autograd import Variable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T13:08:04.785096Z",
     "start_time": "2020-07-29T13:08:04.741855Z"
    }
   },
   "outputs": [],
   "source": [
    "# MNIST Dataset (Images and Labels)  # 데이터셋 불러오기\n",
    "train_dataset = dsets.MNIST(root ='./data',  \n",
    "                            train = True,  \n",
    "                            transform = transforms.ToTensor(), \n",
    "                            download = True) \n",
    "\n",
    "test_dataset = dsets.MNIST(root ='./data',  \n",
    "                           train = False,  \n",
    "                           transform = transforms.ToTensor()) \n",
    "  \n",
    "# Dataset Loader (Input Pipline) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, # 데이터로더 만들기. \n",
    "                                           batch_size = batch_size,  \n",
    "                                           shuffle = True) # shuffle 을 True 로 주어서 학습떄마다 데이터셋이 섞이게 한다.\n",
    "  \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,  \n",
    "                                          batch_size = batch_size,  \n",
    "                                          shuffle = False)  # test 시에는 섞이지 않음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T13:08:11.965544Z",
     "start_time": "2020-07-29T13:08:11.953000Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters  \n",
    "input_size = 784 # input x 데이터의 특성이 784개 이므로 784\n",
    "num_classes = 10 # y 의 클래스가 10개\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T13:08:32.715086Z",
     "start_time": "2020-07-29T13:08:32.705070Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):  # calss 를 이용해 model 을 정의하자.\n",
    "    def __init__(self, input_size, num_classes):  #model 의 forward 계산에 쓰이게 되는 model 들을 정의한다.\n",
    "        super(LogisticRegression, self).__init__() \n",
    "        self.linear = nn.Linear(input_size, num_classes) # nn 에서 직접 선형모델 호출\n",
    "    def forward(self, x):  #forward 계산 메서드 정의\n",
    "        out = self.linear(x)  #층이 한개이므로 그냥 모델을썻따\n",
    "        return out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T13:08:41.231745Z",
     "start_time": "2020-07-29T13:08:41.221148Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_size, num_classes) # 모델선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T13:08:48.557839Z",
     "start_time": "2020-07-29T13:08:48.548398Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T13:11:38.689400Z",
     "start_time": "2020-07-29T13:11:10.555637Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 2.0645\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 1.9261\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 1.8975\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.7934\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.7435\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.6853\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.5432\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.5305\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.4638\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.5137\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.4486\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.4587\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.2870\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.3247\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.1894\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.2553\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.2380\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.2500\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.2181\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.0527\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.1344\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.0682\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.0507\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 1.0616\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 1.1089\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 1.0084\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 1.0292\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 0.9824\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 0.9407\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 1.0152\n"
     ]
    }
   ],
   "source": [
    "# Training the Model \n",
    "for epoch in range(num_epochs): # epoch 만큼 반복\n",
    "    for i, (images, labels) in enumerate(train_loader): # enumerate 를 통해서 batch 동안의 모델향상도 보자! (데이터 크기가 크면 enumerate 는 쓰지않는듯)\n",
    "        images = Variable(images.view(-1, 28 * 28))  # 데이터 전처리작업때 , numpy 를 받았기떄문에, pytorch 에서 쓸 수 있게 tensor 로 변환\n",
    "        labels = Variable(labels) \n",
    "        \n",
    "       # Forward + Backward + Optimize \n",
    "        optimizer.zero_grad() # 기울기초기화\n",
    "        outputs = model(images) # 예측치\n",
    "        loss = criterion(outputs, labels) # loss 선언\n",
    "        loss.backward() # 기울기계산\n",
    "        optimizer.step() #loss 선언\n",
    "        \n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.data.item())) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T13:12:25.216506Z",
     "start_time": "2020-07-29T13:12:24.426026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images:  83 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) # test 에서도 똑같은 작업 \n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs.data, 1) # ㅡmax 의 label 로 predict\n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum() \n",
    "    \n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % ( \n",
    "            100 * correct / total)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T13:18:10.989274Z",
     "start_time": "2020-07-29T13:17:41.219748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 0.5665\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 0.4053\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 0.3134\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 0.2726\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 0.2694\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 0.3428\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 0.2242\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 0.2783\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 0.3588\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 0.4319\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 0.3311\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 0.2018\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 0.3667\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 0.3379\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 0.3824\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 0.2053\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 0.3399\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 0.3295\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 0.2630\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 0.2259\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 0.2754\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 0.2543\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 0.3514\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 0.2655\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 0.2286\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 0.3748\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 0.1587\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 0.2307\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 0.1283\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 0.1966\n",
      "Accuracy of the model on the 10000 test images:  92 %\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision.datasets as dsets \n",
    "import torchvision.transforms as transforms \n",
    "from torch.autograd import Variable \n",
    "  \n",
    "# MNIST Dataset (Images and Labels) \n",
    "train_dataset = dsets.MNIST(root ='./data', \n",
    "                            train = True, \n",
    "                            transform = transforms.ToTensor(), \n",
    "                            download = True) \n",
    "  \n",
    "test_dataset = dsets.MNIST(root ='./data', \n",
    "                           train = False, \n",
    "                           transform = transforms.ToTensor()) \n",
    "  \n",
    "# Dataset Loader (Input Pipline) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                           batch_size = batch_size, \n",
    "                                           shuffle = True) \n",
    "  \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, \n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle = False) \n",
    "  \n",
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "  \n",
    "# Model \n",
    "class LogisticRegression(nn.Module): \n",
    "    def __init__(self, input_size, num_classes): \n",
    "        super(LogisticRegression, self).__init__() \n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "        \n",
    "    def forward(self, x): \n",
    "        out = self.linear(x) \n",
    "        return out \n",
    "    \n",
    "    \n",
    "model = LogisticRegression(input_size, num_classes) \n",
    "  \n",
    "# Loss and Optimizer \n",
    "# Softmax is internally computed. \n",
    "# Set parameters to be updated. \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate) \n",
    "  \n",
    "# Training the Model \n",
    "for epoch in range(num_epochs): \n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        images = Variable(images.view(-1, 28 * 28)) \n",
    "        labels = Variable(labels) \n",
    "        \n",
    "        # Forward + Backward + Optimize \n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(images) \n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        \n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, \n",
    "                     len(train_dataset) // batch_size, loss.data.item())) \n",
    "            \n",
    "# Test the Model \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) \n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs.data, 1) \n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum() \n",
    "    \n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % ( \n",
    "            100 * correct / total)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T13:19:13.642519Z",
     "start_time": "2020-07-29T13:18:43.256758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 0.8766\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 0.6384\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 0.5750\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 0.4537\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 0.4598\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 0.3198\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 0.4135\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 0.3853\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 0.3956\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 0.3342\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 0.3423\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 0.2762\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 0.4759\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 0.3590\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 0.4757\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 0.1145\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 0.3246\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 0.4850\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 0.3746\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 0.3525\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 0.2989\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 0.1781\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 0.3514\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 0.1979\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 0.1455\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 0.2811\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 0.2848\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 0.2479\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 0.2836\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 0.3095\n",
      "Accuracy of the model on the 10000 test images:  92 %\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision.datasets as dsets \n",
    "import torchvision.transforms as transforms \n",
    "from torch.autograd import Variable \n",
    "  \n",
    "# MNIST Dataset (Images and Labels) \n",
    "train_dataset = dsets.MNIST(root ='./data', \n",
    "                            train = True, \n",
    "                            transform = transforms.ToTensor(), \n",
    "                            download = True) \n",
    "  \n",
    "test_dataset = dsets.MNIST(root ='./data', \n",
    "                           train = False, \n",
    "                           transform = transforms.ToTensor()) \n",
    "  \n",
    "# Dataset Loader (Input Pipline) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                           batch_size = batch_size, \n",
    "                                           shuffle = True) \n",
    "  \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, \n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle = False) \n",
    "  \n",
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "  \n",
    "# Model \n",
    "class LogisticRegression(nn.Module): \n",
    "    def __init__(self, input_size, num_classes): \n",
    "        super(LogisticRegression, self).__init__() \n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "        \n",
    "    def forward(self, x): \n",
    "        out = self.linear(x) \n",
    "        return out \n",
    "    \n",
    "    \n",
    "model = LogisticRegression(input_size, num_classes) \n",
    "  \n",
    "# Loss and Optimizer \n",
    "# Softmax is internally computed. \n",
    "# Set parameters to be updated. \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) \n",
    "  \n",
    "# Training the Model \n",
    "for epoch in range(num_epochs): \n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        images = Variable(images.view(-1, 28 * 28)) \n",
    "        labels = Variable(labels) \n",
    "        \n",
    "        # Forward + Backward + Optimize \n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(images) \n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        \n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, \n",
    "                     len(train_dataset) // batch_size, loss.data.item())) \n",
    "            \n",
    "# Test the Model \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) \n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs.data, 1) \n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum() \n",
    "    \n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % ( \n",
    "            100 * correct / total)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD 바꾸기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "cost 가 mse이다. 즉 gradient 를 구하기가 매우 쉬울것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T13:42:31.946595Z",
     "start_time": "2020-07-29T13:42:31.941578Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 \n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "y_train = torch.FloatTensor([[1], [2], [3]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T13:43:31.046888Z",
     "start_time": "2020-07-29T13:43:30.945715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.140, b: 0.060 Cost: 4.666667\n",
      "Epoch  100/1000 W: 0.887, b: 0.256 Cost: 0.009462\n",
      "Epoch  200/1000 W: 0.922, b: 0.178 Cost: 0.004594\n",
      "Epoch  300/1000 W: 0.945, b: 0.124 Cost: 0.002231\n",
      "Epoch  400/1000 W: 0.962, b: 0.087 Cost: 0.001083\n",
      "Epoch  500/1000 W: 0.973, b: 0.060 Cost: 0.000526\n",
      "Epoch  600/1000 W: 0.982, b: 0.042 Cost: 0.000255\n",
      "Epoch  700/1000 W: 0.987, b: 0.029 Cost: 0.000124\n",
      "Epoch  800/1000 W: 0.991, b: 0.020 Cost: 0.000060\n",
      "Epoch  900/1000 W: 0.994, b: 0.014 Cost: 0.000029\n",
      "Epoch 1000/1000 W: 0.996, b: 0.010 Cost: 0.000014\n"
     ]
    }
   ],
   "source": [
    "# For reproducibility \n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 모델 초기화 \n",
    "W=torch.zeros(1)\n",
    "b=torch.zeros(1)\n",
    "\n",
    "# optimizer 설정 \n",
    "lr=0.01\n",
    "\n",
    "nb_epochs=1000\n",
    "for epoch in range(nb_epochs + 1):        \n",
    "\n",
    "    # H(x) 계산    \n",
    "    prediction = (x_train * W) + b       \n",
    "\n",
    "    # cost 계산    \n",
    "    cost = F.mse_loss(prediction, y_train)  # 이 mse loss 를 미분한다는것은 (wx+b -y)^2 미분이다.(by w,b)       \n",
    "    w_gradient=torch.sum(((W * x_train + b)- y_train) * x_train)\n",
    "    b_gradient=torch.sum((W*x_train + b) - y_train)\n",
    "    \n",
    "    #update w , b\n",
    "    W-=lr*w_gradient\n",
    "    b-=lr*b_gradient\n",
    "    \n",
    "    # 100번마다 로그 출력                 \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()        \n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
