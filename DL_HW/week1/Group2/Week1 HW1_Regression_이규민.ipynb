{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1) 이 경우에서는 Type 1 error가 높을까요? Type 2 Error가 높을까요? FP, FN과 관련지어 설명해주세요\n",
    "\n",
    "# A1) FP는 False Positive, 즉 실제로는 False 이지만 True 라고 예측한 경우입니다.\n",
    "# 이는 귀무가설이 맞는데 틀리다고 예측한 Type 1 error과 연관이 있습니다.\n",
    "# 또, FN은 False Negative, 즉 실제로는 True 이지만 False 라고 예측한 경우입니다.\n",
    "# 이는 귀무가설이 틀린데 맞다고 예측한 Type 2 error과 연관이 있습니다.\n",
    "# 이 경우에서는 threshold를 낮춰 positive로 예측되는 데이터가 많아지게 만들었습니다.\n",
    "# 따라서 TP와 함께 FP, 즉 Type 1 error이 증가할 것입니다.\n",
    "# 반대로, TN과 함께 FN, 즉 Type 2 error은 감소할 것입니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2-1) Accuracy, Precision and Recall이 무엇인지 TP,FP,FN,TN의 식으로 나타내 주세요.\n",
    "# A2-1) Accuracy(정확도)는 TP + TN /TP + FP + TN + FN 으로 전체 예측 중 True나 False를 각각 정확히 예측한 경우의 비율입니다,\n",
    "# Precision(정밀도)는 TP / TP + FP 로 True라 예측한 것 중 적중한 것의 비율을 말합니다.\n",
    "# Recall(재현율)는 TP / TP + FN 로 실제 True였던 예측 중 적중한 것의 비율을 말합니다.\n",
    "\n",
    "# Q2-2) Precision and Recall에 관한 제가 든 예시가 아닌 실생활 예시 하나를 들어주세요.생각하신 예시에서 성공확률의 threshold를 높인다는 것은 무슨 의미인가요? 생각하신 예시에서는 그 threshold를 늘리는 것이 비교적 합리적일까요? 아니면 비합리적일까요? 혹은 알 수 없을까요?\n",
    "# A2-2) 스팸 메일인지 아닌지를 판단하는 경우, Precision은 스팸 메일이라고 예측한 것 중 진짜 스팸 메일인 것의 비율이고, Recall은 실제 스팸 메일인 것 중 제대로 예측한 것의 비율을 나타냅니다.\n",
    "# 이 경우 threshold를 높인다는 것은 보다 깐깐하게 True라고 예측한다는, 즉 스팸 메일이라고 예측되는 경우가 줄어든다는 의미입니다.\n",
    "# threshold를 높이게 되면 실제 True였던 것 중 True로 예측하는 경우가 줄어드므로 Recall이 줄어듭니다. Recall과 Prediction의 trade-off 관계로 미루어보아 Prediction은 증가할 것이라 말할 수 있습니다.\n",
    "# 그런데, 스팸 메일 분류의 경우 Recall보다 Prediction이 더 중요한 지표라고 생각합니다.\n",
    "# FP가 FN보다 더 큰 영향을 미치기 때문입니다. 실제로는 정상적인 메일인데 이것을 스팸 메일이라고 예측하면 일상 생활에 큰 문제가 있지만, 실제 스팸 메일을 정상적인 메일이라 분류하면 조금의 불쾌함을 느끼는 정도의 문제가 있기 때문입니다.\n",
    "# 따라서 Precision을 높이기 위해 threshold를 높이는 것이 합리적이고, FP 항목을 줄이는 데에 노력해야 할 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 2.2082\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 2.1205\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 2.0429\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.9502\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.8685\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.7641\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.7125\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.6783\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.5982\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.5706\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.5190\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.5485\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.5358\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.3781\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.3433\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.3226\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.3200\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.2290\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.2771\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.1860\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.2380\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.1886\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.1203\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 1.0694\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 1.0569\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 1.1161\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 1.0017\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 1.0075\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 0.9985\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 1.0079\n",
      "Accuracy of the model on the 10000 test images:  82 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "# Q3) 코드 따라해보고 주석 달기!\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision.datasets as dsets \n",
    "import torchvision.transforms as transforms \n",
    "from torch.autograd import Variable\n",
    "\n",
    "# MNIST Dataset (Images and Labels) \n",
    "# MNIST 데이터 다운로드\n",
    "train_dataset = dsets.MNIST(root ='./data',  \n",
    "                            train = True,  \n",
    "                            transform = transforms.ToTensor(), \n",
    "                            download = True) \n",
    "  \n",
    "test_dataset = dsets.MNIST(root ='./data',  \n",
    "                           train = False,  \n",
    "                           transform = transforms.ToTensor()) \n",
    "  \n",
    "    \n",
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001    \n",
    "\n",
    "\n",
    "# Dataset Loader (Input Pipline)\n",
    "# MNIST 데이터 저장,  batch로 쪼갬\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,  \n",
    "                                           batch_size = batch_size,  \n",
    "                                           shuffle = True) \n",
    "  \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,  \n",
    "                                          batch_size = batch_size,  \n",
    "                                          shuffle = False)\n",
    "\n",
    "  \n",
    "# Model \n",
    "# 분석할 로지스틱 모형 정의(784개 in, 10개 out)\n",
    "class LogisticRegression(nn.Module): \n",
    "    def __init__(self, input_size, num_classes): \n",
    "        super(LogisticRegression, self).__init__() \n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "  \n",
    "    def forward(self, x): \n",
    "        out = self.linear(x) \n",
    "        return out \n",
    "  \n",
    "  \n",
    "model = LogisticRegression(input_size, num_classes) \n",
    "  \n",
    "# Loss and Optimizer \n",
    "# loss는 cross entropy로, optimizer는 stochastic gradient descent로\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) \n",
    "  \n",
    "# Training the Model \n",
    "for epoch in range(num_epochs): \n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        # 차원과 데이터 구조를 맞춰줌\n",
    "        images = Variable(images.view(-1, 28 * 28)) \n",
    "        labels = Variable(labels) \n",
    "  \n",
    "        # Forward + Backward + Optimize \n",
    "        # gradient 0으로 초기화 후 로지스틱으로 forward 진행\n",
    "        # 진행 결과로 loss계산(cross entropy) 후 back propagation으로 나온 weight update\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(images) \n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "  \n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.data)) \n",
    "\n",
    "# Test the Model\n",
    "# 아껴놓은 test data로 정확도 측정\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) \n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs.data, 1) \n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print('Accuracy of the model on the 10000 test images: % d %%' \n",
    "      % (100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.833, b: 0.942 Cost: 0.463185\n",
      "Epoch  100/1000 W: 0.706, b: 0.669 Cost: 0.064433\n",
      "Epoch  200/1000 W: 0.769, b: 0.526 Cost: 0.039815\n",
      "Epoch  300/1000 W: 0.818, b: 0.413 Cost: 0.024604\n",
      "Epoch  400/1000 W: 0.857, b: 0.325 Cost: 0.015203\n",
      "Epoch  500/1000 W: 0.888, b: 0.255 Cost: 0.009395\n",
      "Epoch  600/1000 W: 0.912, b: 0.201 Cost: 0.005805\n",
      "Epoch  700/1000 W: 0.931, b: 0.158 Cost: 0.003587\n",
      "Epoch  800/1000 W: 0.945, b: 0.124 Cost: 0.002217\n",
      "Epoch  900/1000 W: 0.957, b: 0.097 Cost: 0.001370\n",
      "Epoch 1000/1000 W: 0.966, b: 0.077 Cost: 0.000846\n"
     ]
    }
   ],
   "source": [
    "# Q4-1) 2-3 In[14] 코드에서 optim.SGD를 사용하지않고 코드를 짠다면 어떤 방식으로 짜야할까요? 설명해주세요.\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# SGD 사용 시 코드\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.000, b: 0.000 Cost: 4.666667\n",
      "Epoch  100/1000 W: 0.887, b: 0.257 Cost: 0.009462\n",
      "Epoch  200/1000 W: 0.921, b: 0.179 Cost: 0.004594\n",
      "Epoch  300/1000 W: 0.945, b: 0.125 Cost: 0.002231\n",
      "Epoch  400/1000 W: 0.962, b: 0.087 Cost: 0.001083\n",
      "Epoch  500/1000 W: 0.973, b: 0.061 Cost: 0.000526\n",
      "Epoch  600/1000 W: 0.981, b: 0.042 Cost: 0.000255\n",
      "Epoch  700/1000 W: 0.987, b: 0.029 Cost: 0.000124\n",
      "Epoch  800/1000 W: 0.991, b: 0.020 Cost: 0.000060\n",
      "Epoch  900/1000 W: 0.994, b: 0.014 Cost: 0.000029\n",
      "Epoch 1000/1000 W: 0.996, b: 0.010 Cost: 0.000014\n"
     ]
    }
   ],
   "source": [
    "#SGD 미사용 시 코드\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "# optimizer 설정\n",
    "W = torch.zeros(1)\n",
    "b = torch.zeros(1)\n",
    "lr = 0.01\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = (x_train * W) + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    # SGD 대신 일일히 미분한 식을 코드로 쳐야 함\n",
    "    gradient = torch.sum((W * x_train + b - y_train) * x_train)\n",
    "    b_new = torch.sum((W * x_train + b - y_train ))\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))\n",
    "    W -= lr * gradient\n",
    "    b -= lr * b_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.611, b: 0.114 Cost: 14.385770\n",
      "Epoch  100/1000 W: 0.992, b: 0.037 Cost: 0.000443\n",
      "Epoch  200/1000 W: 1.000, b: 0.001 Cost: 0.000000\n",
      "Epoch  300/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  400/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  500/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  600/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  700/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  800/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  900/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.000, b: 0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Q4-2) Gradient descent 코드를 구현하는 문제입니다. \n",
    "# 2-3 In[14]의 코드에서 다른 optimizer(3개 ex)adam, rmsprop, sgd에 momentum 추가 등등)를 이용하여 결과값을 비교해주세요.\n",
    "\n",
    "# 1. SGD with momentum\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)# 보통 0.9를 사용한다고 함\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.266, b: 0.883 Cost: 1.039017\n",
      "Epoch  100/1000 W: 0.976, b: 0.055 Cost: 0.000460\n",
      "Epoch  200/1000 W: 0.999, b: 0.002 Cost: 0.000000\n",
      "Epoch  300/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  400/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  500/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  600/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  700/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  800/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  900/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.000, b: 0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 2. NAG(Nesterov Accelerated Gradient)\n",
    "# momentum 먼저, 그 다음 gradient\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.207, b: -0.285 Cost: 4.043956\n",
      "Epoch  100/1000 W: 0.373, b: -0.119 Cost: 2.155156\n",
      "Epoch  200/1000 W: 0.443, b: -0.049 Cost: 1.561658\n",
      "Epoch  300/1000 W: 0.494, b: 0.001 Cost: 1.194696\n",
      "Epoch  400/1000 W: 0.534, b: 0.041 Cost: 0.939014\n",
      "Epoch  500/1000 W: 0.568, b: 0.074 Cost: 0.750476\n",
      "Epoch  600/1000 W: 0.597, b: 0.102 Cost: 0.606754\n",
      "Epoch  700/1000 W: 0.621, b: 0.126 Cost: 0.494798\n",
      "Epoch  800/1000 W: 0.643, b: 0.147 Cost: 0.406261\n",
      "Epoch  900/1000 W: 0.663, b: 0.166 Cost: 0.335472\n",
      "Epoch 1000/1000 W: 0.680, b: 0.182 Cost: 0.278406\n"
     ]
    }
   ],
   "source": [
    "# 3. Adagrad\n",
    "# step size를 조정해서 많이 이동하는 변수는 step 작게, 조금 이동하는 변수는 step 크게 변화\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
