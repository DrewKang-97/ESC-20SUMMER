{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 경우에서는 threshold가 0.3으로 낮은 편이다. 그렇다면 Acceptance rate 즉 가입이 쉬운 편이므로 FP가 FN보다 높다. Type1 에러(제 1종 오류)는 실제로 참이지만 거짓이라고 판명하는 경우이므로 FN이고, Type2 에러(제 2종 오류)는 실제로 거짓이지만 참이라고 판명하는 것이므로 FP에 대응한다. 즉, Type 2에러가 Type 1 에러보다 높다고 할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2-1) \n",
    "\n",
    "Accuracy = (TP + TN )/(TP+ FN+ FP+ TN) #정확도\n",
    "\n",
    "Precision = TP/ (TP+FP) #True라고 분류한 것 중 실제 True 비율\n",
    "\n",
    "Recall = TP/(TP+FN) #실제 True인 것 중에서 True라고 예측한 비율\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2-2) \n",
    "Precision and recall 에 관련된 예시를 시험 입시라고 생각했다.모든 대학은 한 시험에서 (찍어서) 운이 좋아 시험을 잘 본 학생이 아닌 실력이 뛰어난 학생이 입학하길 원할 것이다.\n",
    "\n",
    "Precision = 원래 공부를 못했음에도 불구하고 시험을 잘봐 입학했다고 예측한 수험생들 중 실제로 공부를 못했을 수험생의 비율\n",
    "\n",
    "Recall = 실제 운이 좋아 입학한 수험생 중 운이 좋았다 예측된 수험생의 비율\n",
    "\n",
    "여기서 성공확률의 threshold를 높인다는 것은 입시기관에서 실제로 우수한 학생들을 뽑을 확률의 threshold를 높인다는 것이다. 이 예시에서는 면접이든 뭐든 활용해서 threshold를 높이는 것이 더 합리적일 것이라 본다. 물론 많은 시험은 상대적으로 이루어지기 때문에 그 점을 고려해야하고 각자 생각차이가 존재하겠지만...?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#메모리에 데이터셋 불러와주기\n",
    "\n",
    "\n",
    "# MNIST Dataset (Images and Labels) \n",
    "train_dataset = dsets.MNIST(root ='./data',  \n",
    "                            train = True,  \n",
    "                            transform = transforms.ToTensor(), \n",
    "                            download = True) \n",
    "  \n",
    "test_dataset = dsets.MNIST(root ='./data',  \n",
    "                           train = False,  \n",
    "                           transform = transforms.ToTensor()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyper Parameters  \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset Loader(Input Pipline)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,  \n",
    "                                           batch_size = batch_size,  \n",
    "                                           shuffle = True) \n",
    "  \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,  \n",
    "                                          batch_size = batch_size,  \n",
    "                                          shuffle = False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "\n",
    "class LogisticRegression(nn.Module): \n",
    "    def __init__(self, input_size, num_classes): \n",
    "        super(LogisticRegression, self).__init__() \n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "  \n",
    "    def forward(self, x): \n",
    "        out = self.linear(x) \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_size, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function -  cross entropy loss 사용\n",
    "#optimizer - SGD 사용\n",
    "#learning rate (=alpha)는 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 1.9929\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 1.8584\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 1.7642\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.7507\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.6557\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.6578\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.5543\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.5827\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.4718\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.4084\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.4228\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.4245\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.3212\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.1854\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.1924\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.1822\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.2177\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.0969\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.1296\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.0165\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.1282\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.1000\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.1559\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 0.9716\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 0.9959\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 1.1142\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 1.0692\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 0.9778\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 1.0410\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 0.8963\n"
     ]
    }
   ],
   "source": [
    "# Training the Model \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        images = Variable(images.view(-1, 28 * 28)) \n",
    "        labels = Variable(labels) \n",
    "  \n",
    "        # Forward + Backward + Optimize, #1.GD 0으로\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(images) \n",
    "        loss = criterion(outputs, labels) #calculate loss\n",
    "        loss.backward() #back propagation\n",
    "        optimizer.step() \n",
    "  \n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.data)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images:  83 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/distiller/project/conda/conda-bld/pytorch_1591914925853/work/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "# Test the Model \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) \n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs.data, 1) \n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum() \n",
    "  \n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % ( \n",
    "            100 * correct / total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgrad:  1.0 2.0 -2.0\n",
      "\tgrad:  2.0 4.0 -7.84\n",
      "\tgrad:  3.0 6.0 -16.2288\n",
      "progres: 0 w= 1.260688 loss= 5.667029348257381\n",
      "\tgrad:  1.0 2.0 -1.478624\n",
      "\tgrad:  2.0 4.0 -5.796206079999999\n",
      "\tgrad:  3.0 6.0 -11.998146585599997\n",
      "progres: 1 w= 1.453417766656 loss= 0.4433467609903525\n",
      "\tgrad:  1.0 2.0 -1.093164466688\n",
      "\tgrad:  2.0 4.0 -4.285204709416961\n",
      "\tgrad:  3.0 6.0 -8.87037374849311\n",
      "progres: 2 w= 1.5959051959019805 loss= 0.6641739962635521\n",
      "\tgrad:  1.0 2.0 -0.8081896081960389\n",
      "\tgrad:  2.0 4.0 -3.1681032641284723\n",
      "\tgrad:  3.0 6.0 -6.557973756745939\n",
      "progres: 3 w= 1.701247862192685 loss= 4.105610613546384\n",
      "\tgrad:  1.0 2.0 -0.59750427561463\n",
      "\tgrad:  2.0 4.0 -2.3422167604093502\n",
      "\tgrad:  3.0 6.0 -4.848388694047353\n",
      "progres: 4 w= 1.7791289594933983 loss= 8.912595853376837\n",
      "\tgrad:  1.0 2.0 -0.44174208101320334\n",
      "\tgrad:  2.0 4.0 -1.7316289575717576\n",
      "\tgrad:  3.0 6.0 -3.584471942173538\n",
      "progres: 5 w= 1.836707389300983 loss= 13.907820705681385\n",
      "\tgrad:  1.0 2.0 -0.3265852213980338\n",
      "\tgrad:  2.0 4.0 -1.2802140678802925\n",
      "\tgrad:  3.0 6.0 -2.650043120512205\n",
      "progres: 6 w= 1.8792758133988885 loss= 18.47563950447109\n",
      "\tgrad:  1.0 2.0 -0.241448373202223\n",
      "\tgrad:  2.0 4.0 -0.946477622952715\n",
      "\tgrad:  3.0 6.0 -1.9592086795121197\n",
      "progres: 7 w= 1.910747160155559 loss= 22.367248333247705\n",
      "\tgrad:  1.0 2.0 -0.17850567968888198\n",
      "\tgrad:  2.0 4.0 -0.6997422643804168\n",
      "\tgrad:  3.0 6.0 -1.4484664872674653\n",
      "progres: 8 w= 1.9340143044689266 loss= 25.540713634212235\n",
      "\tgrad:  1.0 2.0 -0.13197139106214673\n",
      "\tgrad:  2.0 4.0 -0.5173278529636143\n",
      "\tgrad:  3.0 6.0 -1.0708686556346834\n",
      "progres: 9 w= 1.9512159834655312 loss= 28.055091357937002\n",
      "\tgrad:  1.0 2.0 -0.09756803306893769\n",
      "\tgrad:  2.0 4.0 -0.38246668963023644\n",
      "\tgrad:  3.0 6.0 -0.7917060475345892\n",
      "progres: 10 w= 1.9639333911678687 loss= 30.008480250939336\n",
      "\tgrad:  1.0 2.0 -0.07213321766426262\n",
      "\tgrad:  2.0 4.0 -0.2827622132439096\n",
      "\tgrad:  3.0 6.0 -0.5853177814148953\n",
      "progres: 11 w= 1.9733355232910992 loss= 31.505323770447514\n",
      "\tgrad:  1.0 2.0 -0.05332895341780164\n",
      "\tgrad:  2.0 4.0 -0.2090494973977819\n",
      "\tgrad:  3.0 6.0 -0.4327324596134101\n",
      "progres: 12 w= 1.9802866323953892 loss= 32.64117480869805\n",
      "\tgrad:  1.0 2.0 -0.039426735209221686\n",
      "\tgrad:  2.0 4.0 -0.15455280202014876\n",
      "\tgrad:  3.0 6.0 -0.3199243001817109\n",
      "progres: 13 w= 1.9854256707695 loss= 33.497064217083185\n",
      "\tgrad:  1.0 2.0 -0.02914865846100012\n",
      "\tgrad:  2.0 4.0 -0.11426274116712065\n",
      "\tgrad:  3.0 6.0 -0.2365238742159388\n",
      "progres: 14 w= 1.9892250235079405 loss= 34.138725662922845\n",
      "\tgrad:  1.0 2.0 -0.021549952984118992\n",
      "\tgrad:  2.0 4.0 -0.08447581569774698\n",
      "\tgrad:  3.0 6.0 -0.17486493849433593\n",
      "progres: 15 w= 1.9920339305797026 loss= 34.61800219722315\n",
      "\tgrad:  1.0 2.0 -0.015932138840594856\n",
      "\tgrad:  2.0 4.0 -0.062453984255132156\n",
      "\tgrad:  3.0 6.0 -0.12927974740812687\n",
      "progres: 16 w= 1.994110589284741 loss= 34.97502051016362\n",
      "\tgrad:  1.0 2.0 -0.011778821430517894\n",
      "\tgrad:  2.0 4.0 -0.046172980007630926\n",
      "\tgrad:  3.0 6.0 -0.09557806861579543\n",
      "progres: 17 w= 1.9956458879852805 loss= 35.240439771837835\n",
      "\tgrad:  1.0 2.0 -0.008708224029438938\n",
      "\tgrad:  2.0 4.0 -0.03413623819540135\n",
      "\tgrad:  3.0 6.0 -0.07066201306448505\n",
      "progres: 18 w= 1.9967809527381737 loss= 35.437473497376494\n",
      "\tgrad:  1.0 2.0 -0.006438094523652627\n",
      "\tgrad:  2.0 4.0 -0.02523733053271826\n",
      "\tgrad:  3.0 6.0 -0.052241274202728505\n",
      "progres: 19 w= 1.9976201197307648 loss= 35.58358424169024\n",
      "\tgrad:  1.0 2.0 -0.004759760538470381\n",
      "\tgrad:  2.0 4.0 -0.01865826131080439\n",
      "\tgrad:  3.0 6.0 -0.03862260091336722\n",
      "progres: 20 w= 1.998240525958391 loss= 35.691847206772984\n",
      "\tgrad:  1.0 2.0 -0.0035189480832178432\n",
      "\tgrad:  2.0 4.0 -0.01379427648621423\n",
      "\tgrad:  3.0 6.0 -0.028554152326460525\n",
      "progres: 21 w= 1.99869919972735 loss= 35.77201946047429\n",
      "\tgrad:  1.0 2.0 -0.002601600545300009\n",
      "\tgrad:  2.0 4.0 -0.01019827413757568\n",
      "\tgrad:  3.0 6.0 -0.021110427464781978\n",
      "progres: 22 w= 1.9990383027488265 loss= 35.8313640475677\n",
      "\tgrad:  1.0 2.0 -0.001923394502346909\n",
      "\tgrad:  2.0 4.0 -0.007539706449199102\n",
      "\tgrad:  3.0 6.0 -0.01560719234984198\n",
      "progres: 23 w= 1.9992890056818404 loss= 35.875277738968286\n",
      "\tgrad:  1.0 2.0 -0.0014219886363191492\n",
      "\tgrad:  2.0 4.0 -0.005574195454370212\n",
      "\tgrad:  3.0 6.0 -0.011538584590544687\n",
      "progres: 24 w= 1.999474353368653 loss= 35.907765270385944\n",
      "\tgrad:  1.0 2.0 -0.0010512932626940419\n",
      "\tgrad:  2.0 4.0 -0.004121069589761106\n",
      "\tgrad:  3.0 6.0 -0.008530614050808794\n",
      "progres: 25 w= 1.9996113831376856 loss= 35.931795508472256\n",
      "\tgrad:  1.0 2.0 -0.0007772337246287897\n",
      "\tgrad:  2.0 4.0 -0.0030467562005451754\n",
      "\tgrad:  3.0 6.0 -0.006306785335127074\n",
      "progres: 26 w= 1.9997126908902887 loss= 35.94956781174506\n",
      "\tgrad:  1.0 2.0 -0.0005746182194226179\n",
      "\tgrad:  2.0 4.0 -0.002252503420136165\n",
      "\tgrad:  3.0 6.0 -0.00466268207967957\n",
      "progres: 27 w= 1.9997875889274812 loss= 35.96271062022456\n",
      "\tgrad:  1.0 2.0 -0.0004248221450375844\n",
      "\tgrad:  2.0 4.0 -0.0016653028085471533\n",
      "\tgrad:  3.0 6.0 -0.0034471768136938863\n",
      "progres: 28 w= 1.9998429619451539 loss= 35.97242918666696\n",
      "\tgrad:  1.0 2.0 -0.00031407610969225175\n",
      "\tgrad:  2.0 4.0 -0.0012311783499932005\n",
      "\tgrad:  3.0 6.0 -0.0025485391844828342\n",
      "progres: 29 w= 1.9998838998815958 loss= 35.97961529468198\n",
      "\tgrad:  1.0 2.0 -0.00023220023680847746\n",
      "\tgrad:  2.0 4.0 -0.0009102249282886277\n",
      "\tgrad:  3.0 6.0 -0.0018841656015560204\n",
      "progres: 30 w= 1.9999141657892625 loss= 35.98492864737166\n",
      "\tgrad:  1.0 2.0 -0.00017166842147497974\n",
      "\tgrad:  2.0 4.0 -0.0006729402121816719\n",
      "\tgrad:  3.0 6.0 -0.0013929862392156878\n",
      "progres: 31 w= 1.9999365417379913 loss= 35.988857188058816\n",
      "\tgrad:  1.0 2.0 -0.0001269165240174175\n",
      "\tgrad:  2.0 4.0 -0.0004975127741477792\n",
      "\tgrad:  3.0 6.0 -0.0010298514424817995\n",
      "progres: 32 w= 1.9999530845453979 loss= 35.99176177766553\n",
      "\tgrad:  1.0 2.0 -9.383090920422887e-05\n",
      "\tgrad:  2.0 4.0 -0.00036781716408107457\n",
      "\tgrad:  3.0 6.0 -0.0007613815296476645\n",
      "progres: 33 w= 1.9999653148414271 loss= 35.993909269813955\n",
      "\tgrad:  1.0 2.0 -6.937031714571162e-05\n",
      "\tgrad:  2.0 4.0 -0.0002719316432120422\n",
      "\tgrad:  3.0 6.0 -0.0005628985014531906\n",
      "progres: 34 w= 1.999974356846045 loss= 35.99549698801659\n",
      "\tgrad:  1.0 2.0 -5.1286307909848006e-05\n",
      "\tgrad:  2.0 4.0 -0.00020104232700646207\n",
      "\tgrad:  3.0 6.0 -0.0004161576169003922\n",
      "progres: 35 w= 1.9999810417085633 loss= 35.996670835279\n",
      "\tgrad:  1.0 2.0 -3.7916582873442906e-05\n",
      "\tgrad:  2.0 4.0 -0.0001486330048638962\n",
      "\tgrad:  3.0 6.0 -0.0003076703200690645\n",
      "progres: 36 w= 1.9999859839076413 loss= 35.997538690028556\n",
      "\tgrad:  1.0 2.0 -2.8032184717474706e-05\n",
      "\tgrad:  2.0 4.0 -0.0001098861640933535\n",
      "\tgrad:  3.0 6.0 -0.00022746435967313516\n",
      "progres: 37 w= 1.9999896377347262 loss= 35.99818031386693\n",
      "\tgrad:  1.0 2.0 -2.0724530547688857e-05\n",
      "\tgrad:  2.0 4.0 -8.124015974608767e-05\n",
      "\tgrad:  3.0 6.0 -0.00016816713067413502\n",
      "progres: 38 w= 1.999992339052936 loss= 35.99865467866577\n",
      "\tgrad:  1.0 2.0 -1.5321894128117464e-05\n",
      "\tgrad:  2.0 4.0 -6.006182498197177e-05\n",
      "\tgrad:  3.0 6.0 -0.00012432797771566584\n",
      "progres: 39 w= 1.9999943361699042 loss= 35.999005384765724\n",
      "\tgrad:  1.0 2.0 -1.1327660191629008e-05\n",
      "\tgrad:  2.0 4.0 -4.4404427951505454e-05\n",
      "\tgrad:  3.0 6.0 -9.191716585732479e-05\n",
      "progres: 40 w= 1.9999958126624442 loss= 35.99926466736689\n",
      "\tgrad:  1.0 2.0 -8.37467511161094e-06\n",
      "\tgrad:  2.0 4.0 -3.282872643772805e-05\n",
      "\tgrad:  3.0 6.0 -6.795546372551087e-05\n",
      "progres: 41 w= 1.999996904251097 loss= 35.99945635885572\n",
      "\tgrad:  1.0 2.0 -6.191497806007362e-06\n",
      "\tgrad:  2.0 4.0 -2.4270671399762023e-05\n",
      "\tgrad:  3.0 6.0 -5.0240289795056015e-05\n",
      "progres: 42 w= 1.999997711275687 loss= 35.99959807908391\n",
      "\tgrad:  1.0 2.0 -4.5774486259198e-06\n",
      "\tgrad:  2.0 4.0 -1.794359861406747e-05\n",
      "\tgrad:  3.0 6.0 -3.714324913239864e-05\n",
      "progres: 43 w= 1.9999983079186507 loss= 35.99970285477339\n",
      "\tgrad:  1.0 2.0 -3.3841626985164908e-06\n",
      "\tgrad:  2.0 4.0 -1.326591777761621e-05\n",
      "\tgrad:  3.0 6.0 -2.7460449796734565e-05\n",
      "progres: 44 w= 1.9999987490239537 loss= 35.99978031682054\n",
      "\tgrad:  1.0 2.0 -2.5019520926150562e-06\n",
      "\tgrad:  2.0 4.0 -9.807652203264183e-06\n",
      "\tgrad:  3.0 6.0 -2.0301840059744336e-05\n",
      "progres: 45 w= 1.9999990751383971 loss= 35.999837585508494\n",
      "\tgrad:  1.0 2.0 -1.8497232057157476e-06\n",
      "\tgrad:  2.0 4.0 -7.250914967116273e-06\n",
      "\tgrad:  3.0 6.0 -1.5009393983689279e-05\n",
      "progres: 46 w= 1.9999993162387186 loss= 35.99987992497328\n",
      "\tgrad:  1.0 2.0 -1.3675225627451937e-06\n",
      "\tgrad:  2.0 4.0 -5.3606884460322135e-06\n",
      "\tgrad:  3.0 6.0 -1.109662508014253e-05\n",
      "progres: 47 w= 1.9999994944870796 loss= 35.99991122706775\n",
      "\tgrad:  1.0 2.0 -1.0110258408246864e-06\n",
      "\tgrad:  2.0 4.0 -3.963221296032771e-06\n",
      "\tgrad:  3.0 6.0 -8.20386808086937e-06\n",
      "progres: 48 w= 1.9999996262682318 loss= 35.99993436909275\n",
      "\tgrad:  1.0 2.0 -7.474635363990956e-07\n",
      "\tgrad:  2.0 4.0 -2.930057062755509e-06\n",
      "\tgrad:  3.0 6.0 -6.065218119744031e-06\n",
      "progres: 49 w= 1.999999723695619 loss= 35.999951478275484\n",
      "\tgrad:  1.0 2.0 -5.526087618612507e-07\n",
      "\tgrad:  2.0 4.0 -2.166226346744793e-06\n",
      "\tgrad:  3.0 6.0 -4.484088535150477e-06\n",
      "progres: 50 w= 1.9999997957248556 loss= 35.999964127302896\n",
      "\tgrad:  1.0 2.0 -4.08550288710785e-07\n",
      "\tgrad:  2.0 4.0 -1.6015171322436572e-06\n",
      "\tgrad:  3.0 6.0 -3.3151404608133817e-06\n",
      "progres: 51 w= 1.9999998489769344 loss= 35.99997347888241\n",
      "\tgrad:  1.0 2.0 -3.020461312175371e-07\n",
      "\tgrad:  2.0 4.0 -1.1840208351543424e-06\n",
      "\tgrad:  3.0 6.0 -2.4509231284497446e-06\n",
      "progres: 52 w= 1.9999998883468353 loss= 35.999980392618305\n",
      "\tgrad:  1.0 2.0 -2.2330632942768602e-07\n",
      "\tgrad:  2.0 4.0 -8.753608113920563e-07\n",
      "\tgrad:  3.0 6.0 -1.811996877876254e-06\n",
      "progres: 53 w= 1.9999999174534755 loss= 35.999985504026796\n",
      "\tgrad:  1.0 2.0 -1.6509304900935717e-07\n",
      "\tgrad:  2.0 4.0 -6.471647520100987e-07\n",
      "\tgrad:  3.0 6.0 -1.3396310407642886e-06\n",
      "progres: 54 w= 1.999999938972364 loss= 35.99998928295268\n",
      "\tgrad:  1.0 2.0 -1.220552721115098e-07\n",
      "\tgrad:  2.0 4.0 -4.784566662863199e-07\n",
      "\tgrad:  3.0 6.0 -9.904052991061008e-07\n",
      "progres: 55 w= 1.9999999548815364 loss= 35.99999207675815\n",
      "\tgrad:  1.0 2.0 -9.023692726373156e-08\n",
      "\tgrad:  2.0 4.0 -3.5372875473171916e-07\n",
      "\tgrad:  3.0 6.0 -7.322185204827747e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progres: 56 w= 1.9999999666433785 loss= 35.99999414225214\n",
      "\tgrad:  1.0 2.0 -6.671324292994996e-08\n",
      "\tgrad:  2.0 4.0 -2.615159129248923e-07\n",
      "\tgrad:  3.0 6.0 -5.413379398078177e-07\n",
      "progres: 57 w= 1.9999999753390494 loss= 35.99999566929665\n",
      "\tgrad:  1.0 2.0 -4.932190122985958e-08\n",
      "\tgrad:  2.0 4.0 -1.9334185274999527e-07\n",
      "\tgrad:  3.0 6.0 -4.002176350326181e-07\n",
      "progres: 58 w= 1.9999999817678633 loss= 35.99999679825901\n",
      "\tgrad:  1.0 2.0 -3.6464273378555845e-08\n",
      "\tgrad:  2.0 4.0 -1.429399514307761e-07\n",
      "\tgrad:  3.0 6.0 -2.9588569994132286e-07\n",
      "progres: 59 w= 1.9999999865207625 loss= 35.999997632914436\n",
      "\tgrad:  1.0 2.0 -2.6958475007887728e-08\n",
      "\tgrad:  2.0 4.0 -1.0567722164012139e-07\n",
      "\tgrad:  3.0 6.0 -2.1875184863517916e-07\n",
      "progres: 60 w= 1.999999990034638 loss= 35.99999824998523\n",
      "\tgrad:  1.0 2.0 -1.993072418216002e-08\n",
      "\tgrad:  2.0 4.0 -7.812843882959442e-08\n",
      "\tgrad:  3.0 6.0 -1.617258700292723e-07\n",
      "progres: 61 w= 1.9999999926324883 loss= 35.999998706193054\n",
      "\tgrad:  1.0 2.0 -1.473502342363986e-08\n",
      "\tgrad:  2.0 4.0 -5.7761292637792394e-08\n",
      "\tgrad:  3.0 6.0 -1.195658771990793e-07\n",
      "progres: 62 w= 1.99999999455311 loss= 35.99999904347299\n",
      "\tgrad:  1.0 2.0 -1.0893780100218464e-08\n",
      "\tgrad:  2.0 4.0 -4.270361841918202e-08\n",
      "\tgrad:  3.0 6.0 -8.839649012770678e-08\n",
      "progres: 63 w= 1.9999999959730488 loss= 35.99999929282808\n",
      "\tgrad:  1.0 2.0 -8.05390243385773e-09\n",
      "\tgrad:  2.0 4.0 -3.1571296688071016e-08\n",
      "\tgrad:  3.0 6.0 -6.53525820126788e-08\n",
      "progres: 64 w= 1.9999999970228268 loss= 35.999999477179344\n",
      "\tgrad:  1.0 2.0 -5.9543463493128e-09\n",
      "\tgrad:  2.0 4.0 -2.334103754719763e-08\n",
      "\tgrad:  3.0 6.0 -4.8315948575350376e-08\n",
      "progres: 65 w= 1.9999999977989402 loss= 35.99999961347241\n",
      "\tgrad:  1.0 2.0 -4.402119557767037e-09\n",
      "\tgrad:  2.0 4.0 -1.725630838222969e-08\n",
      "\tgrad:  3.0 6.0 -3.5720557178819945e-08\n",
      "progres: 66 w= 1.9999999983727301 loss= 35.99999971423554\n",
      "\tgrad:  1.0 2.0 -3.254539748809293e-09\n",
      "\tgrad:  2.0 4.0 -1.2757796596929438e-08\n",
      "\tgrad:  3.0 6.0 -2.6408640607655798e-08\n",
      "progres: 67 w= 1.9999999987969397 loss= 35.999999788730875\n",
      "\tgrad:  1.0 2.0 -2.406120636067044e-09\n",
      "\tgrad:  2.0 4.0 -9.431992964437086e-09\n",
      "\tgrad:  3.0 6.0 -1.9524227568012975e-08\n",
      "progres: 68 w= 1.999999999110563 loss= 35.99999984380618\n",
      "\tgrad:  1.0 2.0 -1.7788739370416806e-09\n",
      "\tgrad:  2.0 4.0 -6.97318647269185e-09\n",
      "\tgrad:  3.0 6.0 -1.4434496264925656e-08\n",
      "progres: 69 w= 1.9999999993424284 loss= 35.99999988452403\n",
      "\tgrad:  1.0 2.0 -1.3151431055291596e-09\n",
      "\tgrad:  2.0 4.0 -5.155360582875801e-09\n",
      "\tgrad:  3.0 6.0 -1.067159693945996e-08\n",
      "progres: 70 w= 1.9999999995138495 loss= 35.999999914627224\n",
      "\tgrad:  1.0 2.0 -9.72300906454393e-10\n",
      "\tgrad:  2.0 4.0 -3.811418736177075e-09\n",
      "\tgrad:  3.0 6.0 -7.88963561149103e-09\n",
      "progres: 71 w= 1.9999999996405833 loss= 35.999999936882915\n",
      "\tgrad:  1.0 2.0 -7.18833437218791e-10\n",
      "\tgrad:  2.0 4.0 -2.8178277489132597e-09\n",
      "\tgrad:  3.0 6.0 -5.832902161273523e-09\n",
      "progres: 72 w= 1.999999999734279 loss= 35.99999995333678\n",
      "\tgrad:  1.0 2.0 -5.314420015167798e-10\n",
      "\tgrad:  2.0 4.0 -2.0832526814729135e-09\n",
      "\tgrad:  3.0 6.0 -4.31233715403323e-09\n",
      "progres: 73 w= 1.9999999998035491 loss= 35.999999965501324\n",
      "\tgrad:  1.0 2.0 -3.92901711165905e-10\n",
      "\tgrad:  2.0 4.0 -1.5401742103904326e-09\n",
      "\tgrad:  3.0 6.0 -3.188159070077745e-09\n",
      "progres: 74 w= 1.9999999998547615 loss= 35.99999997449473\n",
      "\tgrad:  1.0 2.0 -2.9047697580608656e-10\n",
      "\tgrad:  2.0 4.0 -1.1386696030513122e-09\n",
      "\tgrad:  3.0 6.0 -2.3570478902001923e-09\n",
      "progres: 75 w= 1.9999999998926234 loss= 35.99999998114362\n",
      "\tgrad:  1.0 2.0 -2.1475310418850313e-10\n",
      "\tgrad:  2.0 4.0 -8.418314934033333e-10\n",
      "\tgrad:  3.0 6.0 -1.7425900722400911e-09\n",
      "progres: 76 w= 1.9999999999206153 loss= 35.99999998605928\n",
      "\tgrad:  1.0 2.0 -1.5876944203796484e-10\n",
      "\tgrad:  2.0 4.0 -6.223768167501476e-10\n",
      "\tgrad:  3.0 6.0 -1.2883241140571045e-09\n",
      "progres: 77 w= 1.9999999999413098 loss= 35.99999998969343\n",
      "\tgrad:  1.0 2.0 -1.17380327679939e-10\n",
      "\tgrad:  2.0 4.0 -4.601314884666863e-10\n",
      "\tgrad:  3.0 6.0 -9.524754318590567e-10\n",
      "progres: 78 w= 1.9999999999566096 loss= 35.9999999923802\n",
      "\tgrad:  1.0 2.0 -8.678080476443029e-11\n",
      "\tgrad:  2.0 4.0 -3.4018121652934497e-10\n",
      "\tgrad:  3.0 6.0 -7.041780492045291e-10\n",
      "progres: 79 w= 1.9999999999679208 loss= 35.999999994366576\n",
      "\tgrad:  1.0 2.0 -6.415845632545825e-11\n",
      "\tgrad:  2.0 4.0 -2.5150193039280566e-10\n",
      "\tgrad:  3.0 6.0 -5.206075570640678e-10\n",
      "progres: 80 w= 1.9999999999762834 loss= 35.99999999583514\n",
      "\tgrad:  1.0 2.0 -4.743316850408519e-11\n",
      "\tgrad:  2.0 4.0 -1.8593837580738182e-10\n",
      "\tgrad:  3.0 6.0 -3.8489211817704927e-10\n",
      "progres: 81 w= 1.999999999982466 loss= 35.99999999692086\n",
      "\tgrad:  1.0 2.0 -3.5067948545020045e-11\n",
      "\tgrad:  2.0 4.0 -1.3746692673066718e-10\n",
      "\tgrad:  3.0 6.0 -2.845563784603655e-10\n",
      "progres: 82 w= 1.9999999999870368 loss= 35.99999999772355\n",
      "\tgrad:  1.0 2.0 -2.5926372160256506e-11\n",
      "\tgrad:  2.0 4.0 -1.0163070385260653e-10\n",
      "\tgrad:  3.0 6.0 -2.1037571684701106e-10\n",
      "progres: 83 w= 1.999999999990416 loss= 35.999999998316994\n",
      "\tgrad:  1.0 2.0 -1.9167778475548403e-11\n",
      "\tgrad:  2.0 4.0 -7.51381179497912e-11\n",
      "\tgrad:  3.0 6.0 -1.5553425214420713e-10\n",
      "progres: 84 w= 1.9999999999929146 loss= 35.999999998755726\n",
      "\tgrad:  1.0 2.0 -1.4170886686315498e-11\n",
      "\tgrad:  2.0 4.0 -5.555023108172463e-11\n",
      "\tgrad:  3.0 6.0 -1.1499068364173581e-10\n",
      "progres: 85 w= 1.9999999999947617 loss= 35.999999999080096\n",
      "\tgrad:  1.0 2.0 -1.0476508549572827e-11\n",
      "\tgrad:  2.0 4.0 -4.106759377009439e-11\n",
      "\tgrad:  3.0 6.0 -8.500933290633839e-11\n",
      "progres: 86 w= 1.9999999999961273 loss= 35.999999999319925\n",
      "\tgrad:  1.0 2.0 -7.745359908994942e-12\n",
      "\tgrad:  2.0 4.0 -3.036149109902908e-11\n",
      "\tgrad:  3.0 6.0 -6.285105769165966e-11\n",
      "progres: 87 w= 1.999999999997137 loss= 35.99999999949719\n",
      "\tgrad:  1.0 2.0 -5.726086271806707e-12\n",
      "\tgrad:  2.0 4.0 -2.2446045022661565e-11\n",
      "\tgrad:  3.0 6.0 -4.646416584819235e-11\n",
      "progres: 88 w= 1.9999999999978835 loss= 35.99999999962829\n",
      "\tgrad:  1.0 2.0 -4.233058348290797e-12\n",
      "\tgrad:  2.0 4.0 -1.659294923683774e-11\n",
      "\tgrad:  3.0 6.0 -3.4351188560322043e-11\n",
      "progres: 89 w= 1.9999999999984353 loss= 35.99999999972521\n",
      "\tgrad:  1.0 2.0 -3.1294966618133913e-12\n",
      "\tgrad:  2.0 4.0 -1.226752033289813e-11\n",
      "\tgrad:  3.0 6.0 -2.539835008974478e-11\n",
      "progres: 90 w= 1.9999999999988431 loss= 35.999999999796835\n",
      "\tgrad:  1.0 2.0 -2.3137047833188262e-12\n",
      "\tgrad:  2.0 4.0 -9.070078021977679e-12\n",
      "\tgrad:  3.0 6.0 -1.8779644506139448e-11\n",
      "progres: 91 w= 1.9999999999991447 loss= 35.999999999849784\n",
      "\tgrad:  1.0 2.0 -1.7106316363424412e-12\n",
      "\tgrad:  2.0 4.0 -6.7057470687359455e-12\n",
      "\tgrad:  3.0 6.0 -1.3882228699912957e-11\n",
      "progres: 92 w= 1.9999999999993676 loss= 35.99999999988894\n",
      "\tgrad:  1.0 2.0 -1.2647660696529783e-12\n",
      "\tgrad:  2.0 4.0 -4.957811938766099e-12\n",
      "\tgrad:  3.0 6.0 -1.0263789818054647e-11\n",
      "progres: 93 w= 1.9999999999995324 loss= 35.99999999991789\n",
      "\tgrad:  1.0 2.0 -9.352518759442319e-13\n",
      "\tgrad:  2.0 4.0 -3.666400516522117e-12\n",
      "\tgrad:  3.0 6.0 -7.58859641791787e-12\n",
      "progres: 94 w= 1.9999999999996543 loss= 35.99999999993929\n",
      "\tgrad:  1.0 2.0 -6.914468997365475e-13\n",
      "\tgrad:  2.0 4.0 -2.7107205369247822e-12\n",
      "\tgrad:  3.0 6.0 -5.611511255665391e-12\n",
      "progres: 95 w= 1.9999999999997444 loss= 35.99999999995511\n",
      "\tgrad:  1.0 2.0 -5.111466805374221e-13\n",
      "\tgrad:  2.0 4.0 -2.0037305148434825e-12\n",
      "\tgrad:  3.0 6.0 -4.1460168631601846e-12\n",
      "progres: 96 w= 1.999999999999811 loss= 35.99999999996683\n",
      "\tgrad:  1.0 2.0 -3.779199175824033e-13\n",
      "\tgrad:  2.0 4.0 -1.4814816040598089e-12\n",
      "\tgrad:  3.0 6.0 -3.064215547965432e-12\n",
      "progres: 97 w= 1.9999999999998603 loss= 35.999999999975486\n",
      "\tgrad:  1.0 2.0 -2.793321129956894e-13\n",
      "\tgrad:  2.0 4.0 -1.0942358130705543e-12\n",
      "\tgrad:  3.0 6.0 -2.2648549702353193e-12\n",
      "progres: 98 w= 1.9999999999998967 loss= 35.99999999998188\n",
      "\tgrad:  1.0 2.0 -2.0650148258027912e-13\n",
      "\tgrad:  2.0 4.0 -8.100187187665142e-13\n",
      "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
      "progres: 99 w= 1.9999999999999236 loss= 35.99999999998657\n",
      "\tgrad:  1.0 2.0 -1.5276668818842154e-13\n",
      "\tgrad:  2.0 4.0 -5.986322548778844e-13\n",
      "\tgrad:  3.0 6.0 -1.2363443602225743e-12\n",
      "progres: 100 w= 1.9999999999999436 loss= 35.99999999999011\n"
     ]
    }
   ],
   "source": [
    "#0. torch 등 필요한 모듈 import\n",
    "#1. 데이터셋 생성하기\n",
    "\n",
    "x_data = [1.0,2.0,3.0]\n",
    "y_data = [2.0, 4.0, 6.0] #든 뭐든 상관없지않을까요..?\n",
    "w= 1.0\n",
    "\n",
    "#2. forward pass 정의\n",
    "\n",
    "def forward(x):\n",
    "    return x*w\n",
    "\n",
    "#3. loss function 정의\n",
    "\n",
    "def loss(x,y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred -y)**2\n",
    "\n",
    "#4. gradient 정의\n",
    "\n",
    "def gradient(yx,y):\n",
    "    return 2*x*(x*w-y)\n",
    "\n",
    "#4. epoch loop\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs +1):\n",
    "    for x, y in zip(x_data, y_data): \n",
    "        y_pred = forward(x) #y 예측값\n",
    "        loss_ = loss(y_pred, y) #loss\n",
    "        grad = gradient(x,y) \n",
    "        w += -0.01 * grad #learning rate = 0.01\n",
    "        print(\"\\tgrad: \", x, y, grad)\n",
    "    print('progres:', epoch, 'w=', w, 'loss=', loss_)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.493, b: 0.810 Cost: 6.236278\n",
      "Epoch  100/1000 W: -0.395, b: 0.908 Cost: 4.856025\n",
      "Epoch  200/1000 W: -0.304, b: 0.998 Cost: 3.734399\n",
      "Epoch  300/1000 W: -0.219, b: 1.081 Cost: 2.840681\n",
      "Epoch  400/1000 W: -0.141, b: 1.156 Cost: 2.142472\n",
      "Epoch  500/1000 W: -0.070, b: 1.223 Cost: 1.608853\n",
      "Epoch  600/1000 W: -0.006, b: 1.281 Cost: 1.210767\n",
      "Epoch  700/1000 W: 0.052, b: 1.331 Cost: 0.921470\n",
      "Epoch  800/1000 W: 0.103, b: 1.372 Cost: 0.716969\n",
      "Epoch  900/1000 W: 0.147, b: 1.405 Cost: 0.576402\n",
      "Epoch 1000/1000 W: 0.186, b: 1.429 Cost: 0.482283\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor(([1],[2],[3]))\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.454, b: 0.983 Cost: 0.210358\n",
      "Epoch  100/1000 W: 0.455, b: 0.984 Cost: 0.209801\n",
      "Epoch  200/1000 W: 0.455, b: 0.984 Cost: 0.209100\n",
      "Epoch  300/1000 W: 0.456, b: 0.985 Cost: 0.208288\n",
      "Epoch  400/1000 W: 0.456, b: 0.985 Cost: 0.207385\n",
      "Epoch  500/1000 W: 0.457, b: 0.986 Cost: 0.206406\n",
      "Epoch  600/1000 W: 0.458, b: 0.987 Cost: 0.205362\n",
      "Epoch  700/1000 W: 0.459, b: 0.988 Cost: 0.204263\n",
      "Epoch  800/1000 W: 0.460, b: 0.989 Cost: 0.203117\n",
      "Epoch  900/1000 W: 0.461, b: 0.989 Cost: 0.201933\n",
      "Epoch 1000/1000 W: 0.462, b: 0.990 Cost: 0.200715\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor(([1],[2],[3]))\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "optimizer = optim.Adadelta(model.parameters(),lr=0.001)\n",
    "\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.677, b: 0.512 Cost: 10.175228\n",
      "Epoch  100/1000 W: 0.152, b: 0.847 Cost: 1.226102\n",
      "Epoch  200/1000 W: 0.430, b: 0.940 Cost: 0.258517\n",
      "Epoch  300/1000 W: 0.529, b: 0.955 Cost: 0.148623\n",
      "Epoch  400/1000 W: 0.568, b: 0.945 Cost: 0.131145\n",
      "Epoch  500/1000 W: 0.587, b: 0.926 Cost: 0.123860\n",
      "Epoch  600/1000 W: 0.600, b: 0.906 Cost: 0.117924\n",
      "Epoch  700/1000 W: 0.610, b: 0.885 Cost: 0.112375\n",
      "Epoch  800/1000 W: 0.620, b: 0.864 Cost: 0.107098\n",
      "Epoch  900/1000 W: 0.629, b: 0.843 Cost: 0.102070\n",
      "Epoch 1000/1000 W: 0.638, b: 0.823 Cost: 0.097278\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor(([1],[2],[3]))\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.001)\n",
    "\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
