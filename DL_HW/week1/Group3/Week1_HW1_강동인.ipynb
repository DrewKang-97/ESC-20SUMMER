{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.\n",
    "해당 경우는 Acceptance rate가 높아서 가입이 쉽다. \n",
    "\n",
    "Type 1 error는 FN와 관련이 있고 (가입해줘야 할 사람을 가입 거부), Type 2 error는 FP과 관계가 있다 (가입을 해주면 안될 사람을 가입해주는 것).\n",
    "\n",
    "Type 1 error = $P(reject\\ H_0 | H_0\\ is\\ true)$ <br>\n",
    "Type 2 error = $P(accept\\ H_0 | H_0 is\\ not\\ true)$\n",
    "\n",
    "Acceptance rate이 높기 때문에 type 2 error가 높을 것이라고 생각한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2-1. \n",
    "Accuracy = $\\frac{TP+TN}{TP+FN+FP+TN}$, 모델이 옳게 예측한 비율 <br>\n",
    "Precision = $\\frac{TP}{TP+FP}$, 모델이 True라고 분류한 것 중에 실제 True인 것의 비율 <br>\n",
    "Recall = $\\frac{TP}{TP+FN}$, 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2-2. \n",
    "CCTV 영상 데이터를 통해 범죄를 예측할 때는 Recall 값이 낮다면 안된다고 생각합니다. 즉, 실제 범죄가 일어났는데 잘못 예측하여 범죄가 일어나지 않았다고 한다면 큰 문제가 되기 때문입니다. \n",
    "이 경우에 threshold를 높게 설정한다면, 실제 범죄를 잘못 예측하는 경우가 생길 것이라고 생각합니다. Precision은 높지만, Recall이 낮아질 수 있다고 생각합니다. 반면 낮게 설정한다면 인력 낭비가 심해질 것 같습니다. 따라서 trade off 중에서 최적해를 찾아야 한다고 생각합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision.datasets as dsets \n",
    "import torchvision.transforms as transforms \n",
    "from torch.autograd import Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model에 사용할 변수들 값 설정\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset (Images and Labels) \n",
    "# Training data\n",
    "train_dataset = dsets.MNIST(root ='./data',  \n",
    "                            train = True,  \n",
    "                            transform = transforms.ToTensor(), \n",
    "                            download = True) \n",
    "\n",
    "# Test data \n",
    "test_dataset = dsets.MNIST(root ='./data',  \n",
    "                           train = False,  \n",
    "                           transform = transforms.ToTensor()) \n",
    "  \n",
    "# Dataset Loader (Input Pipline) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,  \n",
    "                                           batch_size = batch_size,  \n",
    "                                           shuffle = True) \n",
    "  \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,  \n",
    "                                          batch_size = batch_size,  \n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용할 함수 정의\n",
    "class LogisticRegression(nn.Module): \n",
    "    def __init__(self, input_size, num_classes): \n",
    "        super(LogisticRegression, self).__init__() \n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "  \n",
    "    def forward(self, x): \n",
    "        out = self.linear(x) \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "model = LogisticRegression(input_size, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # loss fuction으로 cross entropy 설정 \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) # optimizer로 SGD 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 2.1803\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 2.1045\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 1.9849\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.9503\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.8939\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.7511\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.6651\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.6867\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.6427\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.5426\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.5352\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.4624\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.3806\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.4145\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.3709\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.2260\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.3233\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.2020\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.2486\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.1087\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.2490\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.1402\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.0662\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 1.1137\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 1.0579\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 1.1459\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 1.0193\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 1.0009\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 1.0779\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 0.9659\n"
     ]
    }
   ],
   "source": [
    "# Training the Model \n",
    "for epoch in range(num_epochs): \n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        images = Variable(images.view(-1, 28 * 28)) # 1차원으로 차원변환\n",
    "        labels = Variable(labels) \n",
    "  \n",
    "        # Forward + Backward + Optimize \n",
    "        optimizer.zero_grad() # gradient 초기화\n",
    "        outputs = model(images) \n",
    "        loss = criterion(outputs, labels) # 상기된 output을 다시 input으로 설정\n",
    "        loss.backward() # back propagation\n",
    "        optimizer.step() \n",
    "        \n",
    "        # 진행상황\n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, \n",
    "                     len(train_dataset) // batch_size, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images:  82 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "# Test the Model \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) \n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs.data, 1) \n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum() \n",
    "  \n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % ( \n",
    "            100 * correct / total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer 함수가 아닌 스스로 중요도를 초기화하는 구문과 업데이트를 진행하는 구문을 추가해야한다. $w=w-lr*w_0,\\ where\\ w_0\\ is\\ inital\\ value\\ and\\ lr\\ is\\ learning\\ rate$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model1 = LinearRegressionModel()\n",
    "model2 = LinearRegressionModel()\n",
    "model3 = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam\n",
    "optimizer_Adam = optim.Adam(model1.parameters(), lr=0.01)\n",
    "#RMSprop\n",
    "optimizer_R = optim.RMSprop(model2.parameters(), lr=0.01)\n",
    "#Adagrad\n",
    "optimizer_Adag = optim.Adagrad(model3.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W1: 0.359, b1: 0.492 Cost1: 0.953734\n",
      "Epoch  100/1000 W1: 0.713, b1: 0.629 Cost1: 0.058427\n",
      "Epoch  200/1000 W1: 0.820, b1: 0.397 Cost1: 0.023203\n",
      "Epoch  300/1000 W1: 0.903, b1: 0.214 Cost1: 0.006757\n",
      "Epoch  400/1000 W1: 0.955, b1: 0.100 Cost1: 0.001483\n",
      "Epoch  500/1000 W1: 0.982, b1: 0.041 Cost1: 0.000248\n",
      "Epoch  600/1000 W1: 0.993, b1: 0.015 Cost1: 0.000032\n",
      "Epoch  700/1000 W1: 0.998, b1: 0.005 Cost1: 0.000003\n",
      "Epoch  800/1000 W1: 0.999, b1: 0.001 Cost1: 0.000000\n",
      "Epoch  900/1000 W1: 1.000, b1: 0.000 Cost1: 0.000000\n",
      "Epoch 1000/1000 W1: 1.000, b1: 0.000 Cost1: 0.000000\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 1000\n",
    "\n",
    "for epoch1 in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction1 = model1(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost1 = F.mse_loss(prediction1, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer_Adam.zero_grad() \n",
    "    cost1.backward()\n",
    "    optimizer_Adam.step()\n",
    "    \n",
    "    if epoch1 % 100 == 0:\n",
    "        params1 = list(model1.parameters())\n",
    "        W1 = params1[0].item()\n",
    "        b1 = params1[1].item()\n",
    "        \n",
    "        print('Epoch {:4d}/{} W1: {:.3f}, b1: {:.3f} Cost1: {:.6f}'.format(epoch1, nb_epochs, W1, b1, cost1.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W2: -0.522, b2: 0.500 Cost2: 9.844289\n",
      "Epoch  100/1000 W2: 0.422, b2: 1.244 Cost2: 0.231430\n",
      "Epoch  200/1000 W2: 0.549, b2: 0.997 Cost2: 0.145514\n",
      "Epoch  300/1000 W2: 0.689, b2: 0.689 Cost2: 0.069671\n",
      "Epoch  400/1000 W2: 0.828, b2: 0.382 Cost2: 0.021504\n",
      "Epoch  500/1000 W2: 0.933, b2: 0.148 Cost2: 0.003241\n",
      "Epoch  600/1000 W2: 0.986, b2: 0.031 Cost2: 0.000146\n",
      "Epoch  700/1000 W2: 0.999, b2: 0.002 Cost2: 0.000001\n",
      "Epoch  800/1000 W2: 1.000, b2: 0.000 Cost2: 0.000000\n",
      "Epoch  900/1000 W2: 1.012, b2: 0.012 Cost2: 0.002519\n",
      "Epoch 1000/1000 W2: 1.000, b2: 0.000 Cost2: 0.000001\n"
     ]
    }
   ],
   "source": [
    "for epoch2 in range(nb_epochs + 1):\n",
    "    prediction2 = model2(x_train)\n",
    "    \n",
    "    cost2 = F.mse_loss(prediction2, y_train)\n",
    "    \n",
    "    optimizer_R.zero_grad() \n",
    "    cost2.backward()\n",
    "    optimizer_R.step()\n",
    "    \n",
    "    if epoch2 % 100 == 0:\n",
    "        params2 = list(model2.parameters())\n",
    "        W2 = params2[0].item()\n",
    "        b2 = params2[1].item()\n",
    "        \n",
    "        print('Epoch {:4d}/{} W2: {:.3f}, b2: {:.3f} Cost2: {:.6f}'.format(epoch2, nb_epochs, W2, b2, cost2.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W3: 0.983, b3: 0.193 Cost3: 0.035583\n",
      "Epoch  100/1000 W3: 0.948, b3: 0.117 Cost3: 0.001984\n",
      "Epoch  200/1000 W3: 0.959, b3: 0.092 Cost3: 0.001235\n",
      "Epoch  300/1000 W3: 0.967, b3: 0.073 Cost3: 0.000775\n",
      "Epoch  400/1000 W3: 0.974, b3: 0.058 Cost3: 0.000490\n",
      "Epoch  500/1000 W3: 0.979, b3: 0.046 Cost3: 0.000310\n",
      "Epoch  600/1000 W3: 0.984, b3: 0.037 Cost3: 0.000197\n",
      "Epoch  700/1000 W3: 0.987, b3: 0.029 Cost3: 0.000125\n",
      "Epoch  800/1000 W3: 0.990, b3: 0.023 Cost3: 0.000080\n",
      "Epoch  900/1000 W3: 0.992, b3: 0.019 Cost3: 0.000051\n",
      "Epoch 1000/1000 W3: 0.993, b3: 0.015 Cost3: 0.000032\n"
     ]
    }
   ],
   "source": [
    "for epoch3 in range(nb_epochs + 1):    \n",
    "    prediction3 = model3(x_train)\n",
    "\n",
    "    cost3 = F.mse_loss(prediction3, y_train)\n",
    "    \n",
    "    optimizer_Adag.zero_grad() \n",
    "    cost3.backward()\n",
    "    optimizer_Adag.step()\n",
    "\n",
    "    if epoch3 % 100 == 0:       \n",
    "        params3 = list(model3.parameters())\n",
    "        W3 = params3[0].item()\n",
    "        b3 = params3[1].item()\n",
    "        \n",
    "        print('Epoch {:4d}/{} W3: {:.3f}, b3: {:.3f} Cost3: {:.6f}'.format(epoch3, nb_epochs, W3, b3, cost3.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
