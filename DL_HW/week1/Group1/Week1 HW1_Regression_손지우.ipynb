{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week1 과제\n",
    "손지우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![으어](img/Week1_threshold.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1) 이 경우에서는 Type 1 error가 높을까요? Type 2 Error가 높을까요? FP, FN과 관련지어 설명해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "threshold가 0.3으로 낮기 때문에 acceptance할 확률이 더 높다고 볼 수 있다.  \n",
    "즉, True라고 예측할 확률이 False라고 예측할 확률보다 크다.  \n",
    "그러므로 FP(False Positive)가 높고, FN(False Negative)가 낮다.  \n",
    "Type1 Error는 귀무가설(\"음성이다\")이 참일 때, 이를 기각할 확률(\"양성이다\"라고 예측할 확률)를 뜻하므로, FP와 관련이 있다.  \n",
    "Type2 Error는 반대로 귀무가설이 거짓일 때, 이를 채택활 확률을 뜻하므로, FN과 관련이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결론적으로, Type1 Error가 더 높을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2-1) Accuracy, Precision and Recall이 무엇인지 TP,FP,FN,TN의 식으로 나타내 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Accuracy = \\frac{TP+TN}{TP+FP+FN+TN}\\\\\n",
    "Precision = \\frac{TP}{TP+FP}\\\\\n",
    "Recall = \\frac{TP}{TP+FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2-2) Precision and Recall에 관한 제가 든 예시가 아닌 실생활 예시 하나를 들어주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스팸 메일 분류기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision: 스팸 메일로 분류된 메일 중 실제 스팸 메일의 비율  \n",
    "Recall: 실제 스팸 메일 중 스팸 메일로 분류된 메일의 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 Threshold를 높인다는 것은, 스팸 메일로 분류될 비율을 줄인다는 뜻이다. 즉, Precision은 커지고 Recall은 작아질 것이다.  \n",
    "Threshold를 높이는 것이 합리적인지 비합리적인지에 대해서는 단언할 수 없다.  \n",
    "왜냐하면 Precision과 Recall이 tradeoff관계이기에, 스팸 메일을 분류하고자 하는 최종목적에 따라 달라질 것이기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3) 코드 따라해보고 주석 달기!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets \n",
    "import torchvision.transforms as transforms \n",
    "from torch.autograd import Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset (Images and Labels) \n",
    "train_dataset = dsets.MNIST(root ='./data',  \n",
    "                            train = True,  \n",
    "                            transform = transforms.ToTensor(), \n",
    "                            download = True) \n",
    "  \n",
    "test_dataset = dsets.MNIST(root ='./data',  \n",
    "                           train = False,  \n",
    "                           transform = transforms.ToTensor()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loader (Input Pipline) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,  \n",
    "                                           batch_size = batch_size,  \n",
    "                                           shuffle = True) \n",
    "  \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,  \n",
    "                                          batch_size = batch_size,  \n",
    "                                          shuffle = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module): \n",
    "    def __init__(self, input_size, num_classes): \n",
    "        super(LogisticRegression, self).__init__() \n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "\n",
    "    def forward(self, x): \n",
    "        out = self.linear(x) \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_size, num_classes) \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 0.9844\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 1.0042\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 0.9333\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 0.9437\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 0.8447\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 0.9010\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 0.8981\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.0271\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 0.8722\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 0.7967\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 0.8873\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 0.8516\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 0.7927\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 0.7349\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 0.8788\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 0.7505\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 0.8984\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 0.9149\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 0.7701\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 0.8277\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 0.7556\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 0.7990\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 0.8788\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 0.7185\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 0.7095\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 0.6838\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 0.8193\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 0.6995\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 0.7501\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 0.6972\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad() # 1. Reset all gradients to 0\n",
    "        outputs = model(images) # 2. Make a forward pass\n",
    "        loss = criterion(outputs, labels) # 3. Calculate the loss\n",
    "        loss.backward() # 4. Perform backpropagation\n",
    "        optimizer.step() # 5. Update all weights\n",
    "        \n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch+1, num_epochs, i+1, len(train_dataset) // batch_size, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images:  85 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) \n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs.data, 1) \n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum() \n",
    "  \n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % (100 * correct / total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4-1) 2-3 In[14] 코드에서 optim.SGD를 사용하지않고 코드를 짠다면 어떤 방식으로 짜야할까요? 설명해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.219, b: -0.235 Cost: 4.602861\n",
      "Epoch  100/1000 W: 0.968, b: 0.072 Cost: 0.000744\n",
      "Epoch  200/1000 W: 0.975, b: 0.056 Cost: 0.000460\n",
      "Epoch  300/1000 W: 0.980, b: 0.044 Cost: 0.000284\n",
      "Epoch  400/1000 W: 0.985, b: 0.035 Cost: 0.000175\n",
      "Epoch  500/1000 W: 0.988, b: 0.027 Cost: 0.000108\n",
      "Epoch  600/1000 W: 0.991, b: 0.022 Cost: 0.000067\n",
      "Epoch  700/1000 W: 0.993, b: 0.017 Cost: 0.000041\n",
      "Epoch  800/1000 W: 0.994, b: 0.013 Cost: 0.000026\n",
      "Epoch  900/1000 W: 0.995, b: 0.010 Cost: 0.000016\n",
      "Epoch 1000/1000 W: 0.996, b: 0.008 Cost: 0.000010\n"
     ]
    }
   ],
   "source": [
    "# optim.SGD 사용한 ln[14]코드\n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "y_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "\n",
    "class LinearRegressionModel(nn.Module):    \n",
    "    def __init__(self):        \n",
    "        super().__init__()        \n",
    "        self.linear = nn.Linear(1, 1) \n",
    "        \n",
    "    def forward(self, x):        \n",
    "        return self.linear(x)\n",
    "\n",
    "model = LinearRegressionModel() \n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n",
    "nb_epochs = 1000 \n",
    "for epoch in range(nb_epochs + 1):        \n",
    "\n",
    "    prediction = model(x_train)        \n",
    "    cost = F.mse_loss(prediction, y_train)        \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()    \n",
    "    optimizer.step()        \n",
    "\n",
    "    if epoch % 100 == 0:        \n",
    "        params = list(model.parameters())        \n",
    "        W = params[0].item()        \n",
    "        b = params[1].item()        \n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()        \n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.000, b: 0.000 Cost: 4.666667\n",
      "Epoch  100/1000 W: 0.887, b: 0.257 Cost: 0.009462\n",
      "Epoch  200/1000 W: 0.921, b: 0.179 Cost: 0.004594\n",
      "Epoch  300/1000 W: 0.945, b: 0.125 Cost: 0.002231\n",
      "Epoch  400/1000 W: 0.962, b: 0.087 Cost: 0.001083\n",
      "Epoch  500/1000 W: 0.973, b: 0.061 Cost: 0.000526\n",
      "Epoch  600/1000 W: 0.981, b: 0.042 Cost: 0.000255\n",
      "Epoch  700/1000 W: 0.987, b: 0.029 Cost: 0.000124\n",
      "Epoch  800/1000 W: 0.991, b: 0.020 Cost: 0.000060\n",
      "Epoch  900/1000 W: 0.994, b: 0.014 Cost: 0.000029\n",
      "Epoch 1000/1000 W: 0.996, b: 0.010 Cost: 0.000014\n"
     ]
    }
   ],
   "source": [
    "# optim.SGD 사용하지 말고 직접 코드 짜기\n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "y_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "\n",
    "W = 0\n",
    "b = 0\n",
    "lr = 0.01\n",
    "\n",
    "nb_epochs = 1000 \n",
    "for epoch in range(nb_epochs + 1):        \n",
    "\n",
    "    prediction = x_train * W + b\n",
    "    cost = F.mse_loss(prediction, y_train)        \n",
    "    \n",
    "    #미분값 계산\n",
    "    w_grad = torch.sum(((W * x_train + b) - y_train) * x_train)\n",
    "    b_grad =torch.sum((W*x_train + b) - y_train)\n",
    "\n",
    "    if epoch % 100 == 0:        \n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()        \n",
    "        ))\n",
    "        \n",
    "    #업데이트\n",
    "    W -= lr * w_grad\n",
    "    b -= lr * b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4-2) Gradient descent 코드를 구현하는 문제입니다. 2-3 In[14]의 코드에서 다른 optimizer(3개 ex)adam, rmsprop, sgd에 momentum 추가 등등)를 이용하여 결과값을 비교해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#리스트 설정\n",
    "ans_W = []\n",
    "ans_b = []\n",
    "ans_cost = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.525, b: -0.431 Cost: 2.147149\n",
      "Epoch  100/1000 W: 0.992, b: 0.032 Cost: 0.000333\n",
      "Epoch  200/1000 W: 0.990, b: 0.022 Cost: 0.000072\n",
      "Epoch  300/1000 W: 0.993, b: 0.017 Cost: 0.000040\n",
      "Epoch  400/1000 W: 0.995, b: 0.012 Cost: 0.000020\n",
      "Epoch  500/1000 W: 0.997, b: 0.008 Cost: 0.000008\n",
      "Epoch  600/1000 W: 0.998, b: 0.005 Cost: 0.000003\n",
      "Epoch  700/1000 W: 0.999, b: 0.003 Cost: 0.000001\n",
      "Epoch  800/1000 W: 0.999, b: 0.002 Cost: 0.000000\n",
      "Epoch  900/1000 W: 1.000, b: 0.001 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.000, b: 0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "#1. Adam\n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "y_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "\n",
    "#모델 초기화\n",
    "model = LinearRegressionModel() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) \n",
    "\n",
    "#Seed 설정\n",
    "torch.manual_seed(1)\n",
    "nb_epochs = 1000 \n",
    "for epoch in range(nb_epochs + 1):        \n",
    "\n",
    "    prediction = model(x_train)        \n",
    "    cost = F.mse_loss(prediction, y_train)        \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()    \n",
    "    optimizer.step()        \n",
    "\n",
    "    if epoch % 100 == 0:        \n",
    "        params = list(model.parameters())        \n",
    "        W = params[0].item()        \n",
    "        b = params[1].item()        \n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()        \n",
    "        ))\n",
    "    if epoch == nb_epochs:\n",
    "        ans_W.append(W)\n",
    "        ans_b.append(b)\n",
    "        ans_cost.append(cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.615, b: -0.341 Cost: 2.147149\n",
      "Epoch  100/1000 W: 0.992, b: 0.019 Cost: 0.000052\n",
      "Epoch  200/1000 W: 0.996, b: 0.008 Cost: 0.000010\n",
      "Epoch  300/1000 W: 0.999, b: 0.002 Cost: 0.000001\n",
      "Epoch  400/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  500/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  600/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  700/1000 W: 0.984, b: -0.016 Cost: 0.002620\n",
      "Epoch  800/1000 W: 0.999, b: -0.001 Cost: 0.000006\n",
      "Epoch  900/1000 W: 0.998, b: -0.002 Cost: 0.000030\n",
      "Epoch 1000/1000 W: 0.996, b: -0.004 Cost: 0.000109\n"
     ]
    }
   ],
   "source": [
    "#2. RMSprop\n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "y_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "\n",
    "#모델 초기화\n",
    "model = LinearRegressionModel() \n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01) \n",
    "\n",
    "#Seed 설정\n",
    "torch.manual_seed(1)\n",
    "nb_epochs = 1000 \n",
    "for epoch in range(nb_epochs + 1):        \n",
    "\n",
    "    prediction = model(x_train)        \n",
    "    cost = F.mse_loss(prediction, y_train)        \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()    \n",
    "    optimizer.step()        \n",
    "\n",
    "    if epoch % 100 == 0:        \n",
    "        params = list(model.parameters())        \n",
    "        W = params[0].item()        \n",
    "        b = params[1].item()        \n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()        \n",
    "        ))\n",
    "        \n",
    "    if epoch == nb_epochs:\n",
    "        ans_W.append(W)\n",
    "        ans_b.append(b)\n",
    "        ans_cost.append(cost.item())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.578, b: -0.413 Cost: 2.147149\n",
      "Epoch  100/1000 W: 1.007, b: -0.008 Cost: 0.000053\n",
      "Epoch  200/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  300/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  400/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  500/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  600/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  700/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  800/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  900/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.000, b: -0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "#3. SGD + Momentum\n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "y_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "\n",
    "#모델 초기화\n",
    "model = LinearRegressionModel() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9) \n",
    "\n",
    "#Seed 설정\n",
    "torch.manual_seed(1)\n",
    "nb_epochs = 1000 \n",
    "for epoch in range(nb_epochs + 1):        \n",
    "\n",
    "    prediction = model(x_train)        \n",
    "    cost = F.mse_loss(prediction, y_train)        \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()    \n",
    "    optimizer.step()        \n",
    "\n",
    "    if epoch % 100 == 0:        \n",
    "        params = list(model.parameters())        \n",
    "        W = params[0].item()        \n",
    "        b = params[1].item()        \n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()        \n",
    "        ))\n",
    "\n",
    "    if epoch == nb_epochs:\n",
    "        ans_W.append(W)\n",
    "        ans_b.append(b)\n",
    "        ans_cost.append(cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Adam] W: 0.9998, b: 0.0004, Cost: 0.0000000216\n",
      "[RMSprop] W: 0.9965, b: -0.0035, Cost: 0.0001090314\n",
      "[SGD+momentum] W: 1.0000, b: -0.0000, Cost: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "#정리 비교\n",
    "print('[Adam] W: {:.4f}, b: {:.4f}, Cost: {:.10f}'.format(ans_W[0], ans_b[0], ans_cost[0]))\n",
    "print('[RMSprop] W: {:.4f}, b: {:.4f}, Cost: {:.10f}'.format(ans_W[1], ans_b[1], ans_cost[1]))\n",
    "print('[SGD+momentum] W: {:.4f}, b: {:.4f}, Cost: {:.10f}'.format(ans_W[2], ans_b[2], ans_cost[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
