{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1주차 과제 - 김민회\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1) Type 1, Type 2 Error 비교 (FP, FN의 관점에서)\n",
    "[자료 링크](https://sumniya.tistory.com/26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![예시](img/threshold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 경우 threshold가 0.3으로 낮고, 그로 인해 acceptance rated가 높다.  \n",
    "따라서 FP가 높고 FN이 낮다.  \n",
    "FP는 2종 오류, FN은 1종 오류와 같은 개념이므로 1종 오류는 낮지만 2종 오류는 높다.  \n",
    "### A) Type 1 Error < Type 2 Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2-1) Accuracy, Precision and Recall (confusion matrix 관점)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![혼동행렬](img/confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accracy : 모든 분류 결과 중에서 옳게 분류한 비율, 즉 정답/전체  \n",
    "Precision : True로 분류한 것들 중 옳게 분류한 비율  \n",
    "Recall : 실제 값이 True인 것 중 모델이 True로 분류한 비율  \n",
    "### $Accracy   = \\frac{TP + TN}{TP + TN + FP+ FN}$  \n",
    "### $Precision   = \\frac{TP}{TP + FP}$  \n",
    "### $Recall   = \\frac{TP}{TP + FN}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2-2) Precision and Recall 예시 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 신약 개발 예시  \n",
    "제약 회사에서 신약을 개발했을 때 기존 약보다 성능 향상이 유의미한가  \n",
    "True를 '신약 출시가 합리적'이라 했을 때, FP가 더 치명적이므로 Precision에 더 집중할 것  \n",
    "이 경우 threshold를 높이는 것은 FP가 낮아지게 하고, Precision은 높아질 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3) MNIST 코드 따라해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[코드 출처](https://www.geeksforgeeks.org/identifying-handwritten-digits-using-logistic-regression-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision.datasets as dsets \n",
    "import torchvision.transforms as transforms \n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset (Images and Labels) \n",
    "train_dataset = dsets.MNIST(root ='./data',  \n",
    "                            train = True,  \n",
    "                            transform = transforms.ToTensor(), \n",
    "                            download = True) \n",
    "  \n",
    "test_dataset = dsets.MNIST(root ='./data',  \n",
    "                           train = False,  \n",
    "                           transform = transforms.ToTensor()) \n",
    "\n",
    "# Hyper Parameters  \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Dataset Loader (Input Pipline) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,  \n",
    "                                           batch_size = batch_size,  \n",
    "                                           shuffle = True) \n",
    "  \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,  \n",
    "                                          batch_size = batch_size,  \n",
    "                                          shuffle = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우리가 사용할 class 지정\n",
    "class LogisticRegression(nn.Module): \n",
    "    def __init__(self, input_size, num_classes): \n",
    "        super(LogisticRegression, self).__init__() \n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "  \n",
    "    def forward(self, x): \n",
    "        out = self.linear(x) \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_size, num_classes) # 모델 정의\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # loss 함수 설정\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) # optimizer 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 2.2124\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 2.0889\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 2.0536\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.9415\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.8689\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.8263\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.7235\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.6325\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.6378\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.6044\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.5570\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.4624\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.4384\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.4294\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.3291\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.2936\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.3345\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.3444\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.2063\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.1916\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.1227\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.0759\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.1435\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 1.0164\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 1.1629\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 0.9865\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 1.1244\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 1.0155\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 1.0195\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 1.1074\n"
     ]
    }
   ],
   "source": [
    "# Training the Model \n",
    "for epoch in range(num_epochs): \n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        images = Variable(images.view(-1, 28 * 28)) \n",
    "        labels = Variable(labels) \n",
    "  \n",
    "        # Forward + Backward + Optimize \n",
    "        optimizer.zero_grad()  # Gradient: starts from zero\n",
    "        outputs = model(images)  ### forward pass\n",
    "        loss = criterion(outputs, labels)  ### calculating loss \n",
    "        loss.backward()  ### backward propagation\n",
    "        optimizer.step()  ### updating weight\n",
    "  \n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch+1, num_epochs, i+1, len(train_dataset) \n",
    "                     // batch_size, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images:  83 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) \n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs.data, 1) \n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum() \n",
    "  \n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % ( \n",
    "            100 * correct // total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4-1) optim.SGD를 사용하지 않고 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 \n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "y_train = torch.FloatTensor([[1], [2], [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility \n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 모델 초기화 \n",
    "W=torch.zeros(1)\n",
    "b=torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.140, b: 0.060 Cost: 4.666667\n",
      "Epoch  100/1000 W: 0.887, b: 0.256 Cost: 0.009462\n",
      "Epoch  200/1000 W: 0.922, b: 0.178 Cost: 0.004594\n",
      "Epoch  300/1000 W: 0.945, b: 0.124 Cost: 0.002231\n",
      "Epoch  400/1000 W: 0.962, b: 0.087 Cost: 0.001083\n",
      "Epoch  500/1000 W: 0.973, b: 0.060 Cost: 0.000526\n",
      "Epoch  600/1000 W: 0.982, b: 0.042 Cost: 0.000255\n",
      "Epoch  700/1000 W: 0.987, b: 0.029 Cost: 0.000124\n",
      "Epoch  800/1000 W: 0.991, b: 0.020 Cost: 0.000060\n",
      "Epoch  900/1000 W: 0.994, b: 0.014 Cost: 0.000029\n",
      "Epoch 1000/1000 W: 0.996, b: 0.010 Cost: 0.000014\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 1000 # 전체 데이터셋 대상 SGD 반복 횟수 지정\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2) # Cost function 정의\n",
    "\n",
    "    ### SGD func. 쓰지 않고 gradient 계산식 지정 \n",
    "    w_grad = torch.sum(((W * x_train + b) - y_train) * x_train)\n",
    "    b_grad = torch.sum((W * x_train + b) - y_train)\n",
    "    \n",
    "    # manually update W and b (optimizer 함수를 쓰지 않고 cost func. 편미분하여 update)\n",
    "    W -= lr * w_grad\n",
    "    b -= lr * b_grad\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) 간단히 말하면, 기존 GD 방식은 one-by-one 으로 계산하여 시간이 오래 걸리므로,  \n",
    "SGD는 모델이 wide and deep 할 때 데이터를 mini-batch로 쪼개서 이런 단점을 보완한다.  \n",
    "하지만 우리가 가진 data는 굳이 batch를 더 쪼갤 필요성이 없으므로 직접 지정한 수식으로 update해도 충분하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4-2) 다른 optimizer 적용해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.278, b: 0.050 Cost: 2.296805\n",
      "Epoch  100/1000 W: 0.375, b: 0.146 Cost: 1.487315\n",
      "Epoch  200/1000 W: 0.460, b: 0.231 Cost: 0.919919\n",
      "Epoch  300/1000 W: 0.534, b: 0.303 Cost: 0.544140\n",
      "Epoch  400/1000 W: 0.596, b: 0.361 Cost: 0.310156\n",
      "Epoch  500/1000 W: 0.646, b: 0.407 Cost: 0.174275\n",
      "Epoch  600/1000 W: 0.686, b: 0.441 Cost: 0.101120\n",
      "Epoch  700/1000 W: 0.717, b: 0.463 Cost: 0.064650\n",
      "Epoch  800/1000 W: 0.739, b: 0.476 Cost: 0.047645\n",
      "Epoch  900/1000 W: 0.756, b: 0.481 Cost: 0.039950\n",
      "Epoch 1000/1000 W: 0.768, b: 0.480 Cost: 0.036249\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor(([1],[2],[3]))\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.365, b: -0.390 Cost: 3.021716\n",
      "Epoch  100/1000 W: 0.366, b: -0.389 Cost: 3.017332\n",
      "Epoch  200/1000 W: 0.366, b: -0.389 Cost: 3.011763\n",
      "Epoch  300/1000 W: 0.367, b: -0.388 Cost: 3.005242\n",
      "Epoch  400/1000 W: 0.367, b: -0.387 Cost: 2.997899\n",
      "Epoch  500/1000 W: 0.368, b: -0.387 Cost: 2.989824\n",
      "Epoch  600/1000 W: 0.369, b: -0.386 Cost: 2.981086\n",
      "Epoch  700/1000 W: 0.370, b: -0.385 Cost: 2.971736\n",
      "Epoch  800/1000 W: 0.371, b: -0.384 Cost: 2.961820\n",
      "Epoch  900/1000 W: 0.372, b: -0.383 Cost: 2.951376\n",
      "Epoch 1000/1000 W: 0.373, b: -0.382 Cost: 2.940437\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor(([1],[2],[3]))\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.001)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.155, b: 0.006 Cost: 3.425576\n",
      "Epoch  100/1000 W: 0.336, b: 0.186 Cost: 1.608132\n",
      "Epoch  200/1000 W: 0.432, b: 0.281 Cost: 0.951961\n",
      "Epoch  300/1000 W: 0.513, b: 0.360 Cost: 0.537302\n",
      "Epoch  400/1000 W: 0.585, b: 0.429 Cost: 0.276849\n",
      "Epoch  500/1000 W: 0.648, b: 0.485 Cost: 0.131319\n",
      "Epoch  600/1000 W: 0.699, b: 0.523 Cost: 0.066789\n",
      "Epoch  700/1000 W: 0.736, b: 0.535 Cost: 0.046477\n",
      "Epoch  800/1000 W: 0.763, b: 0.515 Cost: 0.039202\n",
      "Epoch  900/1000 W: 0.788, b: 0.468 Cost: 0.031981\n",
      "Epoch 1000/1000 W: 0.818, b: 0.404 Cost: 0.023767\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor(([1],[2],[3]))\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
