{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1) 이 경우에서는 Type 1 error가 높을까요? Type 2 Error가 높을까요? FP, FN과 관련지어 설명해주세요.  \n",
    "이 경우 threshold가 0.3으로 통상적인 경우(0.5)보다 낮기 때문에 True라고 예측하는 경우가 많아질 것이다. 따라서 FP가 높을 것이고 (실제로는 음성이어도 True라고 예측되는 경우 많음) FN는 낮을 것이다. (실제로 True이지만 음성이라고 잘못 예측하는 경우는 적을 것)  \n",
    "  \n",
    "귀무가설을 0(accept하지 않음)이라 하고 대립가설을 1(accept한다)이라고 할 때 Type 1 error는 귀무가설이 참일 때 이를 기각할 확률 (accept하지 말아야 하는데 accept한다 : FP)이고, Type 2 error는 귀무가설이 거짓일 때 이를 채택할 확률 (accept해야 하는데 accept하지 않는 것 : FN)이므로 이 경우 Type 1 error가 더 높을 것이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2-1) Accuracy, Precision and Recall이 무엇인지 TP,FP,FN,TN의 식으로 나타내 주세요.  \n",
    "**Accuracy** : $\\frac{TP + TN}{Total}$  \n",
    "  \n",
    "**Precision** : $\\frac{TP}{TP + FP}$  \n",
    "  \n",
    "**Recall** : $\\frac{TP}{TP + FN}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2-2) Precision and Recall에 관한 실생활 예시를 하나 들어주세요.  \n",
    "\\- 신용카드 이용자들의 신용 한도를 낮출지 말지 결정하는 경우 (낮추는 경우가 true)  \n",
    "고객 만족도가 중요할 경우 정말 신용한도를 낮추어야 하는 경우만 낮춰야 하므로 Precision이 높아야 한다(you want to be very sure about your decision). 예측된 positive 값들 중 실제로 true인 값들이 많아야 하는데, precision값을 높이는 방법은 threshold를 높여서 positive prediction 자체를 적게 하는 것이다.  \n",
    "하지만 이 경우 오류를 피하기 위해 모델이 대부분의 신용한도를 건드리지 않을 거라는 단점이 있다... 그리고 정말 줄여야 하는 이용자들을 그대로 둠으로써 은행 재정에 문제가 생길수도... (model will leave a lot of credit defaulters untouched, predict as 0)  \n",
    "  \n",
    "\\- 암을 진단하는 경우  \n",
    "이 경우 우리는 암이 아니더라도 암이라고 진단하는 경우가 있더라도 최대한 true를 많이 찾아내야 한다. 따라서 Recall을 높여서 실제로 true인 케이스들(TP + FN)중에서 true라고 맞게 예측하는 경우의 비율을 높여야 한다. 이 경우 threshold를 낮추는 것이 합리적이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3) 코드 따라해보고 주석 달기  \n",
    "source : https://www.geeksforgeeks.org/identifying-handwritten-digits-using-logistic-regression-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision.datasets as dsets \n",
    "import torchvision.transforms as transforms \n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters  \n",
    "input_size = 784 # image size 28*28\n",
    "num_classes = 10 # 10 output classes 0 / 1 / 2 / 3 / 4 / 5 / 6 / 7 / 8 / 9\n",
    "num_epochs = 5\n",
    "batch_size = 100 # train in small batches of 100 images\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset (Images and Labels) \n",
    "train_dataset = dsets.MNIST(root ='./data',  \n",
    "                            train = True,  \n",
    "                            transform = transforms.ToTensor(), \n",
    "                            download = True) \n",
    "  \n",
    "test_dataset = dsets.MNIST(root ='./data',  \n",
    "                           train = False,  \n",
    "                           transform = transforms.ToTensor()) \n",
    "  \n",
    "# Dataset Loader (Input Pipline) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,  \n",
    "                                           batch_size = batch_size,  \n",
    "                                           shuffle = True) \n",
    "  \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,  \n",
    "                                          batch_size = batch_size,  \n",
    "                                          shuffle = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class LogisticRegression(nn.Module): \n",
    "    def __init__(self, input_size, num_classes): \n",
    "        # initialize model as a subcalss of nn.Module\n",
    "        super(LogisticRegression, self).__init__() \n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "  \n",
    "    def forward(self, x): \n",
    "        out = self.linear(x) \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate an object\n",
    "model = LogisticRegression(input_size, num_classes)\n",
    "\n",
    "# set loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 2.1020\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 2.0323\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 1.9449\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.8238\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.8281\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.7623\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.6759\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.6281\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.6017\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.5201\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.4368\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.4284\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.4058\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.3391\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.3800\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.2941\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.2563\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.2645\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.2128\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.0751\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.1534\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.2261\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.0454\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 1.0265\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 1.0174\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 1.1241\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 0.9112\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 0.9598\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 1.0797\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 1.0158\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(num_epochs): \n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        images = Variable(images.view(-1, 28 * 28)) \n",
    "        labels = Variable(labels) \n",
    "  \n",
    "        # Forward + Backward + Optimize \n",
    "        optimizer.zero_grad() # reset all grads to 0\n",
    "        outputs = model(images) # forward pass\n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward() # back propagation\n",
    "        optimizer.step() # update\n",
    "  \n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, \n",
    "                     len(train_dataset) // batch_size, loss.item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images:  82 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/distiller/project/conda/conda-bld/pytorch_1591914925853/work/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) \n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs.data, 1) \n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum() \n",
    "  \n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % ( \n",
    "            100 * correct / total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4-1) 2-3 In[14] 코드에서 optim.SGD를 사용하지않고 코드를 짠다면 어떤 방식으로 짜야할까요? 설명해주세요.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.280, b: 0.120 Cost: 14.000000\n",
      "Epoch  100/1000 W: 0.922, b: 0.177 Cost: 0.013747\n",
      "Epoch  200/1000 W: 0.962, b: 0.086 Cost: 0.003233\n",
      "Epoch  300/1000 W: 0.982, b: 0.042 Cost: 0.000760\n",
      "Epoch  400/1000 W: 0.991, b: 0.020 Cost: 0.000179\n",
      "Epoch  500/1000 W: 0.996, b: 0.010 Cost: 0.000042\n",
      "Epoch  600/1000 W: 0.998, b: 0.005 Cost: 0.000010\n",
      "Epoch  700/1000 W: 0.999, b: 0.002 Cost: 0.000002\n",
      "Epoch  800/1000 W: 1.000, b: 0.001 Cost: 0.000001\n",
      "Epoch  900/1000 W: 1.000, b: 0.001 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.000, b: 0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# define model\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# initialize parameters\n",
    "W = torch.zeros(1)\n",
    "b = torch.zeros(1)\n",
    "lr = 0.01\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    hypothesis = x_train * W + b\n",
    "    cost = torch.sum((hypothesis - y_train)**2)\n",
    "\n",
    "    # W_grad / b_grad\n",
    "    W_grad = torch.sum(2 * (hypothesis - y_train) * x_train)\n",
    "    b_grad = torch.sum(2 * (hypothesis - y_train))\n",
    "    \n",
    "    # update gradients\n",
    "    W -= lr * W_grad\n",
    "    b -= lr * b_grad\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4-2) Gradient descent 코드를 구현하는 문제입니다. 2-3 In[14]의 코드에서 다른 optimizer(3개 ex)adam, rmsprop, sgd에 momentum 추가 등등)를 이용하여 결과값을 비교해주세요.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ws = []\n",
    "bs = []\n",
    "Costs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.256, b: 0.274 Cost: 6.213388\n",
      "Epoch  100/1000 W: 0.449, b: 0.944 Cost: 0.233081\n",
      "Epoch  200/1000 W: 0.577, b: 0.936 Cost: 0.127863\n",
      "Epoch  300/1000 W: 0.630, b: 0.819 Cost: 0.097708\n",
      "Epoch  400/1000 W: 0.686, b: 0.695 Cost: 0.070449\n",
      "Epoch  500/1000 W: 0.741, b: 0.574 Cost: 0.048112\n",
      "Epoch  600/1000 W: 0.791, b: 0.462 Cost: 0.031180\n",
      "Epoch  700/1000 W: 0.836, b: 0.362 Cost: 0.019189\n",
      "Epoch  800/1000 W: 0.875, b: 0.277 Cost: 0.011213\n",
      "Epoch  900/1000 W: 0.907, b: 0.206 Cost: 0.006218\n",
      "Epoch 1000/1000 W: 0.933, b: 0.149 Cost: 0.003269\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    prediction = model(x_train)\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))\n",
    "    \n",
    "    if epoch==nb_epochs:\n",
    "        Ws.append(W)\n",
    "        bs.append(b)\n",
    "        Costs.append(cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.008, b: 0.154 Cost: 5.498762\n",
      "Epoch  100/1000 W: 0.703, b: 0.657 Cost: 0.063197\n",
      "Epoch  200/1000 W: 0.797, b: 0.448 Cost: 0.029511\n",
      "Epoch  300/1000 W: 0.891, b: 0.241 Cost: 0.008546\n",
      "Epoch  400/1000 W: 0.961, b: 0.087 Cost: 0.001126\n",
      "Epoch  500/1000 W: 0.993, b: 0.016 Cost: 0.000040\n",
      "Epoch  600/1000 W: 1.000, b: 0.001 Cost: 0.000000\n",
      "Epoch  700/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  800/1000 W: 0.999, b: -0.001 Cost: 0.000015\n",
      "Epoch  900/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 0.991, b: -0.009 Cost: 0.001011\n"
     ]
    }
   ],
   "source": [
    "# RMSprop\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    prediction = model(x_train)\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))\n",
    "    \n",
    "    if epoch==nb_epochs:\n",
    "        Ws.append(W)\n",
    "        bs.append(b)\n",
    "        Costs.append(cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.300, b: -0.468 Cost: 4.827516\n",
      "Epoch  100/1000 W: 1.007, b: -0.005 Cost: 0.000095\n",
      "Epoch  200/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  300/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  400/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  500/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  600/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  700/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  800/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  900/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.000, b: -0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# SGDmomentum\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    prediction = model(x_train)\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))\n",
    "    \n",
    "    if epoch==nb_epochs:\n",
    "        Ws.append(W)\n",
    "        bs.append(b)\n",
    "        Costs.append(cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Adam] W: 0.933, b: 0.149, Cost: 0.003269\n",
      "[RMSprop] W: 0.991, b: -0.009, Cost: 0.001011\n",
      "[SGDmomentum] W: 1.000, b: -0.000, Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "print('[Adam] W: {:.3f}, b: {:.3f}, Cost: {:.6f}'.format(Ws[0], bs[0], Costs[0]))\n",
    "print('[RMSprop] W: {:.3f}, b: {:.3f}, Cost: {:.6f}'.format(Ws[1], bs[1], Costs[1]))\n",
    "print('[SGDmomentum] W: {:.3f}, b: {:.3f}, Cost: {:.6f}'.format(Ws[2], bs[2], Costs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
