{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1_Regression_손동규.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY9ETrgutdj_",
        "colab_type": "text"
      },
      "source": [
        "Q1) 이 경우에서는 Type 1 error가 높을까요? Type 2 Error가 높을까요? FP, FN과 관련지어 설명해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Zdpxl1vttcZ",
        "colab_type": "text"
      },
      "source": [
        "Threshold를 0.3으로 낮춘다는 것은 True라고 판정하는 경우가 더 많아지는 것이기 때문에 False임에도 True라고 판정하는 FP가 더 많아질 것이다. 이는 True가 대립가설이라고 한다면 귀무가설이 맞음에도 불구하고 대립가설을 채택하는 Type 1 Error가 커지는 것이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxHWD2lFtiwD",
        "colab_type": "text"
      },
      "source": [
        "Q2-1) Accuracy, Precision and Recall이 무엇인지 TP,FP,FN,TN의 식으로 나타내 주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5Je9SkPtwup",
        "colab_type": "text"
      },
      "source": [
        "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "전체에서 Positive와 Negative를 얼마나 정확히 예측했는지의 여부를 측정\n",
        "\n",
        "Precision(PPV_Positive Predictive Value) = TP / (TP + FP)\n",
        "Positive로 예측한 것들 중 실제로 Positive인 비율\n",
        "\n",
        "Recall(Sensitivity, TPR_True Positive Rate) = TP / (TP + FN)\n",
        "실제 Positive인 것들 중 Positive로 예측된 비율\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3PMSXPtkj5",
        "colab_type": "text"
      },
      "source": [
        "Q2-2) Precision and Recall에 관한 제가 든 예시가 아닌 실생활 예시 하나를 들어주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUqtxA6fvMf6",
        "colab_type": "text"
      },
      "source": [
        "은행이 대출 고객의 부도 여부를 예측하여 대출 가능 여부를 결정할 때 두 개를 모두 고려하여야 한다. 고객을 우량 고객과 불량 고객으로 나눌 때, 실제로는 불량 고객인데 우량 고객이라고 판단하는 경우(FP)가 늘어나게 되면 Precision이 감소하게 된다. 이 경우는 은행이 대출을 승인하게 되면 대출 금액을 회수하지 못하게 될 가능성이 크므로 은행에 타격을 주게 된다. 반대로, 실제로는 우량 고객이지만 불량 고객이라고 판단하는 경우(FN)가 늘어나면 Recall이 감소하게 된다. 이 경우 은행은 우량 고객들에게도 대출을 해주지 않으므로 대출 이자 수익이 감소하게 된다. 따라서 둘 간의 적절한 균형점을 찾는 것이 중요하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yZt617ntmEp",
        "colab_type": "text"
      },
      "source": [
        "Q3) 코드 따라해보고 주석 달기!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw708jcqGg7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn \n",
        "import torchvision.datasets as dsets \n",
        "import torchvision.transforms as transforms \n",
        "from torch.autograd import Variable "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ocTh-OfG8PU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper Parameters  \n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-5EYShqGir-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MNIST Dataset (Images and Labels) \n",
        "train_dataset = dsets.MNIST(root ='./data', train = True, transform = transforms.ToTensor(), download = True) \n",
        "  \n",
        "test_dataset = dsets.MNIST(root ='./data', train = False, transform = transforms.ToTensor()) \n",
        "\n",
        "# Dataset Loader (Input Pipline) \n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True) \n",
        "  \n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False) "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9y8OW34G909",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression(nn.Module): \n",
        "    def __init__(self, input_size, num_classes): \n",
        "        super(LogisticRegression, self).__init__() \n",
        "        self.linear = nn.Linear(input_size, num_classes) \n",
        "  \n",
        "    def forward(self, x): \n",
        "        out = self.linear(x) \n",
        "        return out "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaFlzr6QHA8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LogisticRegression(input_size, num_classes) "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntcL7h7OHB2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsohUkVNHD4x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "dcf097d7-baf9-48ad-9388-deef4464478b"
      },
      "source": [
        "# Training the Model \n",
        "for epoch in range(num_epochs): \n",
        "    for i, (images, labels) in enumerate(train_loader): \n",
        "        images = Variable(images.view(-1, 28 * 28)) \n",
        "        labels = Variable(labels) \n",
        "  \n",
        "        # Forward + Backward + Optimize \n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(images) \n",
        "        loss = criterion(outputs, labels) \n",
        "        loss.backward() \n",
        "        optimizer.step() \n",
        "  \n",
        "        if (i + 1) % 100 == 0: \n",
        "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
        "                  % (epoch + 1, num_epochs, i + 1, \n",
        "                     len(train_dataset) // batch_size, loss.data)) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 2.2013\n",
            "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 2.1335\n",
            "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 2.0561\n",
            "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.9201\n",
            "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.9128\n",
            "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.7493\n",
            "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.7105\n",
            "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.6116\n",
            "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.6592\n",
            "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.5746\n",
            "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.5829\n",
            "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.5034\n",
            "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.4786\n",
            "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.4779\n",
            "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.3165\n",
            "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.3071\n",
            "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.2100\n",
            "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.2629\n",
            "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.1559\n",
            "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.1699\n",
            "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.0640\n",
            "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.1730\n",
            "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.1983\n",
            "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 1.1030\n",
            "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 1.1089\n",
            "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 1.1180\n",
            "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 1.0507\n",
            "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 0.9866\n",
            "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 0.9383\n",
            "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 1.0635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHxvtTLnHHAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "57d2ba04-fcd9-4162-9ead-4e7523e9de67"
      },
      "source": [
        "# Test the Model \n",
        "correct = 0\n",
        "total = 0\n",
        "for images, labels in test_loader: \n",
        "    images = Variable(images.view(-1, 28 * 28)) \n",
        "    outputs = model(images) \n",
        "    _, predicted = torch.max(outputs.data, 1) \n",
        "    total += labels.size(0) \n",
        "    correct += (predicted == labels).sum() \n",
        "  \n",
        "print('Accuracy of the model on the 10000 test images: % d %%' % ( \n",
        "            100 * correct / total)) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the 10000 test images:  82 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578giCa6tpvZ",
        "colab_type": "text"
      },
      "source": [
        "Q4-1) 2-3 In[14] 코드에서 optim.SGD를 사용하지않고 코드를 짠다면 어떤 방식으로 짜야할까요? 설명해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJQ61V20NfWP",
        "colab_type": "text"
      },
      "source": [
        "optim.SGD를 쓰지 않고 코드를 짜려면 전체 데이터셋을 batch로 나누고 각각에 대해서 gradient를 계산해준 다음 Gradient Descent Algorithm을 구현해주면 된다. 그러나 이 경우에는 데이터가 별로 없기 때문에 batch를 나누는 의미가 없기 때문에 그냥 Gradient Descent Algorithm을 적용해주면 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tb8FD6ktrPf",
        "colab_type": "text"
      },
      "source": [
        "Q4-2) Gradient descent 코드를 구현하는 문제입니다. 2-3 In[14]의 코드에서 다른 optimizer(3개 ex)adam, rmsprop, sgd에 momentum 추가 등등)를 이용하여 결과값을 비교해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKFE4rk_xjnm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "47c09063-83f5-430b-9518-a5d617501a9b"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[1], [2], [3]])\n",
        "\n",
        "class LinearRegressionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)\n",
        "\n",
        "# 모델 초기화\n",
        "model = LinearRegressionModel()\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  # H(x) 계산\n",
        "  prediction = model(x_train)\n",
        "  # cost 계산\n",
        "  cost = F.mse_loss(prediction, y_train)\n",
        "  # cost로 H(x) 개선\n",
        "  optimizer.zero_grad() # 미분값이\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  # 100번마다 로그 출력\n",
        "  if epoch % 100 == 0:\n",
        "    params = list(model.parameters())\n",
        "    W = params[0].item()\n",
        "    b = params[1].item()\n",
        "    print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs, W, b, cost.item()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 W: 0.236, b: -0.281 Cost: 3.783830\n",
            "Epoch  100/1000 W: 0.839, b: 0.303 Cost: 0.017979\n",
            "Epoch  200/1000 W: 0.875, b: 0.277 Cost: 0.011186\n",
            "Epoch  300/1000 W: 0.897, b: 0.229 Cost: 0.007645\n",
            "Epoch  400/1000 W: 0.918, b: 0.182 Cost: 0.004806\n",
            "Epoch  500/1000 W: 0.938, b: 0.138 Cost: 0.002793\n",
            "Epoch  600/1000 W: 0.954, b: 0.101 Cost: 0.001504\n",
            "Epoch  700/1000 W: 0.968, b: 0.072 Cost: 0.000752\n",
            "Epoch  800/1000 W: 0.978, b: 0.049 Cost: 0.000348\n",
            "Epoch  900/1000 W: 0.986, b: 0.032 Cost: 0.000150\n",
            "Epoch 1000/1000 W: 0.991, b: 0.020 Cost: 0.000059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha_65zjVJ56H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "6d4f25b6-60ff-4945-a116-a7ffc91f7766"
      },
      "source": [
        "# 모델 초기화\n",
        "model = LinearRegressionModel()\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.Adamax(model.parameters(), lr=0.01)\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  # H(x) 계산\n",
        "  prediction = model(x_train)\n",
        "  # cost 계산\n",
        "  cost = F.mse_loss(prediction, y_train)\n",
        "  # cost로 H(x) 개선\n",
        "  optimizer.zero_grad() # 미분값이\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  # 100번마다 로그 출력\n",
        "  if epoch % 100 == 0:\n",
        "    params = list(model.parameters())\n",
        "    W = params[0].item()\n",
        "    b = params[1].item()\n",
        "    print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs, W, b, cost.item()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 W: -0.002, b: 0.289 Cost: 3.729678\n",
            "Epoch  100/1000 W: 0.535, b: 0.789 Cost: 0.166727\n",
            "Epoch  200/1000 W: 0.627, b: 0.806 Cost: 0.096437\n",
            "Epoch  300/1000 W: 0.659, b: 0.755 Cost: 0.083086\n",
            "Epoch  400/1000 W: 0.685, b: 0.698 Cost: 0.070789\n",
            "Epoch  500/1000 W: 0.712, b: 0.638 Cost: 0.059296\n",
            "Epoch  600/1000 W: 0.739, b: 0.579 Cost: 0.048743\n",
            "Epoch  700/1000 W: 0.766, b: 0.519 Cost: 0.039242\n",
            "Epoch  800/1000 W: 0.792, b: 0.460 Cost: 0.030872\n",
            "Epoch  900/1000 W: 0.818, b: 0.403 Cost: 0.023674\n",
            "Epoch 1000/1000 W: 0.843, b: 0.348 Cost: 0.017648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvDOvULkJZEz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "d950ef17-667b-482d-efc9-1d701bfa3c51"
      },
      "source": [
        "# 모델 초기화\n",
        "model = LinearRegressionModel()\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  # H(x) 계산\n",
        "  prediction = model(x_train)\n",
        "  # cost 계산\n",
        "  cost = F.mse_loss(prediction, y_train)\n",
        "  # cost로 H(x) 개선\n",
        "  optimizer.zero_grad() # 미분값이\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  # 100번마다 로그 출력\n",
        "  if epoch % 100 == 0:\n",
        "    params = list(model.parameters())\n",
        "    W = params[0].item()\n",
        "    b = params[1].item()\n",
        "    print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs, W, b, cost.item()))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 W: -0.084, b: -0.809 Cost: 11.675529\n",
            "Epoch  100/1000 W: 0.897, b: 0.152 Cost: 0.010511\n",
            "Epoch  200/1000 W: 0.934, b: 0.147 Cost: 0.003138\n",
            "Epoch  300/1000 W: 0.951, b: 0.108 Cost: 0.001711\n",
            "Epoch  400/1000 W: 0.971, b: 0.065 Cost: 0.000628\n",
            "Epoch  500/1000 W: 0.987, b: 0.028 Cost: 0.000120\n",
            "Epoch  600/1000 W: 0.997, b: 0.007 Cost: 0.000008\n",
            "Epoch  700/1000 W: 1.000, b: 0.001 Cost: 0.000000\n",
            "Epoch  800/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
            "Epoch  900/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
            "Epoch 1000/1000 W: 1.017, b: 0.017 Cost: 0.002456\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFW54QW8OsMe",
        "colab_type": "text"
      },
      "source": [
        "Adamax가 제일 느리게 최적화되고있고, 그 다음 Adam이 느리게 최적화되는 중이다. RMSprop은 700/1000에 최적화를 달성하였으나 1000/1000에서 다시 최적화에서 벗어나는 것을 관찰할 수 있다."
      ]
    }
  ]
}