{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type 1 error ~ FP ( 실제는 False인데, True라 판단)\n",
    "\n",
    "Type 2 error ~ FN ( 실제는 True인데, False라 판단)\n",
    "\n",
    "Type 1 error와 Type 2 error가 서로 trade off 관계에 있듯이 FP와 FN도 서로 trade off 관계에 있다.\n",
    "\n",
    "따라서, threshold = 0.3으로 낮추면, FN이 작아지고, FP가 커진다.\n",
    "즉, Type 1 error 가 더 커진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy(정확도: 전체 예측 중 옳게 말한 결과의 비율) = (TP+TN) / (TP+TN+FP+FN)\n",
    "\n",
    "Precision(정밀도: True라고 예측한 것 중 실제로 True의 비율) = TP / (TP+FP)\n",
    "\n",
    "Recall(재현율: 실제 True 중 True라고 예측한 것의 비율) = TP / (TP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "암을 진단하는 문제: 관심사건 = 암이다.\n",
    "\n",
    "threshold를 늘린다는 것은 웬만하면 암으로 진단하지 않겠다는 것을 의미한다.\n",
    "\n",
    "이는 비합리적이다. \n",
    "\n",
    "왜냐하면, 암에 걸린 사람을 암이 아니라고 진단하는 것이 암에 걸리지 않은 사람을 암이라고 진단하는 것보다 더 심각한 오류이기 때문이다.\n",
    "\n",
    "(물론, 암이라고 오진단을 받고 막대한 병원비 때문에 목숨을 걸 정도로, 생명보다 돈이 중요한 사람에게는 threshold를 낮추는 게\n",
    "더 비합리적일 것이다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets \n",
    "import torchvision.transforms as transforms \n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2c520473c90>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility \n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset (Images and Labels) \n",
    "train_dataset = dsets.MNIST(root ='./data', \n",
    "\t\t\t\t\t\t\ttrain = True, \n",
    "\t\t\t\t\t\t\ttransform = transforms.ToTensor(), \n",
    "\t\t\t\t\t\t\tdownload = True) \n",
    "\n",
    "test_dataset = dsets.MNIST(root ='./data', \n",
    "\t\t\t\t\t\ttrain = False, \n",
    "\t\t\t\t\t\ttransform = transforms.ToTensor()) \n",
    "\n",
    "# Dataset Loader (Input Pipline) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "\t\t\t\t\t\t\t\t\t\tbatch_size = batch_size, \n",
    "\t\t\t\t\t\t\t\t\t\tshuffle = True) \n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, \n",
    "\t\t\t\t\t\t\t\t\t\tbatch_size = batch_size, \n",
    "\t\t\t\t\t\t\t\t\t\tshuffle = False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module): \n",
    "\tdef __init__(self, input_size, num_classes): \n",
    "\t\tsuper(LogisticRegression, self).__init__() \n",
    "\t\tself.linear = nn.Linear(input_size, num_classes) \n",
    "\n",
    "\tdef forward(self, x): \n",
    "\t\tout = self.linear(x) \n",
    "\t\treturn out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_size, num_classes) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() #cost function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #optimization 방식: SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 2.0943\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 2.0331\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 1.9534\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.8604\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.8261\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.7157\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.7001\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.5802\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.5340\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.5098\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.4562\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.3380\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.3218\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.3357\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.2131\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.2959\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.2757\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.2354\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.1801\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.1512\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.1760\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.1050\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.0907\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 1.1533\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 1.0592\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 1.1534\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 0.9653\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 0.9832\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 1.0451\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 0.9420\n"
     ]
    }
   ],
   "source": [
    "# Training the Model \n",
    "for epoch in range(num_epochs): \n",
    "\tfor i, (images, labels) in enumerate(train_loader): \n",
    "\t\timages = Variable(images.view(-1, 28 * 28)) \n",
    "\t\tlabels = Variable(labels) \n",
    "\n",
    "\t\t# Forward + Backward + Optimize  \n",
    "\t\toptimizer.zero_grad()  #초기화\n",
    "\t\toutputs = model(images)  #결과\n",
    "\t\tloss = criterion(outputs, labels) \n",
    "\t\tloss.backward() \n",
    "\t\toptimizer.step() #update\n",
    "\n",
    "\t\tif (i + 1) % 100 == 0: \n",
    "\t\t\tprint('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "\t\t\t\t% (epoch + 1, num_epochs, i + 1, \n",
    "\t\t\t\t\tlen(train_dataset) // batch_size, loss.data)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images:  83 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "# Test the Model \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "\timages = Variable(images.view(-1, 28 * 28)) \n",
    "\toutputs = model(images) \n",
    "\t_, predicted = torch.max(outputs.data, 1) \n",
    "\ttotal += labels.size(0) \n",
    "\tcorrect += (predicted == labels).sum() t\n",
    "\n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % ( \n",
    "\t\t\t100 * correct / total)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optim.sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 \n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "y_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "class LinearRegressionModel(nn.Module):    \n",
    "    def __init__(self):        \n",
    "        super().__init__()        \n",
    "        self.linear = nn.Linear(1, 1) \n",
    "        \n",
    "    def forward(self, x):        \n",
    "        return self.linear(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.578, b: -0.413 Cost: 2.147149\n",
      "Epoch  100/1000 W: 1.066, b: -0.150 Cost: 0.003239\n",
      "Epoch  200/1000 W: 1.052, b: -0.118 Cost: 0.002002\n",
      "Epoch  300/1000 W: 1.041, b: -0.093 Cost: 0.001237\n",
      "Epoch  400/1000 W: 1.032, b: -0.073 Cost: 0.000764\n",
      "Epoch  500/1000 W: 1.025, b: -0.057 Cost: 0.000472\n",
      "Epoch  600/1000 W: 1.020, b: -0.045 Cost: 0.000292\n",
      "Epoch  700/1000 W: 1.016, b: -0.035 Cost: 0.000180\n",
      "Epoch  800/1000 W: 1.012, b: -0.028 Cost: 0.000111\n",
      "Epoch  900/1000 W: 1.010, b: -0.022 Cost: 0.000069\n",
      "Epoch 1000/1000 W: 1.008, b: -0.017 Cost: 0.000043\n"
     ]
    }
   ],
   "source": [
    "# For reproducibility \n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 모델 초기화 \n",
    "model = LinearRegressionModel() \n",
    "\n",
    "# optimizer 설정 \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n",
    "nb_epochs = 1000 \n",
    "\n",
    "for epoch in range(nb_epochs + 1):        \n",
    "\n",
    "    # H(x) 계산    \n",
    "    prediction = model(x_train)        \n",
    "\n",
    "    # cost 계산    \n",
    "    cost = F.mse_loss(prediction, y_train)        \n",
    "\n",
    "    # cost로 H(x) 개선    \n",
    "    optimizer.zero_grad() # 미분값이     \n",
    "    cost.backward()    \n",
    "    optimizer.step()        \n",
    "\n",
    "    # 100번마다 로그 출력    \n",
    "    if epoch % 100 == 0:        \n",
    "        params = list(model.parameters())        \n",
    "        W = params[0].item()        \n",
    "        b = params[1].item()        \n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()        \n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## another way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 \n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "y_train = torch.FloatTensor([[1], [2], [3]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.000, b: 0.000 Cost: 4.666667\n",
      "Epoch  100/1000 W: 0.887, b: 0.257 Cost: 0.009462\n",
      "Epoch  200/1000 W: 0.921, b: 0.179 Cost: 0.004594\n",
      "Epoch  300/1000 W: 0.945, b: 0.125 Cost: 0.002231\n",
      "Epoch  400/1000 W: 0.962, b: 0.087 Cost: 0.001083\n",
      "Epoch  500/1000 W: 0.973, b: 0.061 Cost: 0.000526\n",
      "Epoch  600/1000 W: 0.981, b: 0.042 Cost: 0.000255\n",
      "Epoch  700/1000 W: 0.987, b: 0.029 Cost: 0.000124\n",
      "Epoch  800/1000 W: 0.991, b: 0.020 Cost: 0.000060\n",
      "Epoch  900/1000 W: 0.994, b: 0.014 Cost: 0.000029\n",
      "Epoch 1000/1000 W: 0.996, b: 0.010 Cost: 0.000014\n"
     ]
    }
   ],
   "source": [
    "# For reproducibility \n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 모델 초기화 \n",
    "W=torch.zeros(1)\n",
    "b=torch.zeros(1)\n",
    "\n",
    "# optimizer 설정 \n",
    "lr=0.01\n",
    "\n",
    "nb_epochs=1000\n",
    "for epoch in range(nb_epochs + 1):        \n",
    "\n",
    "    # H(x) 계산    \n",
    "    prediction = (x_train * W) + b       \n",
    "\n",
    "    # cost 계산    \n",
    "    cost = F.mse_loss(prediction, y_train)        \n",
    "    gradient=torch.sum(((W * x_train + b)- y_train) * x_train)\n",
    "    b_update=torch.sum((W*x_train + b) - y_train)\n",
    "    \n",
    "    # 100번마다 로그 출력                 \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()        \n",
    "        ))\n",
    "    #cost gradient로 H(X) 개선\n",
    "    W-=lr*gradient\n",
    "    b-=lr*b_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 \n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "y_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "class LinearRegressionModel(nn.Module):    \n",
    "    def __init__(self):        \n",
    "        super().__init__()        \n",
    "        self.linear = nn.Linear(1, 1) \n",
    "        \n",
    "    def forward(self, x):        \n",
    "        return self.linear(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.578, b: -0.413 Cost: 2.147149\n",
      "Epoch  100/1000 W: 1.066, b: -0.150 Cost: 0.003239\n",
      "Epoch  200/1000 W: 1.052, b: -0.118 Cost: 0.002002\n",
      "Epoch  300/1000 W: 1.041, b: -0.093 Cost: 0.001237\n",
      "Epoch  400/1000 W: 1.032, b: -0.073 Cost: 0.000764\n",
      "Epoch  500/1000 W: 1.025, b: -0.057 Cost: 0.000472\n",
      "Epoch  600/1000 W: 1.020, b: -0.045 Cost: 0.000292\n",
      "Epoch  700/1000 W: 1.016, b: -0.035 Cost: 0.000180\n",
      "Epoch  800/1000 W: 1.012, b: -0.028 Cost: 0.000111\n",
      "Epoch  900/1000 W: 1.010, b: -0.022 Cost: 0.000069\n",
      "Epoch 1000/1000 W: 1.008, b: -0.017 Cost: 0.000043\n"
     ]
    }
   ],
   "source": [
    "# For reproducibility \n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 모델 초기화 \n",
    "model = LinearRegressionModel() \n",
    "W=torch.zeros(1)\n",
    "b=torch.zeros(1)\n",
    "\n",
    "# optimizer 설정 \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n",
    "nb_epochs = 1000 \n",
    "\n",
    "for epoch in range(nb_epochs + 1):        \n",
    "\n",
    "    # H(x) 계산    \n",
    "    prediction = model(x_train)        \n",
    "\n",
    "    # cost 계산    \n",
    "    cost = F.mse_loss(prediction, y_train)        \n",
    "\n",
    "    # cost로 H(x) 개선    \n",
    "    optimizer.zero_grad() # 미분값이     \n",
    "    cost.backward()    \n",
    "    optimizer.step()        \n",
    "\n",
    "    # 100번마다 로그 출력    \n",
    "    if epoch % 100 == 0:        \n",
    "        params = list(model.parameters())        \n",
    "        W = params[0].item()        \n",
    "        b = params[1].item()        \n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()        \n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 sgd with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 \n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "y_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "class LinearRegressionModel(nn.Module):    \n",
    "    def __init__(self):        \n",
    "        super().__init__()        \n",
    "        self.linear = nn.Linear(1, 1) \n",
    "        \n",
    "    def forward(self, x):        \n",
    "        return self.linear(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.578, b: -0.413 Cost: 2.147149\n",
      "Epoch  100/1000 W: 1.007, b: -0.008 Cost: 0.000053\n",
      "Epoch  200/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  300/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  400/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  500/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  600/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  700/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  800/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  900/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.000, b: -0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# For reproducibility \n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 모델 초기화 \n",
    "model = LinearRegressionModel() \n",
    "W=torch.zeros(1)\n",
    "b=torch.zeros(1)\n",
    "\n",
    "# optimizer 설정 \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9) \n",
    "nb_epochs = 1000 \n",
    "\n",
    "for epoch in range(nb_epochs + 1):        \n",
    "\n",
    "    # H(x) 계산    \n",
    "    prediction = model(x_train)        \n",
    "\n",
    "    # cost 계산    \n",
    "    cost = F.mse_loss(prediction, y_train)        \n",
    "\n",
    "    # cost로 H(x) 개선    \n",
    "    optimizer.zero_grad() # 미분값이     \n",
    "    cost.backward()    \n",
    "    optimizer.step()        \n",
    "\n",
    "    # 100번마다 로그 출력    \n",
    "    if epoch % 100 == 0:        \n",
    "        params = list(model.parameters())        \n",
    "        W = params[0].item()        \n",
    "        b = params[1].item()        \n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()        \n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 \n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "y_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "class LinearRegressionModel(nn.Module):    \n",
    "    def __init__(self):        \n",
    "        super().__init__()        \n",
    "        self.linear = nn.Linear(1, 1) \n",
    "        \n",
    "    def forward(self, x):        \n",
    "        return self.linear(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.525, b: -0.431 Cost: 2.147149\n",
      "Epoch  100/1000 W: 0.992, b: 0.032 Cost: 0.000333\n",
      "Epoch  200/1000 W: 0.990, b: 0.022 Cost: 0.000072\n",
      "Epoch  300/1000 W: 0.993, b: 0.017 Cost: 0.000040\n",
      "Epoch  400/1000 W: 0.995, b: 0.012 Cost: 0.000020\n",
      "Epoch  500/1000 W: 0.997, b: 0.008 Cost: 0.000008\n",
      "Epoch  600/1000 W: 0.998, b: 0.005 Cost: 0.000003\n",
      "Epoch  700/1000 W: 0.999, b: 0.003 Cost: 0.000001\n",
      "Epoch  800/1000 W: 0.999, b: 0.002 Cost: 0.000000\n",
      "Epoch  900/1000 W: 1.000, b: 0.001 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.000, b: 0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# For reproducibility \n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 모델 초기화 \n",
    "model = LinearRegressionModel() \n",
    "W=torch.zeros(1)\n",
    "b=torch.zeros(1)\n",
    "\n",
    "# optimizer 설정 \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) \n",
    "nb_epochs = 1000 \n",
    "\n",
    "for epoch in range(nb_epochs + 1):        \n",
    "\n",
    "    # H(x) 계산    \n",
    "    prediction = model(x_train)        \n",
    "\n",
    "    # cost 계산    \n",
    "    cost = F.mse_loss(prediction, y_train)        \n",
    "\n",
    "    # cost로 H(x) 개선    \n",
    "    optimizer.zero_grad() # 미분값이     \n",
    "    cost.backward()    \n",
    "    optimizer.step()        \n",
    "\n",
    "    # 100번마다 로그 출력    \n",
    "    if epoch % 100 == 0:        \n",
    "        params = list(model.parameters())        \n",
    "        W = params[0].item()        \n",
    "        b = params[1].item()        \n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()        \n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 \n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "y_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "class LinearRegressionModel(nn.Module):    \n",
    "    def __init__(self):        \n",
    "        super().__init__()        \n",
    "        self.linear = nn.Linear(1, 1) \n",
    "        \n",
    "    def forward(self, x):        \n",
    "        return self.linear(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.615, b: -0.341 Cost: 2.147149\n",
      "Epoch  100/1000 W: 0.992, b: 0.019 Cost: 0.000052\n",
      "Epoch  200/1000 W: 0.996, b: 0.008 Cost: 0.000010\n",
      "Epoch  300/1000 W: 0.999, b: 0.002 Cost: 0.000001\n",
      "Epoch  400/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  500/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  600/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  700/1000 W: 0.984, b: -0.016 Cost: 0.002620\n",
      "Epoch  800/1000 W: 0.999, b: -0.001 Cost: 0.000006\n",
      "Epoch  900/1000 W: 0.998, b: -0.002 Cost: 0.000030\n",
      "Epoch 1000/1000 W: 0.996, b: -0.004 Cost: 0.000109\n"
     ]
    }
   ],
   "source": [
    "# For reproducibility \n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 모델 초기화 \n",
    "model = LinearRegressionModel() \n",
    "W=torch.zeros(1)\n",
    "b=torch.zeros(1)\n",
    "\n",
    "# optimizer 설정 \n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01) \n",
    "nb_epochs = 1000 \n",
    "\n",
    "for epoch in range(nb_epochs + 1):        \n",
    "\n",
    "    # H(x) 계산    \n",
    "    prediction = model(x_train)        \n",
    "\n",
    "    # cost 계산    \n",
    "    cost = F.mse_loss(prediction, y_train)        \n",
    "\n",
    "    # cost로 H(x) 개선    \n",
    "    optimizer.zero_grad() # 미분값이     \n",
    "    cost.backward()    \n",
    "    optimizer.step()        \n",
    "\n",
    "    # 100번마다 로그 출력    \n",
    "    if epoch % 100 == 0:        \n",
    "        params = list(model.parameters())        \n",
    "        W = params[0].item()        \n",
    "        b = params[1].item()        \n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()        \n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
