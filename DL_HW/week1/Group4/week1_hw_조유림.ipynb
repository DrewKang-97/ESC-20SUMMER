{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. 이 경우에서는 Type 1 error가 높을까요? Type 2 Error가 높을까요? FP, FN과 관련지어 설명해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FP: 실제 FALSE것을 TRUE라고 예측(Type2 error)\n",
    "- FN: 실제 TRUE인 것을 FALSE라고 예측(Type1 error)\n",
    "- 이 경우에 threshold가 0.3으로 매우 낮아 false를 accept라고 결정할 가능성이 높아진다 \n",
    "따라서 FP가 크고 FN이 상대적으로 낮아,\n",
    "TYPE2 에러가 더 높다고 볼 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2-1) Accuracy, Precision and Recall이 무엇인지 TP,FP,FN,TN의 식으로 나타내 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Positive(TP) : 실제 True인 정답을 True라고 예측 (정답) / \n",
    "False Positive(FP) : 실제 False인 정답을 True라고 예측 (오답) / \n",
    "False Negative(FN) : 실제 True인 정답을 False라고 예측 (오답) /\n",
    "True Negative(TN) : 실제 False인 정답을 False라고 예측 (정답) /\n",
    "\n",
    "- Precision(정밀도)\n",
    "정밀도는 true라고 분류한 것이 실제로 true일 비율 \n",
    "TP/(TP+FP)이다 \n",
    "\n",
    "- Recall(재현율,sensitivity)\n",
    "재현율은 실제로 true인 것중에 모델이 true라고 예측한 비율\n",
    "TP/(TP+FN)\n",
    "\n",
    "- TRADE OFF관계\n",
    "왜? 분자는 동일한데 분모에서 FP와 FN의 값에서 차이를 보인다. FN는 Type1 error FP는 Type2 error에 해당하기 때문에 서로 trade off 관계에 있다\n",
    "\n",
    "- Accuracy: (TP+TN) / (TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2-2) Precision and Recall에 관한 제가 든 예시가 아닌 실생활 예시 하나를 들어주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신약을 개발한다고 했을 때 치료가 가능한 경우를 TRUE, 치료가 불가능한 경우를 FALSE라고 하자. 정밀도의 경우 치료가 가능하다고 분류한 약품들 중 실제 치료가 가능한 약품의 비율을, 재현율은 실제로 치료가 가능한 약품들 중에 치료가 가능하다고 판단되어진 약품의 비율을 나타낸다. 제약회사의 입장에서 실제로 아무 효능없는 약을 효능이 있는 약이라고 판단하는 Error2(FP)를 낮추는데 포커스를 두어야한다. 즉 정밀도를 높이는데 치중을 해야 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3) 코드 따라해보고 주석 달기!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn # model을 위해 필요한 코드 포함\n",
    "import torchvision.datasets as dsets # MNIST dataset 포함\n",
    "import torchvision.transforms as transforms \n",
    "# contains various methods to transform objects into others\n",
    "# 이미지를 파이토치 tensor로 변환하기 위함(다차원 array와 유사)\n",
    "from torch.autograd import Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset (Images and Labels) \n",
    "train_dataset = dsets.MNIST(root ='./data',  \n",
    "                            train = True,  \n",
    "                            transform = transforms.ToTensor(), \n",
    "                            download = True) \n",
    "  \n",
    "test_dataset = dsets.MNIST(root ='./data',  \n",
    "                           train = False,  \n",
    "                           transform = transforms.ToTensor()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters  \n",
    "input_size = 784 #이미지 사이즈가 28*28\n",
    "num_classes = 10 #10 digits\n",
    "num_epochs = 5 #전체 데이터를 5번 train 시킬 것을 의미(forward+backward pass 총 5번 실행)\n",
    "batch_size = 100 #한번에 100개의 이미지를 train할 것 \n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train과 test로 구분\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,  \n",
    "                                           batch_size = batch_size,  \n",
    "                                           shuffle = True) \n",
    "  \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,  \n",
    "                                          batch_size = batch_size,  \n",
    "                                          shuffle = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module): \n",
    "    def __init__(self, input_size, num_classes):  #초기화\n",
    "        super(LogisticRegression, self).__init__() \n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "  \n",
    "    def forward(self, x): \n",
    "        out = self.linear(x) \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_size, num_classes) #  instantiate an object 객체를 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() #entropy loss개념을 사용해서 분류하는 기준으로 사용\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) \n",
    "#stochastic gradient descent 알고리즘을 사용해서 파라미터 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 2.2085\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 2.1026\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 2.0273\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.9163\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.8227\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.8192\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.6864\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.6733\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.6196\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.6156\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.5213\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.4732\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.3957\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.3965\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.4386\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.3860\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.2538\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.2459\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.2307\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.2849\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.1657\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.1741\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.1447\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 1.1195\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 0.9718\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 1.0953\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 1.0958\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 1.0126\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 1.0619\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 0.9230\n"
     ]
    }
   ],
   "source": [
    "# Training the Model \n",
    "for epoch in range(num_epochs): \n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        images = Variable(images.view(-1, 28 * 28)) \n",
    "        labels = Variable(labels) \n",
    "  \n",
    "        # Forward + Backward + Optimize \n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(images) \n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "  \n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, \n",
    "                     len(train_dataset) // batch_size, loss.item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images:  82 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) \n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs.data, 1) \n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum() \n",
    "  \n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % ( \n",
    "            100 * correct / total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4-1) 2-4 코드에서 optim.SGD를 사용하지않고 코드를 짠다면 어떤 방식으로 짜야할까요? 설명해주세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w: 0.280, b: 0.120 Cost: 4.666667\n",
      "Epoch  100/1000 w: 0.922, b: 0.177 Cost: 0.004582\n",
      "Epoch  200/1000 w: 0.962, b: 0.086 Cost: 0.001078\n",
      "Epoch  300/1000 w: 0.982, b: 0.042 Cost: 0.000253\n",
      "Epoch  400/1000 w: 0.991, b: 0.020 Cost: 0.000060\n",
      "Epoch  500/1000 w: 0.996, b: 0.010 Cost: 0.000014\n",
      "Epoch  600/1000 w: 0.998, b: 0.005 Cost: 0.000003\n",
      "Epoch  700/1000 w: 0.999, b: 0.002 Cost: 0.000001\n",
      "Epoch  800/1000 w: 1.000, b: 0.001 Cost: 0.000000\n",
      "Epoch  900/1000 w: 1.000, b: 0.001 Cost: 0.000000\n",
      "Epoch 1000/1000 w: 1.000, b: 0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "w = torch.zeros(1)\n",
    "b = torch.zeros(1)\n",
    "\n",
    "for epoch in range(1001):\n",
    "    \n",
    "    cost = torch.mean(((x_train*w+b)-y_train)**2)\n",
    "    w_gradient = torch.sum(2*((x_train*w+b)-y_train)*x_train)\n",
    "    b_gradient = torch.sum(2*((x_train*w+b)-y_train))\n",
    "    \n",
    "    w = w - 0.01*w_gradient\n",
    "    b = b - 0.01*b_gradient\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        print('Epoch {:4d}/1000 w: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, w.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4-2) Gradient descent 코드를 구현하는 문제입니다. 2-4의 코드에서 다른 optimizer(3개 ex)adam, rmsprop, sgd에 momentum 추가 등등)를 이용하여 결과값을 비교해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.953, b: 0.393 Cost: 15.130841\n",
      "Epoch  100/1000 W: -0.126, b: 1.203 Cost: 1.992804\n",
      "Epoch  200/1000 W: 0.232, b: 1.478 Cost: 0.398943\n",
      "Epoch  300/1000 W: 0.339, b: 1.446 Cost: 0.307047\n",
      "Epoch  400/1000 W: 0.396, b: 1.337 Cost: 0.260401\n",
      "Epoch  500/1000 W: 0.451, b: 1.216 Cost: 0.215195\n",
      "Epoch  600/1000 W: 0.507, b: 1.091 Cost: 0.173292\n",
      "Epoch  700/1000 W: 0.564, b: 0.966 Cost: 0.136046\n",
      "Epoch  800/1000 W: 0.618, b: 0.845 Cost: 0.104132\n",
      "Epoch  900/1000 W: 0.670, b: 0.730 Cost: 0.077696\n",
      "Epoch 1000/1000 W: 0.719, b: 0.622 Cost: 0.056488\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2)rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.889, b: -0.622 Cost: 24.727514\n",
      "Epoch  100/1000 W: 0.422, b: 0.655 Cost: 0.489327\n",
      "Epoch  200/1000 W: 0.633, b: 0.770 Cost: 0.091354\n",
      "Epoch  300/1000 W: 0.695, b: 0.675 Cost: 0.066548\n",
      "Epoch  400/1000 W: 0.762, b: 0.529 Cost: 0.040859\n",
      "Epoch  500/1000 W: 0.840, b: 0.354 Cost: 0.018379\n",
      "Epoch  600/1000 W: 0.917, b: 0.184 Cost: 0.004974\n",
      "Epoch  700/1000 W: 0.972, b: 0.062 Cost: 0.000581\n",
      "Epoch  800/1000 W: 0.995, b: 0.010 Cost: 0.000017\n",
      "Epoch  900/1000 W: 1.000, b: 0.001 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.000, b: 0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.180, b: 0.080 Cost: 7.746202\n",
      "Epoch  100/1000 W: 0.826, b: 0.395 Cost: 0.022440\n",
      "Epoch  200/1000 W: 0.864, b: 0.310 Cost: 0.013867\n",
      "Epoch  300/1000 W: 0.893, b: 0.244 Cost: 0.008569\n",
      "Epoch  400/1000 W: 0.916, b: 0.192 Cost: 0.005295\n",
      "Epoch  500/1000 W: 0.934, b: 0.151 Cost: 0.003272\n",
      "Epoch  600/1000 W: 0.948, b: 0.118 Cost: 0.002022\n",
      "Epoch  700/1000 W: 0.959, b: 0.093 Cost: 0.001249\n",
      "Epoch  800/1000 W: 0.968, b: 0.073 Cost: 0.000772\n",
      "Epoch  900/1000 W: 0.975, b: 0.058 Cost: 0.000477\n",
      "Epoch 1000/1000 W: 0.980, b: 0.045 Cost: 0.000295\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
