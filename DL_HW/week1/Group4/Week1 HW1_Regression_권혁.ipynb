{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1)\n",
    "\n",
    "가입이 쉽기 때문에 FN은 낮고, FP는 높을 것이다. 가입 조건을 신용도로 예를 들면, 이 경우에는 신용도가 불량해도 가입되는 경우는 있지만 신용도가 양호함에도 불구하고 가입거절되는 경우는 적다는 것이다.\n",
    "이것을 '이 고객은 신용도가 양호하다'는 귀무가설을 가설검정하는 경우라고 보면,\n",
    "귀무가설을 기각해야하는 경우에 귀무가설을 기각하지 못하는 경우가 귀무가설을 기각할 수 없음에도 귀무가설을 기각하는 경우보다 많다는 것이다.\n",
    "따라서 실제로는 거짓인 귀무가설을 기각하지 못하는 경우인 Type 2 error가 그 반대 경우인 type 1 error보다 높을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2-1)\n",
    "\n",
    "accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "recall = TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2-2)\n",
    "\n",
    "어제 뉴스 예보에서는 비가 온다고 했지만 밖이 몹시 맑다면 우산을 챙겨야 할까?\n",
    "\n",
    "(우산을 사는 건 제외)\n",
    "\n",
    "이 경우 우산을 챙겼지만 비가 오지 않는 경우, 우산을 챙기지 않았지만 비가 와서 비에 맞는 경우가 있을 것이다.\n",
    "\n",
    "이 예시에서 성공확률의 threshold를 높인다는 것은 비가 올 것이라고 판단해 우산을 챙길 조건이 더 까다로워 진다는 것이다. \n",
    "\n",
    "(ex) 예상 강수량이 10mm 이상 이었을 때 비가 올 것이라 판단, 우산 챙기기 => 예상강수량이 30mm 이상이었을 때 비가 올 것이라 판단, 우산 챙기기)\n",
    "\n",
    "이 경우에서는 threshold를 높이지 않는 편이 좋다. 우산을 챙겼을 때 비가 오지 않는 경우에는 그냥 우산을 들고다니거나 크기에 따라 가방에 넣을 수도 있을 것이지만, 우산을 챙기지 않았는데 비가 올 경우엔 몹시 난감하기 때문이다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 2.2231\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 2.1668\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 2.0244\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.9992\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.9119\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.7972\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.8134\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.7368\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.6889\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.6367\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.5073\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.4789\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.3699\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.3943\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.3038\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.3619\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.2047\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.2787\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.2015\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.2069\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.2000\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.1272\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.1530\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 1.1399\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 1.1714\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 0.9974\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 1.0726\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 0.9628\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 1.0173\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 1.0555\n",
      "Accuracy of the model on the 10000 test images:  82 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 import\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision.datasets as dsets \n",
    "import torchvision.transforms as transforms \n",
    "from torch.autograd import Variable \n",
    "\n",
    "\n",
    "# MNIST Dataset (Images and Labels) \n",
    "batch_size = 100\n",
    "train_dataset = dsets.MNIST(root ='./data', train = True, transform = transforms.ToTensor(), download = True) \n",
    "\n",
    "test_dataset = dsets.MNIST(root ='./data', train = False, transform = transforms.ToTensor()) \n",
    "\n",
    "# Dataset Loader (Input Pipline) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True) \n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False) \n",
    "\n",
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model \n",
    "class LogisticRegression(nn.Module): \n",
    "    def __init__(self, input_size, num_classes): \n",
    "        super(LogisticRegression, self).__init__() \n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "\n",
    "    def forward(self, x): \n",
    "        out = self.linear(x) \n",
    "        return out \n",
    "\n",
    "\n",
    "model = LogisticRegression(input_size, num_classes) \n",
    "\n",
    "# Loss and Optimizer \n",
    "# Softmax is internally computed. \n",
    "# Set parameters to be updated. \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) \n",
    "\n",
    "# Training the Model \n",
    "for epoch in range(num_epochs): \n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        images = Variable(images.view(-1, 28 * 28)) \n",
    "        labels = Variable(labels) \n",
    "\n",
    "        # Forward + Backward + Optimize \n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(images) \n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'% (epoch + 1, num_epochs, i + 1, \n",
    "                                                                    len(train_dataset) // batch_size, loss.data)) \n",
    "\n",
    "# Test the Model \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) \n",
    "    outputs = model(images) _, predicted = torch.max(outputs.data, 1) \n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum() \n",
    "\n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % (100 * correct / total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD는 mini batch (일부 데이터의 모음)에 대해 Loss 계산을 하는데, \n",
    "이것을 함수로 짜서 계산을 해본다고 한다면 (x,y,theta초깃값, iteration, batch size, learning rate)에 관한 함수일 것이고\n",
    "iteration에 관한 for문을 하나 만들고 다시 그 안의 for문에서 mini batch가 무작위하게 선정되면\n",
    "그 mini batch에서의 Loss를 구한 후 grad(=d loss/ d theta) 를 구한다.\n",
    "그 다음엔 theta = theta - grad * learning rate로 theta를 update한다.\n",
    "그리고 이 밖에 있는 for문으로 지정한 iteration 만큼 반복하여 theta 값을 계산한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 구현해보려 했으나 값이 이상하게 튀는 것으로 보아 실패\n",
    "def SGD(x,y,theta, num_iters, batch_size, learning_rate) : \n",
    "    for i in range(num_iters) :\n",
    "        cost = []\n",
    "        \n",
    "        for s in range(batch_size) : \n",
    "            v1 = random.randrange(len(x))\n",
    "            x_ = x[v1]\n",
    "            pred_y = theta * x_\n",
    "            y_ = y[v1]\n",
    "            differed = 2*x_*(pred_y - y_)\n",
    "            cost.append(differed)\n",
    "            \n",
    "        gradient = np.mean(y_c)\n",
    "        \n",
    "        theta -= gradient*learning_rate\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.093, b: -0.323 Cost: 6.466759\n",
      "Epoch  100/1000 W: 0.978, b: 0.050 Cost: 0.000363\n",
      "Epoch  200/1000 W: 0.983, b: 0.039 Cost: 0.000224\n",
      "Epoch  300/1000 W: 0.986, b: 0.031 Cost: 0.000139\n",
      "Epoch  400/1000 W: 0.989, b: 0.024 Cost: 0.000086\n",
      "Epoch  500/1000 W: 0.992, b: 0.019 Cost: 0.000053\n",
      "Epoch  600/1000 W: 0.993, b: 0.015 Cost: 0.000033\n",
      "Epoch  700/1000 W: 0.995, b: 0.012 Cost: 0.000020\n",
      "Epoch  800/1000 W: 0.996, b: 0.009 Cost: 0.000012\n",
      "Epoch  900/1000 W: 0.997, b: 0.007 Cost: 0.000008\n",
      "Epoch 1000/1000 W: 0.997, b: 0.006 Cost: 0.000005\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.281, b: -0.129 Cost: 2.901676\n",
      "Epoch  100/1000 W: 0.827, b: 0.384 Cost: 0.021464\n",
      "Epoch  200/1000 W: 0.855, b: 0.321 Cost: 0.015061\n",
      "Epoch  300/1000 W: 0.886, b: 0.254 Cost: 0.009390\n",
      "Epoch  400/1000 W: 0.914, b: 0.190 Cost: 0.005275\n",
      "Epoch  500/1000 W: 0.939, b: 0.136 Cost: 0.002687\n",
      "Epoch  600/1000 W: 0.958, b: 0.092 Cost: 0.001244\n",
      "Epoch  700/1000 W: 0.973, b: 0.060 Cost: 0.000524\n",
      "Epoch  800/1000 W: 0.983, b: 0.037 Cost: 0.000201\n",
      "Epoch  900/1000 W: 0.990, b: 0.022 Cost: 0.000070\n",
      "Epoch 1000/1000 W: 0.994, b: 0.012 Cost: 0.000022\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.189, b: -0.538 Cost: 11.455127\n",
      "Epoch  100/1000 W: 0.791, b: 0.393 Cost: 0.030209\n",
      "Epoch  200/1000 W: 0.845, b: 0.343 Cost: 0.017207\n",
      "Epoch  300/1000 W: 0.887, b: 0.251 Cost: 0.009225\n",
      "Epoch  400/1000 W: 0.932, b: 0.150 Cost: 0.003301\n",
      "Epoch  500/1000 W: 0.971, b: 0.064 Cost: 0.000606\n",
      "Epoch  600/1000 W: 0.993, b: 0.016 Cost: 0.000037\n",
      "Epoch  700/1000 W: 0.999, b: 0.002 Cost: 0.000000\n",
      "Epoch  800/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  900/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.014, b: 0.014 Cost: 0.002414\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sgd + momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.405, b: 0.892 Cost: 0.375979\n",
      "Epoch  100/1000 W: 0.876, b: 0.283 Cost: 0.011769\n",
      "Epoch  200/1000 W: 0.965, b: 0.079 Cost: 0.000919\n",
      "Epoch  300/1000 W: 0.990, b: 0.022 Cost: 0.000072\n",
      "Epoch  400/1000 W: 0.997, b: 0.006 Cost: 0.000006\n",
      "Epoch  500/1000 W: 0.999, b: 0.002 Cost: 0.000000\n",
      "Epoch  600/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  700/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  800/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  900/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.000, b: 0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.8)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
