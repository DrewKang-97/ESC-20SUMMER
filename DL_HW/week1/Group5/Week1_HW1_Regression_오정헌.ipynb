{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "threshold가 0.3으로 가입이 쉬워진다면 실제로는 자격이 없지만 가입될 수 있는 경우가 늘어나기 때문에\n",
    "다시 말해 FP가 커지기 때문에 Type 1 Error가 늘어납니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 1\n",
    "Accuracy : 정확도 , 모델의 예측이 성공한 비율 -> (TP+TN) / (TP+TN+FP+FN)\n",
    "Precision : 정밀도, 모델이 True라고 예측한 값들 중 실제로 True인 경우의 비율 -> (TP) / (TP + FP)\n",
    "Recall : 재현율, 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율 -> (TP) / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 2\n",
    "범죄에 대한 판결의 경우를 생각한다면 실제 경우는 죄가 있음, 죄가 없음, 예측은 판결이 되어 유죄판결, 무죄판결\n",
    "Precision은 유죄판결은 내린 경우 중 실제로 죄가 있는 비율\n",
    "Recall은 실제로 죄가 있는 경우 중 유죄판결을 받은 비율\n",
    "\n",
    "이 경우 threshold를 높인다면 유죄를 판결받을 확률이 낮아집니다. 그렇다면 1종의 오류인 FP가 낮아지고 FN은 높아지는데 \n",
    "바꿔 생각해보면 죄가 있는 사람에게 무죄판결을 하는 경우는 많아지고 죄가 없는 사람에게 유죄판결을 하는 경우는 적어지는데\n",
    "많은 범죄자를 놓치더라도 한 명의 억울한 판결을 받는 경우를 막을 수 있기 때문에 합리적이라고 생각합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision.datasets as dsets \n",
    "import torchvision.transforms as transforms \n",
    "from torch.autograd import Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset (Images and Labels) \n",
    "train_dataset = dsets.MNIST(root ='./data',  \n",
    "                            train = True,  \n",
    "                            transform = transforms.ToTensor(), \n",
    "                            download = True) \n",
    "  \n",
    "test_dataset = dsets.MNIST(root ='./data',  \n",
    "                           train = False,  \n",
    "                           transform = transforms.ToTensor()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters  \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loader (Input Pipline) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,  \n",
    "                                           batch_size = batch_size,  \n",
    "                                           shuffle = True) \n",
    "  \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,  \n",
    "                                          batch_size = batch_size,  \n",
    "                                          shuffle = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module): \n",
    "    def __init__(self, input_size, num_classes): \n",
    "        super(LogisticRegression, self).__init__() \n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "  \n",
    "    def forward(self, x): \n",
    "        out = self.linear(x) \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model\n",
    "model = LogisticRegression(input_size, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [100/600], Loss: 2.2297\n",
      "Epoch: [1/5], Step: [200/600], Loss: 2.1092\n",
      "Epoch: [1/5], Step: [300/600], Loss: 2.0007\n",
      "Epoch: [1/5], Step: [400/600], Loss: 1.9391\n",
      "Epoch: [1/5], Step: [500/600], Loss: 1.8162\n",
      "Epoch: [1/5], Step: [600/600], Loss: 1.7962\n",
      "Epoch: [2/5], Step: [100/600], Loss: 1.6750\n",
      "Epoch: [2/5], Step: [200/600], Loss: 1.6059\n",
      "Epoch: [2/5], Step: [300/600], Loss: 1.6214\n",
      "Epoch: [2/5], Step: [400/600], Loss: 1.6023\n",
      "Epoch: [2/5], Step: [500/600], Loss: 1.5166\n",
      "Epoch: [2/5], Step: [600/600], Loss: 1.4229\n",
      "Epoch: [3/5], Step: [100/600], Loss: 1.4924\n",
      "Epoch: [3/5], Step: [200/600], Loss: 1.4630\n",
      "Epoch: [3/5], Step: [300/600], Loss: 1.2327\n",
      "Epoch: [3/5], Step: [400/600], Loss: 1.3669\n",
      "Epoch: [3/5], Step: [500/600], Loss: 1.2947\n",
      "Epoch: [3/5], Step: [600/600], Loss: 1.2579\n",
      "Epoch: [4/5], Step: [100/600], Loss: 1.2831\n",
      "Epoch: [4/5], Step: [200/600], Loss: 1.2135\n",
      "Epoch: [4/5], Step: [300/600], Loss: 1.1582\n",
      "Epoch: [4/5], Step: [400/600], Loss: 1.1164\n",
      "Epoch: [4/5], Step: [500/600], Loss: 1.2638\n",
      "Epoch: [4/5], Step: [600/600], Loss: 1.1065\n",
      "Epoch: [5/5], Step: [100/600], Loss: 0.9935\n",
      "Epoch: [5/5], Step: [200/600], Loss: 1.1274\n",
      "Epoch: [5/5], Step: [300/600], Loss: 1.0785\n",
      "Epoch: [5/5], Step: [400/600], Loss: 1.0591\n",
      "Epoch: [5/5], Step: [500/600], Loss: 1.1052\n",
      "Epoch: [5/5], Step: [600/600], Loss: 0.9269\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28 * 28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f'\n",
    "                 % (epoch + 1, num_epochs, i+1,\n",
    "                   len(train_dataset) // batch_size, loss.data))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 82 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28 * 28))\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print('Accuracy of the model on the 10000 test images: %d %%' %(\n",
    "       100* correct / total ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ac436ad3d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.578, b: -0.413 Cost: 2.147149\n",
      "Epoch  100/1000 W: 1.066, b: -0.150 Cost: 0.003239\n",
      "Epoch  200/1000 W: 1.052, b: -0.118 Cost: 0.002002\n",
      "Epoch  300/1000 W: 1.041, b: -0.093 Cost: 0.001237\n",
      "Epoch  400/1000 W: 1.032, b: -0.073 Cost: 0.000764\n",
      "Epoch  500/1000 W: 1.025, b: -0.057 Cost: 0.000472\n",
      "Epoch  600/1000 W: 1.020, b: -0.045 Cost: 0.000292\n",
      "Epoch  700/1000 W: 1.016, b: -0.035 Cost: 0.000180\n",
      "Epoch  800/1000 W: 1.012, b: -0.028 Cost: 0.000111\n",
      "Epoch  900/1000 W: 1.010, b: -0.022 Cost: 0.000069\n",
      "Epoch 1000/1000 W: 1.008, b: -0.017 Cost: 0.000043\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 데이터는 전체 양이 그리 많지 않기 때문에 SGD 대신 GD를 사용해도 크게 문제가 없을 것 같습니다.\n",
    "# 미분에 대한 함수식을 짭니다. 미분 => lim {f(x+h)-f(x)}/h (h->0)이니\n",
    "# h에 매우 작은 값을 집어 넣는다면 근사값을 구할 수 있을 것입니다.\n",
    "# Loss function을 W와 b를 이용하여 다시 짠 다음 편미분을 통해 계속해서 gradient를 갱신하여 \n",
    "# 우리가 아는 식의 방법으로 weight를 계속해서 update하면 될 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ac436ad3d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.525, b: -0.431 Cost: 2.147149\n",
      "Epoch  100/1000 W: 0.992, b: 0.032 Cost: 0.000333\n",
      "Epoch  200/1000 W: 0.990, b: 0.022 Cost: 0.000072\n",
      "Epoch  300/1000 W: 0.993, b: 0.017 Cost: 0.000040\n",
      "Epoch  400/1000 W: 0.995, b: 0.012 Cost: 0.000020\n",
      "Epoch  500/1000 W: 0.997, b: 0.008 Cost: 0.000008\n",
      "Epoch  600/1000 W: 0.998, b: 0.005 Cost: 0.000003\n",
      "Epoch  700/1000 W: 0.999, b: 0.003 Cost: 0.000001\n",
      "Epoch  800/1000 W: 0.999, b: 0.002 Cost: 0.000000\n",
      "Epoch  900/1000 W: 1.000, b: 0.001 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.000, b: 0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정 -> Adam 알고리즘 사용 -> SGD와 달리 1보다 작은 쪽에서 점점 1에 가까워짐\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ac436ad3d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.615, b: -0.341 Cost: 2.147149\n",
      "Epoch  100/1000 W: 0.992, b: 0.019 Cost: 0.000052\n",
      "Epoch  200/1000 W: 0.996, b: 0.008 Cost: 0.000010\n",
      "Epoch  300/1000 W: 0.999, b: 0.002 Cost: 0.000001\n",
      "Epoch  400/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  500/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  600/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  700/1000 W: 0.984, b: -0.016 Cost: 0.002620\n",
      "Epoch  800/1000 W: 0.999, b: -0.001 Cost: 0.000006\n",
      "Epoch  900/1000 W: 0.998, b: -0.002 Cost: 0.000030\n",
      "Epoch 1000/1000 W: 0.996, b: -0.004 Cost: 0.000109\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정 -> RMSprop 알고리즘 사용 -> 1로 미분값이 0이어도 계속 구하면서 이동함\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ac436ad3d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.578, b: -0.413 Cost: 2.147149\n",
      "Epoch  100/1000 W: 1.007, b: -0.008 Cost: 0.000053\n",
      "Epoch  200/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  300/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  400/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  500/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  600/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  700/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  800/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  900/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.000, b: -0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정 -> Momentum 추가 -> momentum이 크다 ex)1이면 크게 이동해서 잘 못찾았음, 반면 작으면 ex)0.01은 없는것과(0과) 비슷\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) # 보통 0.9사용\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
