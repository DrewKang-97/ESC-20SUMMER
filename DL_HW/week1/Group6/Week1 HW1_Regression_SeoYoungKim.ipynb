{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1)\n",
    "\n",
    "가설검정과 모델 분류와 연결지어 생각해보겠다. \n",
    "- 가설검정에서의 $H_0$는 모델 분류에서의 실제 정답, 위 예시에서는 실제 해당 고객이 가입 요건에 충족하는지 여부에 대한 것이다.\n",
    "또한, 가설검정에서의 $H_0$의 기각 여부는 모델 분류에서의 분류결과이며, 위 예시에서는 해당 고객의 가입에 대한 승인 여부와 비슷하다. $H_0$를 기각하는 것은 Negative라고 판단하는 것과 일맥상통한다.  <br>\n",
    "- 위 예시의 가입 승인 비율이 높은 것으로 보아, 가입 요건에 충족하지 않음에도 승인해준 경우에 해당되는 FP의 비율은 높지만 반대로 FN의 비율은 낮음을 알 수 있다. 이를 가설검정으로 적용해보면, **FP와 비슷한 맥락의 제 1종 오류는 높지만 FN과 관계가 깊은 제 2종 오류는 낮다**고 볼 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2-1)\n",
    "- Accurary(정확도) $= \\frac{TP+TN}{TP+FN+FP+TN}$ \n",
    " - 정확도는 모든 경우의 수 중에서 옳은 결정을 하는 것의 비율이며, 이때의 옳은 결정은 실제 Positive를 Positive로 분류하거나, Negative를 Negative로 분류하는 것이다. <br>\n",
    "<br>\n",
    "- Precision(정밀도) $= \\frac{TP}{TP+FP}$\n",
    " - 정밀도는 모델이 Positive로 분류한 것 중에서 실제로도 Positive한 것의 비율이다. <br>\n",
    "<br>\n",
    "- Recall(재현율) $= \\frac{TP}{TP+FN}$\n",
    " - 재현율은 실제 Positive한 것 중에서 모델이 Positive로 분류한 것의 비율이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2-2)\n",
    "※ *성공확률의 threshold를 늘리면 그만큼 조건이 까다로워져서 FP가 낮아지고 FN이 높아진다.* <br> \n",
    "   *그대신 판단 기준이 까다로워진 만큼 Positive로 분류할 확률은 줄어든다.* \n",
    "\n",
    "- 중고등학교에서 아프다는 핑계로 야간자율학습을 빼고 조퇴하는 경우에 담당 교사의 판단을 예시로 들 수 있다. 학생들이 실제로 아픈 것과 아픈 척하는 것에 대해 담당 교사가 조퇴 여부에 옳은 판단을 내릴지와 연결지을 수 있다. FP를 예로 들면, 실제로는 학생이 야자를 째고 놀러가려고 아픈 척 연기를 하는 것인데 담당 교사가 진짜 아픈 줄 알고 조퇴를 시켜주는 경우가 해당된다. <br>\n",
    "- 이때 성공확률의 treshold를 늘리는 것은, 교사의 판단 조건이 까다로워지는 것과 일맥상통하며 합리적이라고 할 수 있다. 꾀를 부려 야자를 상습적으로 째려고 하는 학생들을 바로잡아 올바른 방향으로 지도할 수 있기 때문이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키기 불러오기 \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision.datasets as dsets \n",
    "import torchvision.transforms as transforms \n",
    "from torch.autograd import Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters  \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset (Images and Labels) \n",
    "\n",
    "# 과적합 방지를 위해 train과 test set으로 나누기 \n",
    "train_dataset = dsets.MNIST(root ='./mnist_data/',  \n",
    "                            train = True,  \n",
    "                            transform = transforms.ToTensor(),  # transforms.ToTensor():Image를 PyTorch로 변환\n",
    "                            download = True) \n",
    "  \n",
    "test_dataset = dsets.MNIST(root ='./mnist_data/',  \n",
    "                           train = False,  \n",
    "                           transform = transforms.ToTensor()) \n",
    "  \n",
    "    \n",
    "# Dataset Loader (Input Pipline) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,  \n",
    "                                           batch_size = batch_size,  \n",
    "                                           shuffle = True)  # 전체 데이터에서 batch_size만큼 불러올때 무작위로 불러올지 여부 \n",
    "  \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,  \n",
    "                                          batch_size = batch_size,  \n",
    "                                          shuffle = False) # test 셋은 shuffle 시행 안함 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class 활용해서 모델 정의 \n",
    "class LogisticRegression(nn.Module): \n",
    "    def __init__(self, input_size, num_classes):  \n",
    "        super(LogisticRegression, self).__init__()  # super-class인 nn.Module의 __init__불러오기 \n",
    "        self.linear = nn.Linear(input_size, num_classes) # 단층 propagation \n",
    "  \n",
    "    def forward(self, x): \n",
    "        out = self.linear(x) # out: y_prediction \n",
    "        return out \n",
    "    \n",
    "# 모델 초기화     \n",
    "model = LogisticRegression(input_size, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 함수 정의 \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# optimizer 설정 \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 2.0283\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 1.9679\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 1.8286\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.7713\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.7940\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.6702\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.5771\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.5821\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.4949\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.4766\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.4371\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.4347\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.3263\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.2704\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.2623\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.3267\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.1119\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.2522\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.1629\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.1822\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.0681\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.1094\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.0644\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 1.1093\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 1.0724\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 1.0613\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 1.1368\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 1.0364\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 1.0447\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 1.0209\n"
     ]
    }
   ],
   "source": [
    "# Training the Model \n",
    "for epoch in range(num_epochs): \n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        images = Variable(images.view(-1, 28 * 28)) \n",
    "        labels = Variable(labels) \n",
    "  \n",
    "        # Forward + Backward + Optimize \n",
    "        optimizer.zero_grad() # 모든 gradient 0으로 \n",
    "        outputs = model(images) \n",
    "        loss = criterion(outputs, labels)  #loss 계산 \n",
    "        # 여기까지는 forward pass\n",
    "        \n",
    "        loss.backward() #back-propagation\n",
    "        optimizer.step() #weight 업데이트 \n",
    "  \n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, \n",
    "                     len(train_dataset) // batch_size, loss.data)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images:  82 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model \n",
    "correct = 0 \n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) \n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs.data, 1) \n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum() \n",
    "  \n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % ( \n",
    "            100 * correct / total)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4-1)\n",
    "```optim.SGD()```는 모델을 최적화하기 위한 확률적 경사 하강법의 함수다. PyTorch에서는 이와 같이 자동적으로 gradient를 계산 해주지만, 해당 코드를 사용하지 않고 코드를 짠다면 NumPy를 사용하면 된다. <br>\n",
    "※ 참고로 PyTorch는 NumPy와 매우 유사하지만 GPU를 사용하여 수치 연산을 가속화할 수 있다는 점에서 좀 더 효율적이다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y): \n",
    "    return 2 * x * (x * w - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 gradient함수를 NumPy연산으로 정의한 후, 이를 활용해서 gradient를 계산하고 weight를 업데이트하면 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4-2)\n",
    "참고: https://wjddyd66.github.io/dl/NeuralNetwork-(3)-Optimazation2/#optimazation-%EA%B3%A0%EB%A0%A4%EC%82%AC%ED%95%AD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20f73e22730>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - SGD + momentum\n",
    "**momentum**은 Local Minima에 덜빠지기 위해 직전에 계산된 기울기를 고려해서 새로 계산된 기울기와 일정한 비율로 계산하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.440, b: -0.357 Cost: 3.021716\n",
      "Epoch  100/1000 W: 1.005, b: -0.003 Cost: 0.000059\n",
      "Epoch  200/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  300/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  400/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  500/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  600/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  700/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  800/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  900/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.000, b: -0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - RMSProp\n",
    "- ***RMSProp**은 Adagrad의 단점을 해결하기 위한 방법이다.\n",
    " - Adagrad는 변수들을 업데이트할때 각 변수마다 step size를 다르게 설정해서 이동하는 방식이다. <br>\n",
    " *'지금까지 많이 변화하지 않은 변수들은 step size를 크게, 많이 변화해왔으면 이제는 step size를 작게'* <br>\n",
    " 왜냐면 적게 변화한 경우 그만큼 optimum에 도달하려면 더 많이 이동해야하기 때문이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.774, b: 0.211 Cost: 0.014099\n",
      "Epoch  100/1000 W: 0.999, b: 0.003 Cost: 0.000001\n",
      "Epoch  200/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  300/1000 W: 1.000, b: -0.000 Cost: 0.000001\n",
      "Epoch  400/1000 W: 0.999, b: -0.001 Cost: 0.000016\n",
      "Epoch  500/1000 W: 0.998, b: -0.002 Cost: 0.000056\n",
      "Epoch  600/1000 W: 0.997, b: -0.003 Cost: 0.000107\n",
      "Epoch  700/1000 W: 0.995, b: -0.005 Cost: 0.000262\n",
      "Epoch  800/1000 W: 0.994, b: -0.006 Cost: 0.000313\n",
      "Epoch  900/1000 W: 0.995, b: -0.005 Cost: 0.000227\n",
      "Epoch 1000/1000 W: 0.995, b: -0.005 Cost: 0.000218\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Adam \n",
    "- **Adam**(Adaptive Moment Estimation)은 RMSProp과 Momentum을 합친 것 같은 알고리즘이다. Momentum과 비슷하게 이전에 계산해온 기울기의 지수평균을 저장하며, RMSProp과 유사하게 기울기의 제곱값의 지수평균을 저장한다. <br>\n",
    "- 다만 초기에는 weight 업데이트 속도가 느리다는 단점이 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.362, b: -0.594 Cost: 12.469952\n",
      "Epoch  100/1000 W: 0.442, b: 0.204 Cost: 1.070746\n",
      "Epoch  200/1000 W: 0.733, b: 0.462 Cost: 0.053716\n",
      "Epoch  300/1000 W: 0.787, b: 0.466 Cost: 0.031823\n",
      "Epoch  400/1000 W: 0.808, b: 0.427 Cost: 0.026483\n",
      "Epoch  500/1000 W: 0.827, b: 0.384 Cost: 0.021406\n",
      "Epoch  600/1000 W: 0.847, b: 0.340 Cost: 0.016803\n",
      "Epoch  700/1000 W: 0.866, b: 0.297 Cost: 0.012817\n",
      "Epoch  800/1000 W: 0.885, b: 0.255 Cost: 0.009499\n",
      "Epoch  900/1000 W: 0.902, b: 0.217 Cost: 0.006840\n",
      "Epoch 1000/1000 W: 0.918, b: 0.181 Cost: 0.004782\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
