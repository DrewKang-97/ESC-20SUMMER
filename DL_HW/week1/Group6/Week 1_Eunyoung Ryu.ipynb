{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1)  이 경우 Type 1 error와 Type 2 error 중 무엇이 더 높을까요?\n",
    "\n",
    "FP는 실제 답이 False일때 True라고 예측 즉 긍정을 잘못한 것, FN은 실제 답이 True일때 False라고 예측 즉 부정을 잘못한 것.\n",
    "\n",
    "실제로 참인 귀무가설을 잘못 기각하는 경우가 type 1 error고, 실제로 거짓인 귀무가설을 기각하지 않는 경우가 type 2 error다.\n",
    "\n",
    "threshold가 낮은 경우 FP가 높고 FN이 낮을 것이다. 따라서 type 2 error가 더 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2-1) Accuracy, Precision and Recall이 무엇인지 TP,FP,FN,TN의 식으로 나타내 주세요.\n",
    "Precision은 정밀도로, True로 분류된 것 중 실제 True의 비율이다. TP/(TP+FP)\n",
    "\n",
    "Recall은 재현율로, 실제 True 중 모델이 True로 분류한 것의 비율이다. TP/(TP+FN)\n",
    "\n",
    "Accuracy는 정확도로, True를 True로 False를 False로 옳게 예측한 경우를 둘다 고려하는 지표이다. (TP+TN)/(TP+TN+FP+FN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2-2) Precision and Recall에 관한 실생활 예시 하나를 들어주세요. \n",
    "임신 판정에 대해 생각해보자. 이 경우 임신한 사람을 임신하지 않았다고 판단하는 FN이 임신하지 않은 사람을 임신했다고 하는 FP보다 더 위험하다. 따라서 precision보다 recall을 우선시해야 하며, threshold를 낮춰서 FN이 낮도록 하는 것이 합리적일 것 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3) 코드 따라하고 주석 달기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision.datasets as dsets \n",
    "import torchvision.transforms as transforms \n",
    "from torch.autograd import Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset (Images and Labels) \n",
    "train_dataset = dsets.MNIST(root ='./data',  \n",
    "                            train = True,  \n",
    "                            transform = transforms.ToTensor(), \n",
    "                            download = True) \n",
    "  \n",
    "test_dataset = dsets.MNIST(root ='./data',  \n",
    "                           train = False,  \n",
    "                           transform = transforms.ToTensor()) \n",
    "\n",
    "# Hyper Parameters  \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "  \n",
    "# Dataset Loader (Input Pipline) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,  \n",
    "                                           batch_size = batch_size,  \n",
    "                                           shuffle = True) \n",
    "  \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,  \n",
    "                                          batch_size = batch_size,  \n",
    "                                          shuffle = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module): \n",
    "    def __init__(self, input_size, num_classes): \n",
    "        super(LogisticRegression, self).__init__() \n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "  \n",
    "    def forward(self, x): \n",
    "        out = self.linear(x) \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_size, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 2.1184\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 1.9859\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 1.8862\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.8568\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.7613\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.7514\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.6718\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.6260\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.5068\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.5223\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.4675\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.4272\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.3553\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.3728\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.3420\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.2926\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.2508\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.2172\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.1916\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.1581\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.2056\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.1399\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.1069\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 1.0980\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 1.1278\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 1.0992\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 1.1028\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 1.0797\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 1.0946\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 1.0393\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad() # Reset all gradients to 0\n",
    "        outputs = model(images) # Make a forward pass\n",
    "        loss = criterion(outputs, labels) # Calculate the loss\n",
    "        loss.backward() # Perform backpropagation\n",
    "        optimizer.step() # Update all weights\n",
    "        \n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch+1, num_epochs, i+1, len(train_dataset) // batch_size, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images:  82 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) \n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs.data, 1) \n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum() \n",
    "  \n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % ( 100 * correct / total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4-1) 2-4 코드에서 optim.SGD를 사용하지않고 코드를 짠다면?\n",
    "\n",
    "for 구문에 weight를 업데이트 하는 (w.data = w.data - lr * w.grad.item( )) 과\n",
    "\n",
    "다시 초기화하는 (w.grad.data.zero_( ))를 넣으면 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4-2) Gradient descent 코드 구현 (adam, rmsprop, sgd에 momentum 추가) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.665, b: 0.123 Cost: 0.411208\n",
      "Epoch  100/1000 W: 0.905, b: 0.212 Cost: 0.006594\n",
      "Epoch  200/1000 W: 0.958, b: 0.092 Cost: 0.001258\n",
      "Epoch  300/1000 W: 0.987, b: 0.030 Cost: 0.000131\n",
      "Epoch  400/1000 W: 0.997, b: 0.007 Cost: 0.000008\n",
      "Epoch  500/1000 W: 0.999, b: 0.001 Cost: 0.000000\n",
      "Epoch  600/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  700/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  800/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  900/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.000, b: 0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "#Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs, W, b, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.671, b: -0.275 Cost: 17.438334\n",
      "Epoch  100/1000 W: 0.502, b: 0.836 Cost: 0.195549\n",
      "Epoch  200/1000 W: 0.632, b: 0.810 Cost: 0.096204\n",
      "Epoch  300/1000 W: 0.704, b: 0.657 Cost: 0.063028\n",
      "Epoch  400/1000 W: 0.791, b: 0.464 Cost: 0.031587\n",
      "Epoch  500/1000 W: 0.881, b: 0.264 Cost: 0.010243\n",
      "Epoch  600/1000 W: 0.953, b: 0.105 Cost: 0.001624\n",
      "Epoch  700/1000 W: 0.990, b: 0.023 Cost: 0.000078\n",
      "Epoch  800/1000 W: 0.999, b: 0.002 Cost: 0.000001\n",
      "Epoch  900/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 0.982, b: -0.018 Cost: 0.001760\n"
     ]
    }
   ],
   "source": [
    "#RMS prop\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = LinearRegressionModel()\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    optimizer.zero_grad() # 미분값이\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs, W, b, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.683, b: 0.406 Cost: 0.141928\n",
      "Epoch  100/1000 W: 0.991, b: 0.023 Cost: 0.000081\n",
      "Epoch  200/1000 W: 1.000, b: 0.001 Cost: 0.000000\n",
      "Epoch  300/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  400/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  500/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  600/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  700/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  800/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  900/1000 W: 1.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.000, b: 0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "#SGD에 Momentum 추가 \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = LinearRegressionModel()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) \n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs, W, b, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
