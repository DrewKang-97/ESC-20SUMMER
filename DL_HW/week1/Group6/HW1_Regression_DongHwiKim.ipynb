{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q1)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 threshold가 0.3으로 매우 낮아 acceptance rate가 매우 높다.\n",
    "그러므로 FP가 높고 FN가 낮다.\n",
    "\n",
    "FN은 Type 1 Error와 같은 개념이며, FP는 Type 2 Error와 같은 개념이다.\n",
    "\n",
    "그러므로 이 경우에서는 Type 2 Error가 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q2-1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy : 전체 중 제대로 맞춘 비율\n",
    "\n",
    "Accuracy = (TP +TN) / (TP + FN + FP+ TN)\n",
    "\n",
    "\n",
    "Precision: True라고 판단한 것 중 실제로 True인 것의 비율\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall: 실제로 True일 때, True라고 판단한 것의 비율\n",
    "\n",
    "Recall = TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q2-2)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "날씨를 예측하는 시스템\n",
    "\n",
    "내일 날씨가 맑을 지 아니면 비가 내릴 지를 예측하는 시스템이 있다고 가정한다.  \n",
    "\n",
    "\n",
    "그리고 비가 온다는 것을 True라고 하자.  \n",
    "\n",
    "\n",
    "이때 이 시스템을 어느 정도 믿을 것인지에 대한 것이 threshold가 될 것이다.\n",
    "\n",
    "이 때 True가 될 확률의 threshold를 높인다면 날씨 예측 시스템의 결과를 웬만해선 잘 믿지 않는 것이다.  \n",
    "\n",
    "즉, 시스템에서 비가 온다 하여도 이를 믿지 않는 것이다.\n",
    "\n",
    "이 예시에서는 True가 될 확률의 threshold를 낮추는 것이 더 합리적일 것이라 생각한다.\n",
    "\n",
    "비가 온다는 시스템의 결과를 믿지 않았는데 비가 오면 우산을 사야하는 비용적 손해가 있지만,  \n",
    "\n",
    "비가 온다는 시스템을 믿었는데 비가 오지 않을 때는 이러한 비용적 손해는 존재하지 않기 때문이다.  \n",
    "\n",
    "(짐이 늘어난다는 거 빼곤 손해볼 게 없을 것이다.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 2.2164\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 2.1189\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 2.0428\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.9594\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.9025\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.7986\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.7351\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.6751\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.5865\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.5560\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.5834\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.4815\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.3608\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.4468\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.3153\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.3212\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.2410\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.2144\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.2006\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.1102\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.1330\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.1189\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.1377\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 1.1593\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 1.0641\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 1.1198\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 1.0966\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 1.0422\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 1.0456\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 1.0168\n",
      "Accuracy of the model on the 10000 test images:  82 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "#torch 불러오기\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision.datasets as dsets \n",
    "import torchvision.transforms as transforms \n",
    "from torch.autograd import Variable \n",
    "\n",
    "# MNIST 데이터셋 불러오기 (train과 test 데이터셋으로 나누어 부름)\n",
    "train_dataset = dsets.MNIST(root ='./data',  \n",
    "                            train = True,  \n",
    "                            transform = transforms.ToTensor(), \n",
    "                            download = True) \n",
    "  \n",
    "test_dataset = dsets.MNIST(root ='./data',  \n",
    "                           train = False,  \n",
    "                           transform = transforms.ToTensor()) \n",
    "\n",
    "\n",
    "#기본적인 parameter 값 설정\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "  \n",
    "# 데이터셋을 불러올 loader 설정\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,batch_size = batch_size, shuffle = True) \n",
    "  \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size,shuffle = False) \n",
    "\n",
    "\n",
    "\n",
    "#로지스틱 회귀와 forward pass 함수를 정의\n",
    "class LogisticRegression(nn.Module): \n",
    "    def __init__(self, input_size, num_classes): \n",
    "        super(LogisticRegression, self).__init__() \n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "  \n",
    "    def forward(self, x): \n",
    "        out = self.linear(x) \n",
    "        return out \n",
    "    \n",
    "#모델을 정의\n",
    "model = LogisticRegression(input_size, num_classes)\n",
    "\n",
    "#Loss function을 crossentropyloss로 정의함\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "#optimiser를 SGD로 정의하고, learning rate를 설정, 해당 값은 위의 parameter에서 설정하였음\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) \n",
    "\n",
    "\n",
    "# 모델을 train하는 코드 \n",
    "for epoch in range(num_epochs): \n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        images = Variable(images.view(-1, 28 * 28))  #데이터셋의 크기를 1*784로 변환\n",
    "        labels = Variable(labels) \n",
    "  \n",
    "        \n",
    "        optimizer.zero_grad() #gradient를 0으로 초기화 \n",
    "        outputs = model(images) \n",
    "        loss = criterion(outputs, labels) #loss 값을 설정 \n",
    "        loss.backward() #back propagation을 수행\n",
    "        optimizer.step() #SGD를 활용한 optimization을 수행\n",
    "  \n",
    "\n",
    "        #epoch과 step의 진행상황 그리고 Loss의 값을 표시하는 line\n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'% (epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.data))\n",
    "            \n",
    "            \n",
    "#train set을 통해 학습한 모델을 test set을 통해 정확도를 구함\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) \n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs.data, 1) \n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum() \n",
    "\n",
    "#test set에 대한 모델의 정확도를 출력하는 line    \n",
    "print ('Accuracy of the model on the 10000 test images: % d %%' % (100 * correct / total)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q4-1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model을 학습하는 for 구문에 weight를 update하는 수식 **(w.data = w.data - lr * w.grad.item( ))** 과 다시 weight를 초기화하는 구문 **(w.grad.data.zero_( ))**을 넣어야 할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q4-2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Adam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.656, b: 0.350 Cost: 10.811549\n",
      "Epoch  100/1000 W: 0.132, b: 1.116 Cost: 0.911273\n",
      "Epoch  200/1000 W: 0.393, b: 1.274 Cost: 0.249786\n",
      "Epoch  300/1000 W: 0.463, b: 1.188 Cost: 0.205655\n",
      "Epoch  400/1000 W: 0.517, b: 1.068 Cost: 0.166252\n",
      "Epoch  500/1000 W: 0.574, b: 0.944 Cost: 0.129773\n",
      "Epoch  600/1000 W: 0.630, b: 0.820 Cost: 0.097939\n",
      "Epoch  700/1000 W: 0.684, b: 0.700 Cost: 0.071501\n",
      "Epoch  800/1000 W: 0.734, b: 0.588 Cost: 0.050499\n",
      "Epoch  900/1000 W: 0.780, b: 0.486 Cost: 0.034494\n",
      "Epoch 1000/1000 W: 0.822, b: 0.395 Cost: 0.022775\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs, W, b, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. RMSprop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.727, b: -0.897 Cost: 3.128026\n",
      "Epoch  100/1000 W: 1.140, b: -0.313 Cost: 0.014280\n",
      "Epoch  200/1000 W: 1.078, b: -0.175 Cost: 0.004492\n",
      "Epoch  300/1000 W: 1.030, b: -0.067 Cost: 0.000669\n",
      "Epoch  400/1000 W: 1.006, b: -0.014 Cost: 0.000029\n",
      "Epoch  500/1000 W: 1.000, b: -0.001 Cost: 0.000000\n",
      "Epoch  600/1000 W: 1.000, b: -0.000 Cost: 0.000000\n",
      "Epoch  700/1000 W: 0.974, b: -0.025 Cost: 0.004319\n",
      "Epoch  800/1000 W: 0.997, b: -0.003 Cost: 0.000120\n",
      "Epoch  900/1000 W: 1.000, b: -0.000 Cost: 0.000001\n",
      "Epoch 1000/1000 W: 0.998, b: -0.002 Cost: 0.000027\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs, W, b, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Adagrad**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.842, b: 0.294 Cost: 13.991162\n",
      "Epoch  100/1000 W: -0.671, b: 0.465 Cost: 10.159396\n",
      "Epoch  200/1000 W: -0.595, b: 0.541 Cost: 8.721702\n",
      "Epoch  300/1000 W: -0.537, b: 0.597 Cost: 7.721966\n",
      "Epoch  400/1000 W: -0.490, b: 0.644 Cost: 6.946383\n",
      "Epoch  500/1000 W: -0.449, b: 0.684 Cost: 6.312260\n",
      "Epoch  600/1000 W: -0.413, b: 0.720 Cost: 5.777288\n",
      "Epoch  700/1000 W: -0.381, b: 0.752 Cost: 5.316406\n",
      "Epoch  800/1000 W: -0.351, b: 0.781 Cost: 4.913328\n",
      "Epoch  900/1000 W: -0.324, b: 0.807 Cost: 4.556783\n",
      "Epoch 1000/1000 W: -0.298, b: 0.832 Cost: 4.238591\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs, W, b, cost.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
