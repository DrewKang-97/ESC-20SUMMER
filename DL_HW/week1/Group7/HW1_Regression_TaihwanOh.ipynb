{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 HW1_Regression_TaihwanOh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오태환 1주차 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 경우 Threshold를 0.3으로 낮추어 acceptance rate가 매우 높다. 즉 0.3 이상이면 True로 본다는 뜻이다. 따라서 H_0를 True로 더 많이 판정하게 된다. 따라서 H_0이 not True임에도 True라고 판정하는 경우(False Positive)가 많아지므로 Type 2 Error가 더 높다고 할 수 있을 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Accuracy = \\frac{TP + TN}{TP + FN + FP + TN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Precision = \\frac{TP}{TP+FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Recall = \\frac{TP}{FN + TP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESC는 작년 파이널 프로젝트로 파산 기업 예측 프로젝트를 진행했다. 파산 기업을 예측할 때 중요한 것은 파산하는 기업을 잡아내는 것이다. 파산하는 기업이 은행에 더 많은 손해를 끼치기 때문이다. 그렇기 때문에 실제로 파산하지 않더라도 파산으로 예측하는 비율을 늘리는 것을 감수하고도 최대한 파산할 회사를 많이 예측하는 것이 중요하다. 즉 Recall을 높이는 것이 중요하다. 이 예시에서 threshold를 늘린다는 것의 의미는 파산으로 예측하는 기준 파산 확률을 높인다는 뜻이므로 파산으로 예측하는 비율을 더 낮춘다는 것을 의미한다. 그렇기 때문에 비합리적이라고 생각한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters  \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset (Images and Labels) \n",
    "train_dataset = dsets.MNIST(root ='./data',  \n",
    "                            train = True,  # 이럴로 train 데이터 불러오기\n",
    "                            transform = transforms.ToTensor(), # 이걸로 텐서\n",
    "                            download = True) \n",
    "  \n",
    "test_dataset = dsets.MNIST(root ='./data',  \n",
    "                           train = False,  # 이걸로 test데이터 다운로드\n",
    "                           transform = transforms.ToTensor()) \n",
    "  \n",
    "# Dataset Loader (Input Pipline) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,  \n",
    "                                           batch_size = batch_size,  \n",
    "                                           shuffle = True) #랜덤으로 뽑아서 배치 만들기\n",
    "  \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,  \n",
    "                                          batch_size = batch_size,  \n",
    "                                          shuffle = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module): \n",
    "    def __init__(self, input_size, num_classes): \n",
    "        super(LogisticRegression, self).__init__() \n",
    "        self.linear = nn.Linear(input_size, num_classes) # 1 layer perceptron\n",
    "  \n",
    "    def forward(self, x): \n",
    "        out = self.linear(x) \n",
    "        return out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_size, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # multi class 문제이므로 CE loss 사용\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) \n",
    "# optimizer는 SGD(stochastic gradient descent)를 쓴다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 2.1094\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 2.0178\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 1.9505\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.8744\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.8288\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.6997\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.6013\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.5616\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.4907\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.5579\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.4216\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.4204\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.3585\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.2957\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.2327\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.3307\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.2406\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.1330\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.2528\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.2107\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.1440\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.1368\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.1080\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 1.2118\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 1.0055\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 0.9850\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 1.0528\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 1.0501\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 0.9280\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 0.9842\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        images = Variable(images.view(-1, 28 * 28)) \n",
    "        labels = Variable(labels) \n",
    "  \n",
    "        # Forward + Backward + Optimize \n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(images) \n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "  \n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, \n",
    "                     len(train_dataset) // batch_size, loss.data.item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images:  83 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "# Test the Model \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) \n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs.data, 1) \n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum() \n",
    "  \n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % ( \n",
    "            100 * correct / total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optim.SGD를 사용하지 않으려면 이런 방식을 쓰면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{1}{m}\\sum_{i=1} (H(x^i)-y^i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 cost의 gradient를 learning rate만큼 곱해 빼주면서 업데이트 하자. 코드는 아래와 같다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hypothesis = x_train * W 로 initial hypothesis를 만든 후\n",
    "\n",
    "cost = torch.mean((hypothesis - y_train) ** 2) 로 cost를 만든다.\n",
    "\n",
    "gradient = torch.sum((W * x_train - y_train) * x_train) 으로 gradient 계산 후\n",
    "\n",
    "W -= lr * gradient로 업데이트하는 것을 반복하자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # multi class 문제이므로 CE loss 사용\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) \n",
    "# optimizer는 Adam을 쓴다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 0.6399\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 0.4551\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 0.4013\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 0.3227\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 0.3218\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 0.2855\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 0.2966\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 0.3384\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 0.2063\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 0.2004\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 0.2429\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 0.2189\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 0.3457\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 0.2568\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 0.2491\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 0.3972\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 0.2486\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 0.3762\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 0.2636\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 0.1986\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 0.3512\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 0.2561\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 0.2637\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 0.3531\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 0.1782\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 0.1411\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 0.2234\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 0.2168\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 0.2522\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 0.2895\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        images = Variable(images.view(-1, 28 * 28)) \n",
    "        labels = Variable(labels) \n",
    "  \n",
    "        # Forward + Backward + Optimize \n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(images) \n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "  \n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, \n",
    "                     len(train_dataset) // batch_size, loss.data.item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images:  92 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) \n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs.data, 1) \n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum() \n",
    "  \n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % ( \n",
    "            100 * correct / total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy가 92%로 더 좋아졌다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
