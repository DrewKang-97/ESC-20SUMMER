{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1) 이 경우에서는 Type 1 error 와 Type 2 Error 중 무엇이 더 높을까요? FP, FN과 관련지어 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold를 낮추는 경우에는 acceptance rate가 높아진다. 즉 False Positive rate이 커지고 False Negative rate이 작아지는 경우라고 볼 수 있다. FP는 가입을 받으면 안되는 사람을 잘못해서 받아주는 경우이므로 Type 2 error에 대응이 되고, FN은 가입을 받아야하는 사람을 잘못해서 받아주지 않는 경우이므로 Type 1 error에 대응이 된다. 그렇기 때문에 Type 2 error가 더 크다고 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2-1) Accuracy, Precision and Recall이 무엇인지 TP, FP, TN, FN 의 식으로 나타내 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accurarcy = (TP + TN) / (TP + FP + FN + TN)  \n",
    "Precision = TP / (TP + FP)  \n",
    "Recall = TP / (TP + FN)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2-2) Precision & Recall 에 관한 예시 하나를 들어주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코로나 검사 키트를 생각해보면, threshold를 높이는 것은 코로나에 걸린 사람을 코로나에 걸리지 않았다고 판단하는 경우가 늘어나는 결과를 발생시킨다. 이 경우는 Precision은 높아지겠지만 Recall이 낮아지는 결과를 만들고, 이는 코로나의 전염성을 생각해보았을 때 적절하지 못한 처사로 보인다. 그래서 코로나 검사 키트의 경우는 Recall을 좀 더 고려해야 하는 것으로 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3) 코드 따라해보고 주석 달기! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision.datasets as dsets \n",
    "import torchvision.transforms as transforms \n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac4cec16d3c4b7aba92f16ed456b979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a549238ba84c7e85c600803e49fdd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05028a94842645f3a30f80df3773c072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fefec4b5225b4085a4beb2a7ded82f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Hyper Parameters  \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "# MNIST Dataset (Images and Labels) \n",
    "train_dataset = dsets.MNIST(root ='./data',  \n",
    "                            train = True,                       # train set\n",
    "                            transform = transforms.ToTensor(), \n",
    "                            download = True) \n",
    "  \n",
    "test_dataset = dsets.MNIST(root ='./data',  \n",
    "                           train = False,                      # test set\n",
    "                           transform = transforms.ToTensor()) \n",
    "  \n",
    "# Dataset Loader (Input Pipline) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,  \n",
    "                                           batch_size = batch_size,  \n",
    "                                           shuffle = True) \n",
    "  \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,  \n",
    "                                          batch_size = batch_size,  \n",
    "                                          shuffle = False)\n",
    "\n",
    "class LogisticRegression(nn.Module):                 # the softmax is internally calculated\n",
    "    def __init__(self, input_size, num_classes): \n",
    "        super(LogisticRegression, self).__init__() \n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "  \n",
    "    def forward(self, x): \n",
    "        out = self.linear(x) \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 2.2223\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 2.1365\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 2.0350\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.9340\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.8759\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.7730\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.7241\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.6932\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.5950\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.5418\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.5700\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.4856\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.4501\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.3883\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.2984\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.2720\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.2853\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.2918\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.3145\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.1742\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.1523\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.0764\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.2209\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 1.0802\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 1.0360\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 1.1403\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 1.1006\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 1.0455\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 0.9631\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 1.0433\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(input_size, num_classes) \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) \n",
    "\n",
    "# Training the Model \n",
    "for epoch in range(num_epochs): \n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        images = Variable(images.view(-1, 28 * 28)) \n",
    "        labels = Variable(labels) \n",
    "  \n",
    "        # Forward + Backward + Optimize \n",
    "        optimizer.zero_grad()                  # reset all gradients\n",
    "        outputs = model(images) \n",
    "        loss = criterion(outputs, labels)      # forward \n",
    "        loss.backward()                        # backward\n",
    "        optimizer.step()                       # optimize\n",
    "  \n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, \n",
    "                     len(train_dataset) // batch_size, loss.data)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images:  82 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "# Test the Model \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) \n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs.data, 1) \n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum() \n",
    "  \n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % ( \n",
    "            100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4-1) 2-4 코드에서 optim.SGD를 사용하지않고 코드를 짠다면 어떤 방식으로 짜야할지 설명해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss함수를 미분한 gradient에 적절한 learning rate을 곱한 값을 먼저 구한다. 기존의 w에서 앞에서 구한 값을 빼준 값으로 w를 업데이트 시키는 코드를 직접 작성해서 결과를 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4-2) Gradient descent 코드를 구현하는 문제입니다. 2-4의 코드에서 다른 optimizer 3개를 이용하여 결과값을 비교해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.010, b: 0.010 Cost: 4.666667\n",
      "Epoch  100/1000 W: 0.177, b: 0.176 Cost: 2.620481\n",
      "Epoch  200/1000 W: 0.248, b: 0.247 Cost: 1.959896\n",
      "Epoch  300/1000 W: 0.300, b: 0.298 Cost: 1.543696\n",
      "Epoch  400/1000 W: 0.342, b: 0.339 Cost: 1.248513\n",
      "Epoch  500/1000 W: 0.376, b: 0.372 Cost: 1.027004\n",
      "Epoch  600/1000 W: 0.406, b: 0.401 Cost: 0.855161\n",
      "Epoch  700/1000 W: 0.432, b: 0.426 Cost: 0.718901\n",
      "Epoch  800/1000 W: 0.456, b: 0.448 Cost: 0.609184\n",
      "Epoch  900/1000 W: 0.476, b: 0.468 Cost: 0.519834\n",
      "Epoch 1000/1000 W: 0.495, b: 0.485 Cost: 0.446445\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[1],[2],[3]])\n",
    "\n",
    "W = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "optimizer_Adagrad = optim.Adagrad([W,b], lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs +1):\n",
    "    hypothesis = x_train*W + b\n",
    "    cost = torch.mean((hypothesis - y_train)**2 )\n",
    "    \n",
    "    optimizer_Adagrad.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer_Adagrad.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:,.3f} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, W.item(), b.item(), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.093, b: 0.040 Cost: 4.666667\n",
      "Epoch  100/1000 W: 0.873, b: 0.289 Cost: 0.012043\n",
      "Epoch  200/1000 W: 0.900, b: 0.227 Cost: 0.007443\n",
      "Epoch  300/1000 W: 0.921, b: 0.179 Cost: 0.004601\n",
      "Epoch  400/1000 W: 0.938, b: 0.140 Cost: 0.002844\n",
      "Epoch  500/1000 W: 0.951, b: 0.110 Cost: 0.001758\n",
      "Epoch  600/1000 W: 0.962, b: 0.087 Cost: 0.001087\n",
      "Epoch  700/1000 W: 0.970, b: 0.068 Cost: 0.000673\n",
      "Epoch  800/1000 W: 0.976, b: 0.054 Cost: 0.000416\n",
      "Epoch  900/1000 W: 0.981, b: 0.042 Cost: 0.000258\n",
      "Epoch 1000/1000 W: 0.985, b: 0.033 Cost: 0.000160\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "optimizer_ASGD = optim.ASGD([W,b], lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs +1):\n",
    "    hypothesis = x_train*W + b\n",
    "    cost = torch.mean((hypothesis - y_train)**2 )\n",
    "    \n",
    "    optimizer_ASGD.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer_ASGD.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:,.3f} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, W.item(), b.item(), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.010, b: 0.010 Cost: 4.666667\n",
      "Epoch  100/1000 W: 0.660, b: 0.628 Cost: 0.081239\n",
      "Epoch  200/1000 W: 0.733, b: 0.591 Cost: 0.050898\n",
      "Epoch  300/1000 W: 0.774, b: 0.501 Cost: 0.036588\n",
      "Epoch  400/1000 W: 0.815, b: 0.410 Cost: 0.024475\n",
      "Epoch  500/1000 W: 0.854, b: 0.324 Cost: 0.015306\n",
      "Epoch  600/1000 W: 0.888, b: 0.248 Cost: 0.008968\n",
      "Epoch  700/1000 W: 0.917, b: 0.184 Cost: 0.004927\n",
      "Epoch  800/1000 W: 0.941, b: 0.132 Cost: 0.002537\n",
      "Epoch  900/1000 W: 0.959, b: 0.091 Cost: 0.001224\n",
      "Epoch 1000/1000 W: 0.972, b: 0.061 Cost: 0.000552\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "optimizer_Adam = optim.Adam([W,b], lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs +1):\n",
    "    hypothesis = x_train*W + b\n",
    "    cost = torch.mean((hypothesis - y_train)**2 )\n",
    "    \n",
    "    optimizer_Adam.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer_Adam.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:,.3f} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, W.item(), b.item(), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
